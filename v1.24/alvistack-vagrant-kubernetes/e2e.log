I1214 15:36:34.187601      14 e2e.go:129] Starting e2e run "e50888bf-6310-4046-976d-a2bdad20bdfe" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1671032193 - Will randomize all specs
Will run 356 of 6973 specs

Dec 14 15:36:36.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:36:36.785: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 14 15:36:36.813: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 14 15:36:36.847: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 14 15:36:36.847: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec 14 15:36:36.847: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 14 15:36:36.851: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 14 15:36:36.852: INFO: e2e test version: v1.24.9
Dec 14 15:36:36.853: INFO: kube-apiserver version: v1.24.9
Dec 14 15:36:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:36:36.859: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:36:36.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
Dec 14 15:36:36.897: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W1214 15:36:36.897184      14 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:36:36.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6400" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:36:36.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:36:36.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50" in namespace "downward-api-3859" to be "Succeeded or Failed"
Dec 14 15:36:36.977: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275315ms
Dec 14 15:36:38.989: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016874297s
Dec 14 15:36:40.997: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024423729s
Dec 14 15:36:43.008: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035093751s
Dec 14 15:36:45.028: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Pending", Reason="", readiness=false. Elapsed: 8.055557852s
Dec 14 15:36:47.044: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.071794972s
STEP: Saw pod success
Dec 14 15:36:47.045: INFO: Pod "downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50" satisfied condition "Succeeded or Failed"
Dec 14 15:36:47.051: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50 container client-container: <nil>
STEP: delete the pod
Dec 14 15:36:47.122: INFO: Waiting for pod downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50 to disappear
Dec 14 15:36:47.128: INFO: Pod downwardapi-volume-77bb3ec6-0bac-4477-a357-524f0ff5be50 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:36:47.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3859" for this suite.

• [SLOW TEST:10.226 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":2,"skipped":16,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:36:47.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:36:48.316: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:36:51.372: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:36:51.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6633" for this suite.
STEP: Destroying namespace "webhook-6633-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":3,"skipped":18,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:36:51.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c631cec2-905e-4fda-a24d-29edb645cede
STEP: Creating a pod to test consume secrets
Dec 14 15:36:51.803: INFO: Waiting up to 5m0s for pod "pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531" in namespace "secrets-925" to be "Succeeded or Failed"
Dec 14 15:36:51.807: INFO: Pod "pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261646ms
Dec 14 15:36:53.826: INFO: Pod "pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022927031s
Dec 14 15:36:55.835: INFO: Pod "pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03192741s
STEP: Saw pod success
Dec 14 15:36:55.835: INFO: Pod "pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531" satisfied condition "Succeeded or Failed"
Dec 14 15:36:55.842: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 15:36:55.889: INFO: Waiting for pod pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531 to disappear
Dec 14 15:36:55.894: INFO: Pod pod-secrets-9c278cd6-dcee-425b-be2a-c8de0911f531 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:36:55.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-925" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":69,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:36:55.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-d501ab67-5511-4094-a2f4-28214c623684
STEP: Creating a pod to test consume configMaps
Dec 14 15:36:56.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102" in namespace "projected-708" to be "Succeeded or Failed"
Dec 14 15:36:56.033: INFO: Pod "pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102": Phase="Pending", Reason="", readiness=false. Elapsed: 7.268626ms
Dec 14 15:36:58.053: INFO: Pod "pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027261166s
Dec 14 15:37:00.064: INFO: Pod "pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038266628s
STEP: Saw pod success
Dec 14 15:37:00.064: INFO: Pod "pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102" satisfied condition "Succeeded or Failed"
Dec 14 15:37:00.071: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 15:37:00.103: INFO: Waiting for pod pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102 to disappear
Dec 14 15:37:00.107: INFO: Pod pod-projected-configmaps-ba1db98f-221e-4a6e-8ee0-3e7baa7bb102 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 15:37:00.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-708" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":98,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:37:00.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 15:37:00.157: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 15:37:00.171: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 15:37:00.178: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-1 before test
Dec 14 15:37:00.190: INFO: kube-flannel-ds-9s7dq from kube-flannel started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:37:00.190: INFO: kube-addon-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 15:37:00.190: INFO: kube-apiserver-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 15:37:00.190: INFO: kube-controller-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 15:37:00.190: INFO: kube-proxy-zp6s8 from kube-system started at 2022-12-14 15:34:55 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:37:00.190: INFO: kube-scheduler-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.190: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 15:37:00.191: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:37:00.191: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:37:00.191: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 15:37:00.191: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-2 before test
Dec 14 15:37:00.201: INFO: kube-flannel-ds-2jdlw from kube-flannel started at 2022-12-14 15:34:53 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.202: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:37:00.202: INFO: coredns-57575c5f89-h2qxf from kube-system started at 2022-12-14 15:34:51 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.202: INFO: 	Container coredns ready: true, restart count 0
Dec 14 15:37:00.202: INFO: kube-addon-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.202: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 15:37:00.202: INFO: kube-apiserver-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.203: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 15:37:00.203: INFO: kube-controller-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.203: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 15:37:00.203: INFO: kube-proxy-b847h from kube-system started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.203: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:37:00.203: INFO: kube-scheduler-queith7zooya-2 from kube-system started at 2022-12-14 15:29:31 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.203: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 15:37:00.203: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-rhr5t from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:37:00.203: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:37:00.203: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 15:37:00.203: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-3 before test
Dec 14 15:37:00.211: INFO: kube-flannel-ds-xknfx from kube-flannel started at 2022-12-14 15:34:52 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.211: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:37:00.211: INFO: coredns-57575c5f89-47x6s from kube-system started at 2022-12-14 15:34:51 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.211: INFO: 	Container coredns ready: true, restart count 0
Dec 14 15:37:00.211: INFO: kube-proxy-ms4r6 from kube-system started at 2022-12-14 15:34:52 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.211: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:37:00.211: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:35:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:37:00.211: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 15:37:00.211: INFO: sonobuoy-e2e-job-d6f98c81f7634948 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:37:00.212: INFO: 	Container e2e ready: true, restart count 0
Dec 14 15:37:00.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:37:00.212: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-dc4lx from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:37:00.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:37:00.212: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b5284b9e-1665-499f-9ac5-929a2d4d123d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.248 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b5284b9e-1665-499f-9ac5-929a2d4d123d off the node queith7zooya-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b5284b9e-1665-499f-9ac5-929a2d4d123d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:42:06.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4736" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.300 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":6,"skipped":127,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:06.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:42:06.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6671" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":7,"skipped":133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:06.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 14 15:42:16.668: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 15:42:16.830: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 15:42:16.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5623" for this suite.

• [SLOW TEST:10.350 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":8,"skipped":156,"failed":0}
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:16.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:42:16.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-8446 version'
Dec 14 15:42:17.167: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Dec 14 15:42:17.167: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.9\", GitCommit:\"9710807c82740b9799453677c977758becf0acbb\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:09Z\", GoVersion:\"go1.18.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.9\", GitCommit:\"9710807c82740b9799453677c977758becf0acbb\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:06Z\", GoVersion:\"go1.18.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 15:42:17.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8446" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":9,"skipped":156,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:17.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:42:17.226: INFO: Creating pod...
Dec 14 15:42:19.266: INFO: Creating service...
Dec 14 15:42:19.300: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/DELETE
Dec 14 15:42:19.310: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 15:42:19.311: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/GET
Dec 14 15:42:19.318: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 15:42:19.318: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/HEAD
Dec 14 15:42:19.323: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 15:42:19.323: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/OPTIONS
Dec 14 15:42:19.332: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 15:42:19.332: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/PATCH
Dec 14 15:42:19.337: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 15:42:19.338: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/POST
Dec 14 15:42:19.348: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 15:42:19.348: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/pods/agnhost/proxy/some/path/with/PUT
Dec 14 15:42:19.354: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 15:42:19.354: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/DELETE
Dec 14 15:42:19.362: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 15:42:19.362: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/GET
Dec 14 15:42:19.373: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Dec 14 15:42:19.373: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/HEAD
Dec 14 15:42:19.382: INFO: http.Client request:HEAD | StatusCode:200
Dec 14 15:42:19.382: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/OPTIONS
Dec 14 15:42:19.389: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 15:42:19.389: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/PATCH
Dec 14 15:42:19.397: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 15:42:19.397: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/POST
Dec 14 15:42:19.403: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 15:42:19.403: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4543/services/test-service/proxy/some/path/with/PUT
Dec 14 15:42:19.409: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Dec 14 15:42:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4543" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":10,"skipped":168,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-612a3414-4b8b-48f1-a0d3-2fb16389ba6f
STEP: Creating secret with name secret-projected-all-test-volume-590a89b6-b010-47f3-a2f8-89177cec8c8c
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 14 15:42:19.489: INFO: Waiting up to 5m0s for pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9" in namespace "projected-2093" to be "Succeeded or Failed"
Dec 14 15:42:19.492: INFO: Pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280158ms
Dec 14 15:42:21.511: INFO: Pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021940719s
Dec 14 15:42:23.528: INFO: Pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038660481s
Dec 14 15:42:25.541: INFO: Pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051946018s
STEP: Saw pod success
Dec 14 15:42:25.541: INFO: Pod "projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9" satisfied condition "Succeeded or Failed"
Dec 14 15:42:25.548: INFO: Trying to get logs from node queith7zooya-3 pod projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 14 15:42:25.615: INFO: Waiting for pod projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9 to disappear
Dec 14 15:42:25.620: INFO: Pod projected-volume-8c7bf057-a0dd-4206-8c11-62515d281af9 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Dec 14 15:42:25.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2093" for this suite.

• [SLOW TEST:6.213 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":11,"skipped":172,"failed":0}
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:25.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Dec 14 15:42:25.711: INFO: Found Service test-service-jfhcb in namespace services-8800 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Dec 14 15:42:25.712: INFO: Service test-service-jfhcb created
STEP: Getting /status
Dec 14 15:42:25.719: INFO: Service test-service-jfhcb has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Dec 14 15:42:25.735: INFO: observed Service test-service-jfhcb in namespace services-8800 with annotations: map[] & LoadBalancer: {[]}
Dec 14 15:42:25.736: INFO: Found Service test-service-jfhcb in namespace services-8800 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Dec 14 15:42:25.736: INFO: Service test-service-jfhcb has service status patched
STEP: updating the ServiceStatus
Dec 14 15:42:25.748: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Dec 14 15:42:25.750: INFO: Observed Service test-service-jfhcb in namespace services-8800 with annotations: map[] & Conditions: {[]}
Dec 14 15:42:25.750: INFO: Observed event: &Service{ObjectMeta:{test-service-jfhcb  services-8800  8ed0ec92-7cda-4379-8403-c84ce0814961 3070 0 2022-12-14 15:42:25 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-12-14 15:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-12-14 15:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.35.164,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.35.164],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Dec 14 15:42:25.751: INFO: Found Service test-service-jfhcb in namespace services-8800 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 15:42:25.751: INFO: Service test-service-jfhcb has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Dec 14 15:42:25.768: INFO: observed Service test-service-jfhcb in namespace services-8800 with labels: map[test-service-static:true]
Dec 14 15:42:25.768: INFO: observed Service test-service-jfhcb in namespace services-8800 with labels: map[test-service-static:true]
Dec 14 15:42:25.768: INFO: observed Service test-service-jfhcb in namespace services-8800 with labels: map[test-service-static:true]
Dec 14 15:42:25.768: INFO: Found Service test-service-jfhcb in namespace services-8800 with labels: map[test-service:patched test-service-static:true]
Dec 14 15:42:25.768: INFO: Service test-service-jfhcb patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Dec 14 15:42:25.788: INFO: Observed event: ADDED
Dec 14 15:42:25.789: INFO: Observed event: MODIFIED
Dec 14 15:42:25.792: INFO: Observed event: MODIFIED
Dec 14 15:42:25.792: INFO: Observed event: MODIFIED
Dec 14 15:42:25.792: INFO: Found Service test-service-jfhcb in namespace services-8800 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Dec 14 15:42:25.792: INFO: Service test-service-jfhcb deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:42:25.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8800" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":12,"skipped":172,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:25.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Dec 14 15:42:25.863: INFO: Waiting up to 5m0s for pod "var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c" in namespace "var-expansion-303" to be "Succeeded or Failed"
Dec 14 15:42:25.868: INFO: Pod "var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.432386ms
Dec 14 15:42:27.887: INFO: Pod "var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024391903s
Dec 14 15:42:29.908: INFO: Pod "var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045158982s
STEP: Saw pod success
Dec 14 15:42:29.908: INFO: Pod "var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c" satisfied condition "Succeeded or Failed"
Dec 14 15:42:29.914: INFO: Trying to get logs from node queith7zooya-3 pod var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c container dapi-container: <nil>
STEP: delete the pod
Dec 14 15:42:29.957: INFO: Waiting for pod var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c to disappear
Dec 14 15:42:29.962: INFO: Pod var-expansion-fb3cd5e7-cb21-4820-b761-3e263d66885c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 15:42:29.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-303" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":13,"skipped":186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:29.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Dec 14 15:42:30.033: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 15:42:33.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8057" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":14,"skipped":219,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:33.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-6b21c47f-2bb1-4f5b-9c56-5b0c6c923a5f
STEP: Creating a pod to test consume configMaps
Dec 14 15:42:33.594: INFO: Waiting up to 5m0s for pod "pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19" in namespace "configmap-5320" to be "Succeeded or Failed"
Dec 14 15:42:33.599: INFO: Pod "pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.384875ms
Dec 14 15:42:35.611: INFO: Pod "pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017215483s
Dec 14 15:42:37.626: INFO: Pod "pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032538247s
STEP: Saw pod success
Dec 14 15:42:37.627: INFO: Pod "pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19" satisfied condition "Succeeded or Failed"
Dec 14 15:42:37.634: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:42:37.678: INFO: Waiting for pod pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19 to disappear
Dec 14 15:42:37.682: INFO: Pod pod-configmaps-97601d1b-0243-4b7c-8d76-d6f5c225ef19 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 15:42:37.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5320" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:37.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:42:39.398: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:42:42.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 14 15:42:42.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:42:42.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6375" for this suite.
STEP: Destroying namespace "webhook-6375-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":16,"skipped":243,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:42.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 15:42:42.701: INFO: Waiting up to 5m0s for pod "pod-05721bb0-dab1-4df3-a56a-227ca4771dc5" in namespace "emptydir-9725" to be "Succeeded or Failed"
Dec 14 15:42:42.707: INFO: Pod "pod-05721bb0-dab1-4df3-a56a-227ca4771dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.575348ms
Dec 14 15:42:44.724: INFO: Pod "pod-05721bb0-dab1-4df3-a56a-227ca4771dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022884253s
Dec 14 15:42:46.737: INFO: Pod "pod-05721bb0-dab1-4df3-a56a-227ca4771dc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035604221s
STEP: Saw pod success
Dec 14 15:42:46.737: INFO: Pod "pod-05721bb0-dab1-4df3-a56a-227ca4771dc5" satisfied condition "Succeeded or Failed"
Dec 14 15:42:46.743: INFO: Trying to get logs from node queith7zooya-3 pod pod-05721bb0-dab1-4df3-a56a-227ca4771dc5 container test-container: <nil>
STEP: delete the pod
Dec 14 15:42:46.780: INFO: Waiting for pod pod-05721bb0-dab1-4df3-a56a-227ca4771dc5 to disappear
Dec 14 15:42:46.789: INFO: Pod pod-05721bb0-dab1-4df3-a56a-227ca4771dc5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 15:42:46.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9725" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":17,"skipped":247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:46.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:42:46.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8264" for this suite.
STEP: Destroying namespace "nspatchtest-c3fab562-0c9c-455e-b8a8-abb5306e9fd0-4046" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":18,"skipped":305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:42:46.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 15:43:00.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1037" for this suite.

• [SLOW TEST:13.222 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":19,"skipped":336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:00.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:43:01.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:43:04.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:43:04.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3575" for this suite.
STEP: Destroying namespace "webhook-3575-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":20,"skipped":419,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:04.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-ac52dbc8-a173-4ec0-b9d7-27f3f8c0f730
STEP: Creating a pod to test consume secrets
Dec 14 15:43:04.758: INFO: Waiting up to 5m0s for pod "pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9" in namespace "secrets-8328" to be "Succeeded or Failed"
Dec 14 15:43:04.804: INFO: Pod "pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 45.830042ms
Dec 14 15:43:06.813: INFO: Pod "pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054469064s
Dec 14 15:43:08.829: INFO: Pod "pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070910267s
STEP: Saw pod success
Dec 14 15:43:08.830: INFO: Pod "pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9" satisfied condition "Succeeded or Failed"
Dec 14 15:43:08.834: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 15:43:08.870: INFO: Waiting for pod pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9 to disappear
Dec 14 15:43:08.876: INFO: Pod pod-secrets-e39a51ee-83e7-431f-b83c-7c1563fc4cb9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:43:08.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8328" for this suite.
STEP: Destroying namespace "secret-namespace-8347" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":21,"skipped":422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:08.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Dec 14 15:43:08.963: INFO: created test-event-1
Dec 14 15:43:08.969: INFO: created test-event-2
Dec 14 15:43:08.977: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Dec 14 15:43:08.982: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Dec 14 15:43:09.014: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Dec 14 15:43:09.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6676" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":22,"skipped":449,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:09.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Dec 14 15:43:09.083: INFO: The status of Pod annotationupdate8bf8df90-74be-45f4-a9f6-f4d4935cdb6d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:43:11.104: INFO: The status of Pod annotationupdate8bf8df90-74be-45f4-a9f6-f4d4935cdb6d is Running (Ready = true)
Dec 14 15:43:11.669: INFO: Successfully updated pod "annotationupdate8bf8df90-74be-45f4-a9f6-f4d4935cdb6d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:43:13.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4055" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":464,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:13.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Dec 14 15:43:13.834: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.836: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.839: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.839: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.862: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.862: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.961: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:13.961: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Dec 14 15:43:14.710: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 15:43:14.711: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Dec 14 15:43:21.427: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Dec 14 15:43:21.466: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Dec 14 15:43:21.469: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.469: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.469: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.469: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.470: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.470: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.470: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.470: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 0
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.473: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.476: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.476: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.528: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.528: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:21.555: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:21.555: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:21.571: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:21.572: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:22.772: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:22.774: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:22.826: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
STEP: listing Deployments
Dec 14 15:43:22.838: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Dec 14 15:43:22.868: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Dec 14 15:43:22.880: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:22.889: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:22.955: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:23.006: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:23.025: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:23.064: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:26.239: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:30.856: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:30.894: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:30.976: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Dec 14 15:43:39.540: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Dec 14 15:43:39.637: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.638: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.638: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 1
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 3
Dec 14 15:43:39.639: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:39.640: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 2
Dec 14 15:43:39.640: INFO: observed Deployment test-deployment in namespace deployment-3896 with ReadyReplicas 3
STEP: deleting the Deployment
Dec 14 15:43:39.656: INFO: observed event type MODIFIED
Dec 14 15:43:39.657: INFO: observed event type MODIFIED
Dec 14 15:43:39.657: INFO: observed event type MODIFIED
Dec 14 15:43:39.658: INFO: observed event type MODIFIED
Dec 14 15:43:39.659: INFO: observed event type MODIFIED
Dec 14 15:43:39.659: INFO: observed event type MODIFIED
Dec 14 15:43:39.659: INFO: observed event type MODIFIED
Dec 14 15:43:39.659: INFO: observed event type MODIFIED
Dec 14 15:43:39.660: INFO: observed event type MODIFIED
Dec 14 15:43:39.660: INFO: observed event type MODIFIED
Dec 14 15:43:39.660: INFO: observed event type MODIFIED
Dec 14 15:43:39.661: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 15:43:39.669: INFO: Log out all the ReplicaSets if there is no deployment created
Dec 14 15:43:39.675: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-3896  07f918f9-3b24-4e34-998c-c7627745a9f3 3606 3 2022-12-14 15:43:13 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 01df6173-da37-432e-ae2f-573e477a73e0 0xc0037f3577 0xc0037f3578}] []  [{kube-controller-manager Update apps/v1 2022-12-14 15:43:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01df6173-da37-432e-ae2f-573e477a73e0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:43:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f3600 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 14 15:43:39.691: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-3896  76049541-cebf-4b22-840f-4a1bb6cac594 3708 2 2022-12-14 15:43:22 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 01df6173-da37-432e-ae2f-573e477a73e0 0xc0037f3667 0xc0037f3668}] []  [{kube-controller-manager Update apps/v1 2022-12-14 15:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01df6173-da37-432e-ae2f-573e477a73e0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:43:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f36f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Dec 14 15:43:39.705: INFO: pod: "test-deployment-74c6dd549b-9jztq":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-9jztq test-deployment-74c6dd549b- deployment-3896  da3f1001-5dea-4e63-aeff-dd05e5b7c742 3665 0 2022-12-14 15:43:22 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 76049541-cebf-4b22-840f-4a1bb6cac594 0xc0020ce377 0xc0020ce378}] []  [{kube-controller-manager Update v1 2022-12-14 15:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76049541-cebf-4b22-840f-4a1bb6cac594\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:43:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnfmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnfmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.24,StartTime:2022-12-14 15:43:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:43:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://88b8242adeb2f282e3bac3ae19bf2279eb8d0383f7db073c52e57baabe50d975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 15:43:39.705: INFO: pod: "test-deployment-74c6dd549b-mlfck":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-mlfck test-deployment-74c6dd549b- deployment-3896  a55e194d-5e28-4d05-b315-078d7e2953c4 3707 0 2022-12-14 15:43:30 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-74c6dd549b 76049541-cebf-4b22-840f-4a1bb6cac594 0xc0020ce567 0xc0020ce568}] []  [{kube-controller-manager Update v1 2022-12-14 15:43:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76049541-cebf-4b22-840f-4a1bb6cac594\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:43:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98656,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98656,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.4,StartTime:2022-12-14 15:43:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:43:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b55b7e34b6433aae53c07b124e9751bdeb8fc2882031e1041eb9f0c0d5928e07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Dec 14 15:43:39.706: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-3896  e6305022-28af-4067-a744-bde1b21ec8af 3715 4 2022-12-14 15:43:21 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 01df6173-da37-432e-ae2f-573e477a73e0 0xc0037f3757 0xc0037f3758}] []  [{kube-controller-manager Update apps/v1 2022-12-14 15:43:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01df6173-da37-432e-ae2f-573e477a73e0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:43:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f37e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Dec 14 15:43:39.711: INFO: pod: "test-deployment-84b949bdfc-g7h6p":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-g7h6p test-deployment-84b949bdfc- deployment-3896  5c6f90c0-81ca-4c78-86a7-b5fd40cff4d1 3710 0 2022-12-14 15:43:22 +0000 UTC 2022-12-14 15:43:40 +0000 UTC 0xc0020cf900 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-84b949bdfc e6305022-28af-4067-a744-bde1b21ec8af 0xc0020cf937 0xc0020cf938}] []  [{kube-controller-manager Update v1 2022-12-14 15:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6305022-28af-4067-a744-bde1b21ec8af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:43:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mb9sn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mb9sn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:43:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.5,StartTime:2022-12-14 15:43:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:43:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:cri-o://df91971b9388b73e1558f6fe3611560d92cfd4d4df36f1948073759643b24ff7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 15:43:39.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3896" for this suite.

• [SLOW TEST:25.994 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":24,"skipped":466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:43:39.830: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1" in namespace "projected-6170" to be "Succeeded or Failed"
Dec 14 15:43:39.834: INFO: Pod "downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647236ms
Dec 14 15:43:41.846: INFO: Pod "downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016265789s
Dec 14 15:43:43.863: INFO: Pod "downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033565818s
STEP: Saw pod success
Dec 14 15:43:43.863: INFO: Pod "downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1" satisfied condition "Succeeded or Failed"
Dec 14 15:43:43.870: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1 container client-container: <nil>
STEP: delete the pod
Dec 14 15:43:43.900: INFO: Waiting for pod downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1 to disappear
Dec 14 15:43:43.904: INFO: Pod downwardapi-volume-ebfd38e8-b506-45ff-834b-c8dd7dd2c3e1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 15:43:43.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6170" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":25,"skipped":514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:43:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Dec 14 15:45:02.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2125" for this suite.

• [SLOW TEST:78.155 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":26,"skipped":556,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:02.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:45:02.166: INFO: Got root ca configmap in namespace "svcaccounts-5990"
Dec 14 15:45:02.185: INFO: Deleted root ca configmap in namespace "svcaccounts-5990"
STEP: waiting for a new root ca configmap created
Dec 14 15:45:02.699: INFO: Recreated root ca configmap in namespace "svcaccounts-5990"
Dec 14 15:45:02.709: INFO: Updated root ca configmap in namespace "svcaccounts-5990"
STEP: waiting for the root ca configmap reconciled
Dec 14 15:45:03.223: INFO: Reconciled root ca configmap in namespace "svcaccounts-5990"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 15:45:03.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5990" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":27,"skipped":570,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:03.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:45:03.312: INFO: The status of Pod server-envvars-82c7f7c4-7709-4bae-9720-e0a78fdc5aa8 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:45:05.332: INFO: The status of Pod server-envvars-82c7f7c4-7709-4bae-9720-e0a78fdc5aa8 is Running (Ready = true)
Dec 14 15:45:05.366: INFO: Waiting up to 5m0s for pod "client-envvars-d4034424-7d25-461d-be38-81447ff725aa" in namespace "pods-7718" to be "Succeeded or Failed"
Dec 14 15:45:05.371: INFO: Pod "client-envvars-d4034424-7d25-461d-be38-81447ff725aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.373601ms
Dec 14 15:45:07.379: INFO: Pod "client-envvars-d4034424-7d25-461d-be38-81447ff725aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012988423s
Dec 14 15:45:09.396: INFO: Pod "client-envvars-d4034424-7d25-461d-be38-81447ff725aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029749926s
STEP: Saw pod success
Dec 14 15:45:09.396: INFO: Pod "client-envvars-d4034424-7d25-461d-be38-81447ff725aa" satisfied condition "Succeeded or Failed"
Dec 14 15:45:09.401: INFO: Trying to get logs from node queith7zooya-3 pod client-envvars-d4034424-7d25-461d-be38-81447ff725aa container env3cont: <nil>
STEP: delete the pod
Dec 14 15:45:09.443: INFO: Waiting for pod client-envvars-d4034424-7d25-461d-be38-81447ff725aa to disappear
Dec 14 15:45:09.447: INFO: Pod client-envvars-d4034424-7d25-461d-be38-81447ff725aa no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 15:45:09.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7718" for this suite.

• [SLOW TEST:6.217 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":28,"skipped":579,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:09.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 15:45:11.289: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 15:45:11.312: INFO: waiting for watch events with expected annotations
Dec 14 15:45:11.312: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:45:11.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3492" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":29,"skipped":658,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:11.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 14 15:45:12.605: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 15:45:12.740: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 15:45:12.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4891" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":30,"skipped":682,"failed":0}
SSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:12.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Dec 14 15:45:12.838: INFO: Waiting up to 5m0s for pod "client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6" in namespace "containers-4938" to be "Succeeded or Failed"
Dec 14 15:45:12.852: INFO: Pod "client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.753818ms
Dec 14 15:45:14.867: INFO: Pod "client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029142318s
Dec 14 15:45:16.884: INFO: Pod "client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046163633s
STEP: Saw pod success
Dec 14 15:45:16.884: INFO: Pod "client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6" satisfied condition "Succeeded or Failed"
Dec 14 15:45:16.891: INFO: Trying to get logs from node queith7zooya-3 pod client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:45:16.927: INFO: Waiting for pod client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6 to disappear
Dec 14 15:45:16.932: INFO: Pod client-containers-2be38c7d-d312-48d3-ba56-a9f8b964b5e6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Dec 14 15:45:16.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4938" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":689,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:16.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1181
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 15:45:16.989: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 15:45:17.050: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:45:19.066: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 15:45:21.069: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 15:45:23.061: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 15:45:25.076: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 15:45:27.062: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 15:45:29.071: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 15:45:29.080: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 15:45:31.101: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 15:45:33.097: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 15:45:35.097: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 15:45:37.090: INFO: The status of Pod netserver-1 is Running (Ready = false)
Dec 14 15:45:39.098: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 15:45:39.119: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 15:45:41.174: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 15:45:41.174: INFO: Breadth first check of 10.233.64.6 on host 192.168.121.209...
Dec 14 15:45:41.179: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.33:9080/dial?request=hostname&protocol=http&host=10.233.64.6&port=8083&tries=1'] Namespace:pod-network-test-1181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 15:45:41.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:45:41.181: INFO: ExecWithOptions: Clientset creation
Dec 14 15:45:41.182: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 15:45:41.336: INFO: Waiting for responses: map[]
Dec 14 15:45:41.336: INFO: reached 10.233.64.6 after 0/1 tries
Dec 14 15:45:41.336: INFO: Breadth first check of 10.233.65.6 on host 192.168.121.111...
Dec 14 15:45:41.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.33:9080/dial?request=hostname&protocol=http&host=10.233.65.6&port=8083&tries=1'] Namespace:pod-network-test-1181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 15:45:41.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:45:41.344: INFO: ExecWithOptions: Clientset creation
Dec 14 15:45:41.344: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 15:45:41.457: INFO: Waiting for responses: map[]
Dec 14 15:45:41.457: INFO: reached 10.233.65.6 after 0/1 tries
Dec 14 15:45:41.457: INFO: Breadth first check of 10.233.66.32 on host 192.168.121.248...
Dec 14 15:45:41.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.33:9080/dial?request=hostname&protocol=http&host=10.233.66.32&port=8083&tries=1'] Namespace:pod-network-test-1181 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 15:45:41.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:45:41.468: INFO: ExecWithOptions: Clientset creation
Dec 14 15:45:41.469: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1181/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.33%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 15:45:41.574: INFO: Waiting for responses: map[]
Dec 14 15:45:41.574: INFO: reached 10.233.66.32 after 0/1 tries
Dec 14 15:45:41.574: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Dec 14 15:45:41.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1181" for this suite.

• [SLOW TEST:24.649 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":32,"skipped":695,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:41.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:45:41.675: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080" in namespace "projected-9637" to be "Succeeded or Failed"
Dec 14 15:45:41.681: INFO: Pod "downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859041ms
Dec 14 15:45:43.700: INFO: Pod "downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024898185s
Dec 14 15:45:45.711: INFO: Pod "downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035752601s
STEP: Saw pod success
Dec 14 15:45:45.711: INFO: Pod "downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080" satisfied condition "Succeeded or Failed"
Dec 14 15:45:45.718: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080 container client-container: <nil>
STEP: delete the pod
Dec 14 15:45:45.762: INFO: Waiting for pod downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080 to disappear
Dec 14 15:45:45.769: INFO: Pod downwardapi-volume-bc98b918-55be-448f-b350-883e6115f080 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 15:45:45.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9637" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":715,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:45.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 14 15:45:45.911: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1192  5ac9f194-3907-48be-b159-79ef31463e38 4282 0 2022-12-14 15:45:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-12-14 15:45:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 15:45:45.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1192  5ac9f194-3907-48be-b159-79ef31463e38 4283 0 2022-12-14 15:45:45 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-12-14 15:45:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Dec 14 15:45:45.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1192" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":34,"skipped":718,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:45:45.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 15:46:02.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9651" for this suite.

• [SLOW TEST:16.191 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":35,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:46:03.531: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:46:06.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 14 15:46:08.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=webhook-779 attach --namespace=webhook-779 to-be-attached-pod -i -c=container1'
Dec 14 15:46:08.940: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:46:08.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-779" for this suite.
STEP: Destroying namespace "webhook-779-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.909 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":36,"skipped":738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:09.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-7f3f23a8-66d2-4d45-9092-e01fa2f75768
STEP: Creating secret with name s-test-opt-upd-10fddd8a-baf5-4b19-9704-c5bb57097e2b
STEP: Creating the pod
Dec 14 15:46:09.234: INFO: The status of Pod pod-projected-secrets-3739e0f3-1d9c-46f7-a97d-481cb883d1ee is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:46:11.251: INFO: The status of Pod pod-projected-secrets-3739e0f3-1d9c-46f7-a97d-481cb883d1ee is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-7f3f23a8-66d2-4d45-9092-e01fa2f75768
STEP: Updating secret s-test-opt-upd-10fddd8a-baf5-4b19-9704-c5bb57097e2b
STEP: Creating secret with name s-test-opt-create-4aa1394e-65aa-4aae-a9f2-e161518626e4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 15:46:13.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8166" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":765,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-9627a766-a6ea-4c89-9788-3aaf7bbf6b81
STEP: Creating configMap with name cm-test-opt-upd-c575899f-6711-4dfa-87bc-9bb3a3f45921
STEP: Creating the pod
Dec 14 15:46:13.467: INFO: The status of Pod pod-configmaps-a5012be5-cb24-4752-9f57-66ce7e30185e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:46:15.483: INFO: The status of Pod pod-configmaps-a5012be5-cb24-4752-9f57-66ce7e30185e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-9627a766-a6ea-4c89-9788-3aaf7bbf6b81
STEP: Updating configmap cm-test-opt-upd-c575899f-6711-4dfa-87bc-9bb3a3f45921
STEP: Creating configMap with name cm-test-opt-create-72e87bd0-3be6-4a32-836c-de90479631cc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 15:46:17.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7848" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":766,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:46:17.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f" in namespace "downward-api-4785" to be "Succeeded or Failed"
Dec 14 15:46:17.694: INFO: Pod "downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486765ms
Dec 14 15:46:19.707: INFO: Pod "downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01965122s
Dec 14 15:46:21.722: INFO: Pod "downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034098896s
STEP: Saw pod success
Dec 14 15:46:21.722: INFO: Pod "downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f" satisfied condition "Succeeded or Failed"
Dec 14 15:46:21.732: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f container client-container: <nil>
STEP: delete the pod
Dec 14 15:46:21.769: INFO: Waiting for pod downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f to disappear
Dec 14 15:46:21.775: INFO: Pod downwardapi-volume-e5db7113-354b-4055-b701-999ee906595f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:46:21.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4785" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":39,"skipped":768,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:21.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:46:21.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e" in namespace "downward-api-7037" to be "Succeeded or Failed"
Dec 14 15:46:21.887: INFO: Pod "downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.460211ms
Dec 14 15:46:23.901: INFO: Pod "downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018739974s
Dec 14 15:46:25.914: INFO: Pod "downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031723647s
STEP: Saw pod success
Dec 14 15:46:25.914: INFO: Pod "downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e" satisfied condition "Succeeded or Failed"
Dec 14 15:46:25.921: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e container client-container: <nil>
STEP: delete the pod
Dec 14 15:46:25.954: INFO: Waiting for pod downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e to disappear
Dec 14 15:46:25.968: INFO: Pod downwardapi-volume-6c5e1cb0-8052-45ec-9d7f-c1c1c0018b0e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:46:25.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7037" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":40,"skipped":777,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:25.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Dec 14 15:46:26.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1607 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 15:46:26.169: INFO: stderr: ""
Dec 14 15:46:26.169: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 14 15:46:31.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1607 get pod e2e-test-httpd-pod -o json'
Dec 14 15:46:31.365: INFO: stderr: ""
Dec 14 15:46:31.365: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-12-14T15:46:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1607\",\n        \"resourceVersion\": \"4630\",\n        \"uid\": \"beb29695-d153-435f-8cb1-bd9f3942ff5c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-t4pt7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"queith7zooya-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-t4pt7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T15:46:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T15:46:27Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T15:46:27Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-12-14T15:46:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://3c3b2c1c2f65d63b1f36659062754553de86f67c7565c1d5b87d5c2c72a2fae8\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-12-14T15:46:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.248\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.40\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.40\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-12-14T15:46:26Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 14 15:46:31.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1607 replace -f -'
Dec 14 15:46:31.843: INFO: stderr: ""
Dec 14 15:46:31.843: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Dec 14 15:46:31.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1607 delete pods e2e-test-httpd-pod'
Dec 14 15:46:33.834: INFO: stderr: ""
Dec 14 15:46:33.834: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 15:46:33.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1607" for this suite.

• [SLOW TEST:7.874 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":41,"skipped":785,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-86b8db2d-a1a1-43be-8c23-2a459fc86bae
STEP: Creating a pod to test consume configMaps
Dec 14 15:46:33.930: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453" in namespace "projected-4278" to be "Succeeded or Failed"
Dec 14 15:46:33.934: INFO: Pod "pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049087ms
Dec 14 15:46:35.949: INFO: Pod "pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019175931s
Dec 14 15:46:37.964: INFO: Pod "pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033809089s
STEP: Saw pod success
Dec 14 15:46:37.964: INFO: Pod "pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453" satisfied condition "Succeeded or Failed"
Dec 14 15:46:37.971: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:46:38.015: INFO: Waiting for pod pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453 to disappear
Dec 14 15:46:38.025: INFO: Pod pod-projected-configmaps-1a422506-16ea-4c40-be7e-112c16985453 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 15:46:38.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4278" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":801,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:38.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 15:46:49.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5045" for this suite.

• [SLOW TEST:11.185 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":43,"skipped":820,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:46:49.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Dec 14 15:46:51.317: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8537 PodName:var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 15:46:51.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:46:51.318: INFO: ExecWithOptions: Clientset creation
Dec 14 15:46:51.319: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8537/pods/var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Dec 14 15:46:51.425: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8537 PodName:var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 15:46:51.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:46:51.427: INFO: ExecWithOptions: Clientset creation
Dec 14 15:46:51.427: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8537/pods/var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Dec 14 15:46:52.043: INFO: Successfully updated pod "var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Dec 14 15:46:52.049: INFO: Deleting pod "var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66" in namespace "var-expansion-8537"
Dec 14 15:46:52.061: INFO: Wait up to 5m0s for pod "var-expansion-153b76e7-b1a1-4ce1-8f48-92a615c4ee66" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 15:47:26.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8537" for this suite.

• [SLOW TEST:36.887 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":44,"skipped":830,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:47:26.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Dec 14 15:47:26.168: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Dec 14 15:47:26.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-7358" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":45,"skipped":865,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:47:26.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Dec 14 15:47:53.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7416" for this suite.

• [SLOW TEST:27.660 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:47:53.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5059
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-5059
Dec 14 15:47:53.930: INFO: Found 0 stateful pods, waiting for 1
Dec 14 15:48:03.942: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 15:48:04.000: INFO: Deleting all statefulset in ns statefulset-5059
Dec 14 15:48:04.025: INFO: Scaling statefulset ss to 0
Dec 14 15:48:14.124: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 15:48:14.131: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 15:48:14.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5059" for this suite.

• [SLOW TEST:20.309 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":47,"skipped":904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:48:14.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6252
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6252
STEP: Waiting until pod test-pod will start running in namespace statefulset-6252
STEP: Creating statefulset with conflicting port in namespace statefulset-6252
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6252
Dec 14 15:48:22.332: INFO: Observed stateful pod in namespace: statefulset-6252, name: ss-0, uid: 88828a3e-7a1a-4632-8817-e3f19103afa6, status phase: Pending. Waiting for statefulset controller to delete.
Dec 14 15:48:22.348: INFO: Observed stateful pod in namespace: statefulset-6252, name: ss-0, uid: 88828a3e-7a1a-4632-8817-e3f19103afa6, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 15:48:22.365: INFO: Observed stateful pod in namespace: statefulset-6252, name: ss-0, uid: 88828a3e-7a1a-4632-8817-e3f19103afa6, status phase: Failed. Waiting for statefulset controller to delete.
Dec 14 15:48:22.369: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6252
STEP: Removing pod with conflicting port in namespace statefulset-6252
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6252 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 15:48:38.509: INFO: Deleting all statefulset in ns statefulset-6252
Dec 14 15:48:38.517: INFO: Scaling statefulset ss to 0
Dec 14 15:48:48.571: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 15:48:48.581: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 15:48:48.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6252" for this suite.

• [SLOW TEST:34.453 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":48,"skipped":988,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:48:48.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-297
STEP: creating service affinity-clusterip in namespace services-297
STEP: creating replication controller affinity-clusterip in namespace services-297
I1214 15:48:48.710385      14 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-297, replica count: 3
I1214 15:48:51.761747      14 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:48:51.787: INFO: Creating new exec pod
Dec 14 15:48:54.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-297 exec execpod-affinitynpgfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Dec 14 15:48:55.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Dec 14 15:48:55.147: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:48:55.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-297 exec execpod-affinitynpgfg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.45.178 80'
Dec 14 15:48:55.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.45.178 80\nConnection to 10.233.45.178 80 port [tcp/http] succeeded!\n"
Dec 14 15:48:55.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:48:55.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-297 exec execpod-affinitynpgfg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.45.178:80/ ; done'
Dec 14 15:48:55.724: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.45.178:80/\n"
Dec 14 15:48:55.724: INFO: stdout: "\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l\naffinity-clusterip-rrd5l"
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.724: INFO: Received response from host: affinity-clusterip-rrd5l
Dec 14 15:48:55.725: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-297, will wait for the garbage collector to delete the pods
Dec 14 15:48:55.828: INFO: Deleting ReplicationController affinity-clusterip took: 21.956883ms
Dec 14 15:48:55.929: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.49441ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:48:57.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-297" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.273 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":49,"skipped":992,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:48:57.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Dec 14 15:48:58.027: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Dec 14 15:48:58.063: INFO: waiting for watch events with expected annotations in namespace
Dec 14 15:48:58.063: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Dec 14 15:48:58.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-2955" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":50,"skipped":1050,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:48:58.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Dec 14 15:49:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8668" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":51,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:02.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8553
STEP: creating service affinity-clusterip-transition in namespace services-8553
STEP: creating replication controller affinity-clusterip-transition in namespace services-8553
I1214 15:49:02.339259      14 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8553, replica count: 3
I1214 15:49:05.394976      14 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:49:05.411: INFO: Creating new exec pod
Dec 14 15:49:08.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8553 exec execpod-affinitybstmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Dec 14 15:49:08.680: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Dec 14 15:49:08.680: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:49:08.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8553 exec execpod-affinitybstmr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.46.87 80'
Dec 14 15:49:08.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.46.87 80\nConnection to 10.233.46.87 80 port [tcp/http] succeeded!\n"
Dec 14 15:49:08.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:49:08.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8553 exec execpod-affinitybstmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.46.87:80/ ; done'
Dec 14 15:49:09.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n"
Dec 14 15:49:09.505: INFO: stdout: "\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-b285d\naffinity-clusterip-transition-b285d\naffinity-clusterip-transition-b285d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-wrgz8\naffinity-clusterip-transition-wrgz8\naffinity-clusterip-transition-wrgz8\naffinity-clusterip-transition-b285d\naffinity-clusterip-transition-b285d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-b285d"
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-wrgz8
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-wrgz8
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-wrgz8
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.505: INFO: Received response from host: affinity-clusterip-transition-b285d
Dec 14 15:49:09.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8553 exec execpod-affinitybstmr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.46.87:80/ ; done'
Dec 14 15:49:09.924: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.46.87:80/\n"
Dec 14 15:49:09.924: INFO: stdout: "\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d\naffinity-clusterip-transition-8pv8d"
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Received response from host: affinity-clusterip-transition-8pv8d
Dec 14 15:49:09.924: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8553, will wait for the garbage collector to delete the pods
Dec 14 15:49:10.024: INFO: Deleting ReplicationController affinity-clusterip-transition took: 16.124294ms
Dec 14 15:49:10.131: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 107.411962ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:49:11.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8553" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.766 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":52,"skipped":1135,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:12.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 15:49:12.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 15:49:12.149: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 15:49:13.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 15:49:13.172: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 15:49:14.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 15:49:14.171: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Dec 14 15:49:14.187: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Dec 14 15:49:14.210: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Dec 14 15:49:14.214: INFO: Observed &DaemonSet event: ADDED
Dec 14 15:49:14.214: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.215: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.215: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.215: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.215: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.216: INFO: Found daemon set daemon-set in namespace daemonsets-1298 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 15:49:14.216: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Dec 14 15:49:14.229: INFO: Observed &DaemonSet event: ADDED
Dec 14 15:49:14.229: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.231: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.231: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.232: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.232: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.232: INFO: Observed daemon set daemon-set in namespace daemonsets-1298 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 15:49:14.233: INFO: Observed &DaemonSet event: MODIFIED
Dec 14 15:49:14.233: INFO: Found daemon set daemon-set in namespace daemonsets-1298 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Dec 14 15:49:14.233: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1298, will wait for the garbage collector to delete the pods
Dec 14 15:49:14.309: INFO: Deleting DaemonSet.extensions daemon-set took: 16.257375ms
Dec 14 15:49:14.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.289246ms
Dec 14 15:49:17.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 15:49:17.025: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 15:49:17.031: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5554"},"items":null}

Dec 14 15:49:17.035: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5554"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:49:17.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1298" for this suite.

• [SLOW TEST:5.048 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":53,"skipped":1138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:49:18.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:49:21.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:49:31.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1484" for this suite.
STEP: Destroying namespace "webhook-1484-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:14.570 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":54,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:31.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:49:32.770: INFO: Checking APIGroup: apiregistration.k8s.io
Dec 14 15:49:32.772: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Dec 14 15:49:32.772: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Dec 14 15:49:32.772: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Dec 14 15:49:32.772: INFO: Checking APIGroup: apps
Dec 14 15:49:32.773: INFO: PreferredVersion.GroupVersion: apps/v1
Dec 14 15:49:32.773: INFO: Versions found [{apps/v1 v1}]
Dec 14 15:49:32.773: INFO: apps/v1 matches apps/v1
Dec 14 15:49:32.773: INFO: Checking APIGroup: events.k8s.io
Dec 14 15:49:32.774: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Dec 14 15:49:32.774: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Dec 14 15:49:32.774: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Dec 14 15:49:32.774: INFO: Checking APIGroup: authentication.k8s.io
Dec 14 15:49:32.776: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Dec 14 15:49:32.776: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Dec 14 15:49:32.776: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Dec 14 15:49:32.776: INFO: Checking APIGroup: authorization.k8s.io
Dec 14 15:49:32.777: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Dec 14 15:49:32.777: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Dec 14 15:49:32.777: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Dec 14 15:49:32.777: INFO: Checking APIGroup: autoscaling
Dec 14 15:49:32.778: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Dec 14 15:49:32.779: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Dec 14 15:49:32.779: INFO: autoscaling/v2 matches autoscaling/v2
Dec 14 15:49:32.779: INFO: Checking APIGroup: batch
Dec 14 15:49:32.780: INFO: PreferredVersion.GroupVersion: batch/v1
Dec 14 15:49:32.780: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Dec 14 15:49:32.780: INFO: batch/v1 matches batch/v1
Dec 14 15:49:32.780: INFO: Checking APIGroup: certificates.k8s.io
Dec 14 15:49:32.781: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Dec 14 15:49:32.781: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Dec 14 15:49:32.781: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Dec 14 15:49:32.781: INFO: Checking APIGroup: networking.k8s.io
Dec 14 15:49:32.783: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Dec 14 15:49:32.783: INFO: Versions found [{networking.k8s.io/v1 v1}]
Dec 14 15:49:32.783: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Dec 14 15:49:32.783: INFO: Checking APIGroup: policy
Dec 14 15:49:32.785: INFO: PreferredVersion.GroupVersion: policy/v1
Dec 14 15:49:32.785: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Dec 14 15:49:32.785: INFO: policy/v1 matches policy/v1
Dec 14 15:49:32.785: INFO: Checking APIGroup: rbac.authorization.k8s.io
Dec 14 15:49:32.790: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Dec 14 15:49:32.790: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Dec 14 15:49:32.790: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Dec 14 15:49:32.790: INFO: Checking APIGroup: storage.k8s.io
Dec 14 15:49:32.792: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Dec 14 15:49:32.792: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Dec 14 15:49:32.792: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Dec 14 15:49:32.792: INFO: Checking APIGroup: admissionregistration.k8s.io
Dec 14 15:49:32.794: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Dec 14 15:49:32.794: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Dec 14 15:49:32.794: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Dec 14 15:49:32.794: INFO: Checking APIGroup: apiextensions.k8s.io
Dec 14 15:49:32.796: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Dec 14 15:49:32.796: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Dec 14 15:49:32.796: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Dec 14 15:49:32.796: INFO: Checking APIGroup: scheduling.k8s.io
Dec 14 15:49:32.798: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Dec 14 15:49:32.798: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Dec 14 15:49:32.798: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Dec 14 15:49:32.798: INFO: Checking APIGroup: coordination.k8s.io
Dec 14 15:49:32.800: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Dec 14 15:49:32.800: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Dec 14 15:49:32.800: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Dec 14 15:49:32.800: INFO: Checking APIGroup: node.k8s.io
Dec 14 15:49:32.801: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Dec 14 15:49:32.801: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Dec 14 15:49:32.801: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Dec 14 15:49:32.801: INFO: Checking APIGroup: discovery.k8s.io
Dec 14 15:49:32.802: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Dec 14 15:49:32.802: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Dec 14 15:49:32.802: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Dec 14 15:49:32.802: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Dec 14 15:49:32.804: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Dec 14 15:49:32.804: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Dec 14 15:49:32.804: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Dec 14 15:49:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8152" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":55,"skipped":1204,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:32.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:49:32.906: INFO: The status of Pod busybox-host-aliases480ad782-2786-4074-af7a-32123486482e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:49:34.919: INFO: The status of Pod busybox-host-aliases480ad782-2786-4074-af7a-32123486482e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Dec 14 15:49:34.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5384" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":56,"skipped":1216,"failed":0}
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:34.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:49:35.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3196
I1214 15:49:35.035049      14 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3196, replica count: 1
I1214 15:49:36.087537      14 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:49:36.213: INFO: Created: latency-svc-l6dpd
Dec 14 15:49:36.241: INFO: Got endpoints: latency-svc-l6dpd [50.234896ms]
Dec 14 15:49:36.281: INFO: Created: latency-svc-2ww9p
Dec 14 15:49:36.283: INFO: Got endpoints: latency-svc-2ww9p [41.105914ms]
Dec 14 15:49:36.299: INFO: Created: latency-svc-tds55
Dec 14 15:49:36.312: INFO: Got endpoints: latency-svc-tds55 [68.749436ms]
Dec 14 15:49:36.335: INFO: Created: latency-svc-b6dnb
Dec 14 15:49:36.336: INFO: Got endpoints: latency-svc-b6dnb [93.790137ms]
Dec 14 15:49:36.350: INFO: Created: latency-svc-7scvj
Dec 14 15:49:36.355: INFO: Created: latency-svc-9lkkz
Dec 14 15:49:36.355: INFO: Got endpoints: latency-svc-9lkkz [112.214296ms]
Dec 14 15:49:36.387: INFO: Got endpoints: latency-svc-7scvj [144.212226ms]
Dec 14 15:49:36.388: INFO: Created: latency-svc-6l99d
Dec 14 15:49:36.400: INFO: Got endpoints: latency-svc-6l99d [156.619825ms]
Dec 14 15:49:36.416: INFO: Created: latency-svc-mjhg7
Dec 14 15:49:36.418: INFO: Got endpoints: latency-svc-mjhg7 [173.907752ms]
Dec 14 15:49:36.436: INFO: Created: latency-svc-gkqnn
Dec 14 15:49:36.438: INFO: Created: latency-svc-x7bqq
Dec 14 15:49:36.446: INFO: Got endpoints: latency-svc-gkqnn [202.57301ms]
Dec 14 15:49:36.446: INFO: Got endpoints: latency-svc-x7bqq [202.997447ms]
Dec 14 15:49:36.454: INFO: Created: latency-svc-8mhvk
Dec 14 15:49:36.466: INFO: Got endpoints: latency-svc-8mhvk [223.082192ms]
Dec 14 15:49:36.483: INFO: Created: latency-svc-zlbfm
Dec 14 15:49:36.509: INFO: Got endpoints: latency-svc-zlbfm [265.416596ms]
Dec 14 15:49:36.538: INFO: Created: latency-svc-bj2vx
Dec 14 15:49:36.548: INFO: Created: latency-svc-prrnb
Dec 14 15:49:36.558: INFO: Got endpoints: latency-svc-bj2vx [314.37044ms]
Dec 14 15:49:36.560: INFO: Created: latency-svc-2q486
Dec 14 15:49:36.562: INFO: Got endpoints: latency-svc-prrnb [318.04354ms]
Dec 14 15:49:36.576: INFO: Created: latency-svc-vfnjc
Dec 14 15:49:36.582: INFO: Got endpoints: latency-svc-2q486 [338.46989ms]
Dec 14 15:49:36.591: INFO: Created: latency-svc-9tmx5
Dec 14 15:49:36.592: INFO: Got endpoints: latency-svc-vfnjc [347.848747ms]
Dec 14 15:49:36.600: INFO: Created: latency-svc-jtwx4
Dec 14 15:49:36.598: INFO: Got endpoints: latency-svc-9tmx5 [314.939132ms]
Dec 14 15:49:36.622: INFO: Created: latency-svc-7pgk9
Dec 14 15:49:36.625: INFO: Created: latency-svc-5kgl8
Dec 14 15:49:36.640: INFO: Got endpoints: latency-svc-jtwx4 [327.27926ms]
Dec 14 15:49:36.649: INFO: Got endpoints: latency-svc-7pgk9 [312.156291ms]
Dec 14 15:49:36.649: INFO: Got endpoints: latency-svc-5kgl8 [294.069121ms]
Dec 14 15:49:36.649: INFO: Created: latency-svc-w8kb8
Dec 14 15:49:36.653: INFO: Created: latency-svc-tpdlk
Dec 14 15:49:36.663: INFO: Created: latency-svc-h9td9
Dec 14 15:49:36.694: INFO: Got endpoints: latency-svc-w8kb8 [306.230533ms]
Dec 14 15:49:36.695: INFO: Got endpoints: latency-svc-h9td9 [277.074746ms]
Dec 14 15:49:36.700: INFO: Got endpoints: latency-svc-tpdlk [299.981673ms]
Dec 14 15:49:36.818: INFO: Created: latency-svc-bqqq6
Dec 14 15:49:36.818: INFO: Created: latency-svc-g2r55
Dec 14 15:49:36.819: INFO: Created: latency-svc-nw7gv
Dec 14 15:49:36.823: INFO: Created: latency-svc-t8xmn
Dec 14 15:49:36.823: INFO: Created: latency-svc-bt8fg
Dec 14 15:49:36.824: INFO: Created: latency-svc-hlzhw
Dec 14 15:49:36.824: INFO: Created: latency-svc-rvnf5
Dec 14 15:49:36.825: INFO: Created: latency-svc-bjvw7
Dec 14 15:49:36.825: INFO: Created: latency-svc-nvqxz
Dec 14 15:49:36.825: INFO: Created: latency-svc-lxnph
Dec 14 15:49:36.826: INFO: Created: latency-svc-sb8nk
Dec 14 15:49:36.837: INFO: Created: latency-svc-d8zqz
Dec 14 15:49:36.840: INFO: Created: latency-svc-8q8rg
Dec 14 15:49:36.842: INFO: Created: latency-svc-5wdq6
Dec 14 15:49:36.842: INFO: Created: latency-svc-ldfdm
Dec 14 15:49:36.847: INFO: Got endpoints: latency-svc-bqqq6 [152.432439ms]
Dec 14 15:49:36.848: INFO: Got endpoints: latency-svc-nw7gv [154.5683ms]
Dec 14 15:49:36.856: INFO: Got endpoints: latency-svc-t8xmn [346.537196ms]
Dec 14 15:49:36.856: INFO: Got endpoints: latency-svc-d8zqz [389.747461ms]
Dec 14 15:49:36.857: INFO: Got endpoints: latency-svc-rvnf5 [156.204535ms]
Dec 14 15:49:36.869: INFO: Created: latency-svc-cclp8
Dec 14 15:49:36.875: INFO: Got endpoints: latency-svc-bt8fg [316.51104ms]
Dec 14 15:49:36.880: INFO: Created: latency-svc-g4dpm
Dec 14 15:49:36.891: INFO: Created: latency-svc-bf9qj
Dec 14 15:49:36.894: INFO: Got endpoints: latency-svc-lxnph [301.444941ms]
Dec 14 15:49:36.897: INFO: Created: latency-svc-sl8t4
Dec 14 15:49:36.902: INFO: Created: latency-svc-btjzk
Dec 14 15:49:36.908: INFO: Got endpoints: latency-svc-ldfdm [326.241964ms]
Dec 14 15:49:36.916: INFO: Got endpoints: latency-svc-hlzhw [469.916275ms]
Dec 14 15:49:36.917: INFO: Got endpoints: latency-svc-bjvw7 [268.057426ms]
Dec 14 15:49:36.924: INFO: Got endpoints: latency-svc-5wdq6 [281.336073ms]
Dec 14 15:49:36.927: INFO: Created: latency-svc-hfjxv
Dec 14 15:49:36.930: INFO: Created: latency-svc-fb97r
Dec 14 15:49:36.932: INFO: Got endpoints: latency-svc-8q8rg [485.789098ms]
Dec 14 15:49:36.933: INFO: Got endpoints: latency-svc-g2r55 [370.673924ms]
Dec 14 15:49:36.936: INFO: Created: latency-svc-f5ct2
Dec 14 15:49:36.950: INFO: Got endpoints: latency-svc-sb8nk [301.84281ms]
Dec 14 15:49:36.952: INFO: Created: latency-svc-xmd2s
Dec 14 15:49:36.954: INFO: Got endpoints: latency-svc-nvqxz [354.664738ms]
Dec 14 15:49:36.957: INFO: Got endpoints: latency-svc-cclp8 [109.530986ms]
Dec 14 15:49:36.962: INFO: Created: latency-svc-ltr55
Dec 14 15:49:36.966: INFO: Got endpoints: latency-svc-g4dpm [117.615067ms]
Dec 14 15:49:36.975: INFO: Created: latency-svc-5hvww
Dec 14 15:49:36.977: INFO: Got endpoints: latency-svc-sl8t4 [120.532758ms]
Dec 14 15:49:36.977: INFO: Got endpoints: latency-svc-bf9qj [120.886741ms]
Dec 14 15:49:36.985: INFO: Got endpoints: latency-svc-btjzk [128.51351ms]
Dec 14 15:49:36.991: INFO: Created: latency-svc-f6nss
Dec 14 15:49:36.994: INFO: Created: latency-svc-k66vn
Dec 14 15:49:37.003: INFO: Created: latency-svc-ccqhr
Dec 14 15:49:37.009: INFO: Got endpoints: latency-svc-fb97r [114.902708ms]
Dec 14 15:49:37.010: INFO: Got endpoints: latency-svc-hfjxv [135.034212ms]
Dec 14 15:49:37.023: INFO: Created: latency-svc-gsx5g
Dec 14 15:49:37.024: INFO: Created: latency-svc-n9lvd
Dec 14 15:49:37.033: INFO: Got endpoints: latency-svc-f5ct2 [123.739822ms]
Dec 14 15:49:37.040: INFO: Created: latency-svc-xlds9
Dec 14 15:49:37.055: INFO: Created: latency-svc-k4scl
Dec 14 15:49:37.056: INFO: Created: latency-svc-ldzv8
Dec 14 15:49:37.066: INFO: Created: latency-svc-9zvwh
Dec 14 15:49:37.080: INFO: Created: latency-svc-9cs8r
Dec 14 15:49:37.085: INFO: Got endpoints: latency-svc-xmd2s [167.255238ms]
Dec 14 15:49:37.088: INFO: Created: latency-svc-5jsz8
Dec 14 15:49:37.153: INFO: Got endpoints: latency-svc-ltr55 [236.037446ms]
Dec 14 15:49:37.179: INFO: Created: latency-svc-v66v4
Dec 14 15:49:37.182: INFO: Got endpoints: latency-svc-5hvww [258.705735ms]
Dec 14 15:49:37.200: INFO: Created: latency-svc-rfgbx
Dec 14 15:49:37.202: INFO: Created: latency-svc-c7drb
Dec 14 15:49:37.229: INFO: Created: latency-svc-zqrgb
Dec 14 15:49:37.238: INFO: Got endpoints: latency-svc-f6nss [305.019271ms]
Dec 14 15:49:37.273: INFO: Created: latency-svc-bt5qv
Dec 14 15:49:37.277: INFO: Got endpoints: latency-svc-k66vn [344.526066ms]
Dec 14 15:49:37.318: INFO: Created: latency-svc-8bh7g
Dec 14 15:49:37.333: INFO: Got endpoints: latency-svc-ccqhr [382.145697ms]
Dec 14 15:49:37.371: INFO: Created: latency-svc-fj8xp
Dec 14 15:49:37.387: INFO: Got endpoints: latency-svc-gsx5g [431.824825ms]
Dec 14 15:49:37.415: INFO: Created: latency-svc-jtf8z
Dec 14 15:49:37.426: INFO: Got endpoints: latency-svc-n9lvd [469.177206ms]
Dec 14 15:49:37.448: INFO: Created: latency-svc-f8ptf
Dec 14 15:49:37.477: INFO: Got endpoints: latency-svc-xlds9 [510.326463ms]
Dec 14 15:49:37.501: INFO: Created: latency-svc-t8fpq
Dec 14 15:49:37.535: INFO: Got endpoints: latency-svc-k4scl [558.02885ms]
Dec 14 15:49:37.549: INFO: Created: latency-svc-fbzcd
Dec 14 15:49:37.577: INFO: Got endpoints: latency-svc-ldzv8 [599.125891ms]
Dec 14 15:49:37.593: INFO: Created: latency-svc-x948s
Dec 14 15:49:37.627: INFO: Got endpoints: latency-svc-9zvwh [641.546746ms]
Dec 14 15:49:37.641: INFO: Created: latency-svc-wftvc
Dec 14 15:49:37.678: INFO: Got endpoints: latency-svc-5jsz8 [668.353112ms]
Dec 14 15:49:37.691: INFO: Created: latency-svc-7rtt6
Dec 14 15:49:37.736: INFO: Got endpoints: latency-svc-9cs8r [727.526842ms]
Dec 14 15:49:37.751: INFO: Created: latency-svc-959c2
Dec 14 15:49:37.775: INFO: Got endpoints: latency-svc-v66v4 [742.521398ms]
Dec 14 15:49:37.790: INFO: Created: latency-svc-z4xp5
Dec 14 15:49:37.832: INFO: Got endpoints: latency-svc-rfgbx [747.247941ms]
Dec 14 15:49:37.849: INFO: Created: latency-svc-l2pp6
Dec 14 15:49:37.876: INFO: Got endpoints: latency-svc-c7drb [723.063727ms]
Dec 14 15:49:37.962: INFO: Got endpoints: latency-svc-zqrgb [779.79666ms]
Dec 14 15:49:37.968: INFO: Created: latency-svc-ppnft
Dec 14 15:49:37.972: INFO: Got endpoints: latency-svc-bt5qv [734.399461ms]
Dec 14 15:49:37.996: INFO: Created: latency-svc-xh7qm
Dec 14 15:49:38.007: INFO: Created: latency-svc-n9wdx
Dec 14 15:49:38.025: INFO: Got endpoints: latency-svc-8bh7g [747.014985ms]
Dec 14 15:49:38.039: INFO: Created: latency-svc-266fc
Dec 14 15:49:38.075: INFO: Got endpoints: latency-svc-fj8xp [742.294406ms]
Dec 14 15:49:38.088: INFO: Created: latency-svc-jbsnr
Dec 14 15:49:38.126: INFO: Got endpoints: latency-svc-jtf8z [739.517549ms]
Dec 14 15:49:38.142: INFO: Created: latency-svc-c9lvg
Dec 14 15:49:38.175: INFO: Got endpoints: latency-svc-f8ptf [748.463585ms]
Dec 14 15:49:38.189: INFO: Created: latency-svc-cdjlx
Dec 14 15:49:38.226: INFO: Got endpoints: latency-svc-t8fpq [749.013008ms]
Dec 14 15:49:38.241: INFO: Created: latency-svc-tktrx
Dec 14 15:49:38.274: INFO: Got endpoints: latency-svc-fbzcd [738.74567ms]
Dec 14 15:49:38.293: INFO: Created: latency-svc-8296m
Dec 14 15:49:38.333: INFO: Got endpoints: latency-svc-x948s [756.048287ms]
Dec 14 15:49:38.348: INFO: Created: latency-svc-nmz2x
Dec 14 15:49:38.374: INFO: Got endpoints: latency-svc-wftvc [746.706612ms]
Dec 14 15:49:38.407: INFO: Created: latency-svc-5n4lg
Dec 14 15:49:38.427: INFO: Got endpoints: latency-svc-7rtt6 [748.790358ms]
Dec 14 15:49:38.441: INFO: Created: latency-svc-4vc7w
Dec 14 15:49:38.474: INFO: Got endpoints: latency-svc-959c2 [737.458452ms]
Dec 14 15:49:38.486: INFO: Created: latency-svc-78zbr
Dec 14 15:49:38.540: INFO: Got endpoints: latency-svc-z4xp5 [764.921802ms]
Dec 14 15:49:38.559: INFO: Created: latency-svc-xwn4g
Dec 14 15:49:38.574: INFO: Got endpoints: latency-svc-l2pp6 [741.581021ms]
Dec 14 15:49:38.589: INFO: Created: latency-svc-qnpcp
Dec 14 15:49:38.625: INFO: Got endpoints: latency-svc-ppnft [748.535366ms]
Dec 14 15:49:38.642: INFO: Created: latency-svc-bz57j
Dec 14 15:49:38.678: INFO: Got endpoints: latency-svc-xh7qm [716.011939ms]
Dec 14 15:49:38.691: INFO: Created: latency-svc-lnznw
Dec 14 15:49:38.727: INFO: Got endpoints: latency-svc-n9wdx [754.632272ms]
Dec 14 15:49:38.739: INFO: Created: latency-svc-tgh5s
Dec 14 15:49:38.775: INFO: Got endpoints: latency-svc-266fc [750.338673ms]
Dec 14 15:49:38.789: INFO: Created: latency-svc-qcvq4
Dec 14 15:49:38.833: INFO: Got endpoints: latency-svc-jbsnr [757.446741ms]
Dec 14 15:49:38.847: INFO: Created: latency-svc-9tq28
Dec 14 15:49:38.879: INFO: Got endpoints: latency-svc-c9lvg [752.849679ms]
Dec 14 15:49:38.892: INFO: Created: latency-svc-wkc46
Dec 14 15:49:38.923: INFO: Got endpoints: latency-svc-cdjlx [748.309953ms]
Dec 14 15:49:38.943: INFO: Created: latency-svc-gvkw9
Dec 14 15:49:38.976: INFO: Got endpoints: latency-svc-tktrx [749.864686ms]
Dec 14 15:49:38.991: INFO: Created: latency-svc-rtkf9
Dec 14 15:49:39.026: INFO: Got endpoints: latency-svc-8296m [752.302309ms]
Dec 14 15:49:39.050: INFO: Created: latency-svc-l5xnr
Dec 14 15:49:39.074: INFO: Got endpoints: latency-svc-nmz2x [740.787468ms]
Dec 14 15:49:39.088: INFO: Created: latency-svc-jttkg
Dec 14 15:49:39.128: INFO: Got endpoints: latency-svc-5n4lg [753.123989ms]
Dec 14 15:49:39.140: INFO: Created: latency-svc-dq6nr
Dec 14 15:49:39.176: INFO: Got endpoints: latency-svc-4vc7w [748.459445ms]
Dec 14 15:49:39.190: INFO: Created: latency-svc-vczkh
Dec 14 15:49:39.225: INFO: Got endpoints: latency-svc-78zbr [750.868984ms]
Dec 14 15:49:39.245: INFO: Created: latency-svc-jqg7g
Dec 14 15:49:39.288: INFO: Got endpoints: latency-svc-xwn4g [747.155059ms]
Dec 14 15:49:39.303: INFO: Created: latency-svc-gp6hg
Dec 14 15:49:39.325: INFO: Got endpoints: latency-svc-qnpcp [750.605054ms]
Dec 14 15:49:39.339: INFO: Created: latency-svc-hhv9z
Dec 14 15:49:39.375: INFO: Got endpoints: latency-svc-bz57j [750.255913ms]
Dec 14 15:49:39.391: INFO: Created: latency-svc-lzq6b
Dec 14 15:49:39.424: INFO: Got endpoints: latency-svc-lnznw [745.223621ms]
Dec 14 15:49:39.435: INFO: Created: latency-svc-xqgtk
Dec 14 15:49:39.476: INFO: Got endpoints: latency-svc-tgh5s [748.92142ms]
Dec 14 15:49:39.496: INFO: Created: latency-svc-djgsz
Dec 14 15:49:39.524: INFO: Got endpoints: latency-svc-qcvq4 [748.945764ms]
Dec 14 15:49:39.536: INFO: Created: latency-svc-5gn4k
Dec 14 15:49:39.573: INFO: Got endpoints: latency-svc-9tq28 [740.356695ms]
Dec 14 15:49:39.588: INFO: Created: latency-svc-2mlj5
Dec 14 15:49:39.631: INFO: Got endpoints: latency-svc-wkc46 [751.45703ms]
Dec 14 15:49:39.648: INFO: Created: latency-svc-72b9b
Dec 14 15:49:39.679: INFO: Got endpoints: latency-svc-gvkw9 [755.543594ms]
Dec 14 15:49:39.697: INFO: Created: latency-svc-ph8mh
Dec 14 15:49:39.727: INFO: Got endpoints: latency-svc-rtkf9 [750.250716ms]
Dec 14 15:49:39.742: INFO: Created: latency-svc-w64ls
Dec 14 15:49:39.779: INFO: Got endpoints: latency-svc-l5xnr [752.240645ms]
Dec 14 15:49:39.792: INFO: Created: latency-svc-b8xnh
Dec 14 15:49:39.850: INFO: Got endpoints: latency-svc-jttkg [775.636212ms]
Dec 14 15:49:39.869: INFO: Created: latency-svc-hldl2
Dec 14 15:49:39.875: INFO: Got endpoints: latency-svc-dq6nr [747.754655ms]
Dec 14 15:49:39.886: INFO: Created: latency-svc-m7nwx
Dec 14 15:49:39.923: INFO: Got endpoints: latency-svc-vczkh [746.701813ms]
Dec 14 15:49:39.934: INFO: Created: latency-svc-dqqkw
Dec 14 15:49:39.977: INFO: Got endpoints: latency-svc-jqg7g [751.970552ms]
Dec 14 15:49:39.992: INFO: Created: latency-svc-4gv69
Dec 14 15:49:40.038: INFO: Got endpoints: latency-svc-gp6hg [750.50903ms]
Dec 14 15:49:40.050: INFO: Created: latency-svc-rrqwd
Dec 14 15:49:40.082: INFO: Got endpoints: latency-svc-hhv9z [757.332079ms]
Dec 14 15:49:40.099: INFO: Created: latency-svc-j9z8d
Dec 14 15:49:40.123: INFO: Got endpoints: latency-svc-lzq6b [747.538453ms]
Dec 14 15:49:40.139: INFO: Created: latency-svc-pvmlg
Dec 14 15:49:40.175: INFO: Got endpoints: latency-svc-xqgtk [750.83129ms]
Dec 14 15:49:40.190: INFO: Created: latency-svc-cwg49
Dec 14 15:49:40.224: INFO: Got endpoints: latency-svc-djgsz [748.142024ms]
Dec 14 15:49:40.243: INFO: Created: latency-svc-2475f
Dec 14 15:49:40.281: INFO: Got endpoints: latency-svc-5gn4k [756.378187ms]
Dec 14 15:49:40.311: INFO: Created: latency-svc-znm52
Dec 14 15:49:40.322: INFO: Got endpoints: latency-svc-2mlj5 [748.184882ms]
Dec 14 15:49:40.339: INFO: Created: latency-svc-hrhls
Dec 14 15:49:40.376: INFO: Got endpoints: latency-svc-72b9b [744.67185ms]
Dec 14 15:49:40.408: INFO: Created: latency-svc-558bn
Dec 14 15:49:40.423: INFO: Got endpoints: latency-svc-ph8mh [743.645283ms]
Dec 14 15:49:40.468: INFO: Created: latency-svc-wdhm5
Dec 14 15:49:40.516: INFO: Got endpoints: latency-svc-w64ls [789.301526ms]
Dec 14 15:49:40.557: INFO: Got endpoints: latency-svc-b8xnh [777.701847ms]
Dec 14 15:49:40.565: INFO: Created: latency-svc-6zqlk
Dec 14 15:49:40.577: INFO: Got endpoints: latency-svc-hldl2 [727.370148ms]
Dec 14 15:49:40.593: INFO: Created: latency-svc-x9zdc
Dec 14 15:49:40.599: INFO: Created: latency-svc-lklqr
Dec 14 15:49:40.625: INFO: Got endpoints: latency-svc-m7nwx [749.295318ms]
Dec 14 15:49:40.639: INFO: Created: latency-svc-gj26b
Dec 14 15:49:40.680: INFO: Got endpoints: latency-svc-dqqkw [756.792956ms]
Dec 14 15:49:40.695: INFO: Created: latency-svc-88q8t
Dec 14 15:49:40.730: INFO: Got endpoints: latency-svc-4gv69 [752.988968ms]
Dec 14 15:49:40.744: INFO: Created: latency-svc-8sld7
Dec 14 15:49:40.784: INFO: Got endpoints: latency-svc-rrqwd [745.593908ms]
Dec 14 15:49:40.816: INFO: Created: latency-svc-rjqvs
Dec 14 15:49:40.824: INFO: Got endpoints: latency-svc-j9z8d [742.075665ms]
Dec 14 15:49:40.838: INFO: Created: latency-svc-rjg22
Dec 14 15:49:40.883: INFO: Got endpoints: latency-svc-pvmlg [759.898173ms]
Dec 14 15:49:40.900: INFO: Created: latency-svc-n6hmp
Dec 14 15:49:40.930: INFO: Got endpoints: latency-svc-cwg49 [755.068688ms]
Dec 14 15:49:40.946: INFO: Created: latency-svc-xggzv
Dec 14 15:49:40.978: INFO: Got endpoints: latency-svc-2475f [754.101704ms]
Dec 14 15:49:40.995: INFO: Created: latency-svc-flnb7
Dec 14 15:49:41.028: INFO: Got endpoints: latency-svc-znm52 [746.527676ms]
Dec 14 15:49:41.045: INFO: Created: latency-svc-j7g79
Dec 14 15:49:41.078: INFO: Got endpoints: latency-svc-hrhls [756.440125ms]
Dec 14 15:49:41.097: INFO: Created: latency-svc-nrm8l
Dec 14 15:49:41.126: INFO: Got endpoints: latency-svc-558bn [750.552078ms]
Dec 14 15:49:41.142: INFO: Created: latency-svc-22g27
Dec 14 15:49:41.178: INFO: Got endpoints: latency-svc-wdhm5 [754.668168ms]
Dec 14 15:49:41.218: INFO: Created: latency-svc-l89mt
Dec 14 15:49:41.228: INFO: Got endpoints: latency-svc-6zqlk [711.703523ms]
Dec 14 15:49:41.252: INFO: Created: latency-svc-4m9tz
Dec 14 15:49:41.275: INFO: Got endpoints: latency-svc-x9zdc [718.798042ms]
Dec 14 15:49:41.297: INFO: Created: latency-svc-lddpm
Dec 14 15:49:41.327: INFO: Got endpoints: latency-svc-lklqr [749.59196ms]
Dec 14 15:49:41.342: INFO: Created: latency-svc-27dpl
Dec 14 15:49:41.377: INFO: Got endpoints: latency-svc-gj26b [751.59104ms]
Dec 14 15:49:41.389: INFO: Created: latency-svc-zkwdw
Dec 14 15:49:41.431: INFO: Got endpoints: latency-svc-88q8t [751.02587ms]
Dec 14 15:49:41.472: INFO: Created: latency-svc-pslkl
Dec 14 15:49:41.474: INFO: Got endpoints: latency-svc-8sld7 [743.87207ms]
Dec 14 15:49:41.502: INFO: Created: latency-svc-q6bf2
Dec 14 15:49:41.526: INFO: Got endpoints: latency-svc-rjqvs [741.537824ms]
Dec 14 15:49:41.579: INFO: Created: latency-svc-zsq79
Dec 14 15:49:41.592: INFO: Got endpoints: latency-svc-rjg22 [768.11306ms]
Dec 14 15:49:41.608: INFO: Created: latency-svc-wss68
Dec 14 15:49:41.627: INFO: Got endpoints: latency-svc-n6hmp [744.311913ms]
Dec 14 15:49:41.651: INFO: Created: latency-svc-dcn5m
Dec 14 15:49:41.676: INFO: Got endpoints: latency-svc-xggzv [746.016006ms]
Dec 14 15:49:41.688: INFO: Created: latency-svc-s9ghn
Dec 14 15:49:41.728: INFO: Got endpoints: latency-svc-flnb7 [749.411386ms]
Dec 14 15:49:41.746: INFO: Created: latency-svc-gql2x
Dec 14 15:49:41.777: INFO: Got endpoints: latency-svc-j7g79 [748.917182ms]
Dec 14 15:49:41.791: INFO: Created: latency-svc-k2n69
Dec 14 15:49:41.827: INFO: Got endpoints: latency-svc-nrm8l [748.296659ms]
Dec 14 15:49:41.844: INFO: Created: latency-svc-jswdv
Dec 14 15:49:41.879: INFO: Got endpoints: latency-svc-22g27 [752.773846ms]
Dec 14 15:49:41.914: INFO: Created: latency-svc-c522s
Dec 14 15:49:41.923: INFO: Got endpoints: latency-svc-l89mt [745.607332ms]
Dec 14 15:49:41.937: INFO: Created: latency-svc-bjspf
Dec 14 15:49:41.975: INFO: Got endpoints: latency-svc-4m9tz [746.557392ms]
Dec 14 15:49:41.989: INFO: Created: latency-svc-wf8cg
Dec 14 15:49:42.028: INFO: Got endpoints: latency-svc-lddpm [752.111666ms]
Dec 14 15:49:42.043: INFO: Created: latency-svc-9b5b4
Dec 14 15:49:42.082: INFO: Got endpoints: latency-svc-27dpl [754.91342ms]
Dec 14 15:49:42.099: INFO: Created: latency-svc-tgvpv
Dec 14 15:49:42.127: INFO: Got endpoints: latency-svc-zkwdw [750.251869ms]
Dec 14 15:49:42.138: INFO: Created: latency-svc-kmkmg
Dec 14 15:49:42.184: INFO: Got endpoints: latency-svc-pslkl [753.497958ms]
Dec 14 15:49:42.201: INFO: Created: latency-svc-hqxrl
Dec 14 15:49:42.229: INFO: Got endpoints: latency-svc-q6bf2 [754.652749ms]
Dec 14 15:49:42.253: INFO: Created: latency-svc-w9r85
Dec 14 15:49:42.275: INFO: Got endpoints: latency-svc-zsq79 [749.124534ms]
Dec 14 15:49:42.301: INFO: Created: latency-svc-s88dp
Dec 14 15:49:42.327: INFO: Got endpoints: latency-svc-wss68 [734.210522ms]
Dec 14 15:49:42.345: INFO: Created: latency-svc-rn9sc
Dec 14 15:49:42.376: INFO: Got endpoints: latency-svc-dcn5m [748.15997ms]
Dec 14 15:49:42.390: INFO: Created: latency-svc-x9q6b
Dec 14 15:49:42.425: INFO: Got endpoints: latency-svc-s9ghn [749.470231ms]
Dec 14 15:49:42.442: INFO: Created: latency-svc-sgznv
Dec 14 15:49:42.485: INFO: Got endpoints: latency-svc-gql2x [756.570092ms]
Dec 14 15:49:42.499: INFO: Created: latency-svc-nsjgr
Dec 14 15:49:42.532: INFO: Got endpoints: latency-svc-k2n69 [755.00796ms]
Dec 14 15:49:42.545: INFO: Created: latency-svc-2p9wx
Dec 14 15:49:42.576: INFO: Got endpoints: latency-svc-jswdv [748.791504ms]
Dec 14 15:49:42.589: INFO: Created: latency-svc-rkv8t
Dec 14 15:49:42.629: INFO: Got endpoints: latency-svc-c522s [749.78325ms]
Dec 14 15:49:42.646: INFO: Created: latency-svc-ttw6v
Dec 14 15:49:42.680: INFO: Got endpoints: latency-svc-bjspf [756.737702ms]
Dec 14 15:49:42.696: INFO: Created: latency-svc-xzk7x
Dec 14 15:49:42.727: INFO: Got endpoints: latency-svc-wf8cg [751.962283ms]
Dec 14 15:49:42.739: INFO: Created: latency-svc-hmcw9
Dec 14 15:49:42.777: INFO: Got endpoints: latency-svc-9b5b4 [748.983327ms]
Dec 14 15:49:42.793: INFO: Created: latency-svc-9ngbg
Dec 14 15:49:42.829: INFO: Got endpoints: latency-svc-tgvpv [746.95745ms]
Dec 14 15:49:42.844: INFO: Created: latency-svc-2bczl
Dec 14 15:49:42.875: INFO: Got endpoints: latency-svc-kmkmg [748.223875ms]
Dec 14 15:49:42.896: INFO: Created: latency-svc-b6fbw
Dec 14 15:49:42.930: INFO: Got endpoints: latency-svc-hqxrl [745.660994ms]
Dec 14 15:49:42.956: INFO: Created: latency-svc-f2mr6
Dec 14 15:49:42.975: INFO: Got endpoints: latency-svc-w9r85 [746.644374ms]
Dec 14 15:49:42.989: INFO: Created: latency-svc-wsgrb
Dec 14 15:49:43.031: INFO: Got endpoints: latency-svc-s88dp [755.534294ms]
Dec 14 15:49:43.048: INFO: Created: latency-svc-s5wvp
Dec 14 15:49:43.081: INFO: Got endpoints: latency-svc-rn9sc [753.783589ms]
Dec 14 15:49:43.126: INFO: Got endpoints: latency-svc-x9q6b [750.194538ms]
Dec 14 15:49:43.128: INFO: Created: latency-svc-vkfhz
Dec 14 15:49:43.141: INFO: Created: latency-svc-mzkg2
Dec 14 15:49:43.176: INFO: Got endpoints: latency-svc-sgznv [750.007783ms]
Dec 14 15:49:43.197: INFO: Created: latency-svc-cr2gr
Dec 14 15:49:43.227: INFO: Got endpoints: latency-svc-nsjgr [741.557958ms]
Dec 14 15:49:43.255: INFO: Created: latency-svc-jggqb
Dec 14 15:49:43.283: INFO: Got endpoints: latency-svc-2p9wx [750.61333ms]
Dec 14 15:49:43.328: INFO: Created: latency-svc-jcpfp
Dec 14 15:49:43.331: INFO: Got endpoints: latency-svc-rkv8t [755.314426ms]
Dec 14 15:49:43.357: INFO: Created: latency-svc-r9l2f
Dec 14 15:49:43.379: INFO: Got endpoints: latency-svc-ttw6v [749.458428ms]
Dec 14 15:49:43.393: INFO: Created: latency-svc-b844s
Dec 14 15:49:43.426: INFO: Got endpoints: latency-svc-xzk7x [745.396302ms]
Dec 14 15:49:43.446: INFO: Created: latency-svc-mks7t
Dec 14 15:49:43.473: INFO: Got endpoints: latency-svc-hmcw9 [745.899345ms]
Dec 14 15:49:43.487: INFO: Created: latency-svc-djbtl
Dec 14 15:49:43.523: INFO: Got endpoints: latency-svc-9ngbg [746.713991ms]
Dec 14 15:49:43.537: INFO: Created: latency-svc-xr6gt
Dec 14 15:49:43.580: INFO: Got endpoints: latency-svc-2bczl [751.495029ms]
Dec 14 15:49:43.593: INFO: Created: latency-svc-dtqt2
Dec 14 15:49:43.623: INFO: Got endpoints: latency-svc-b6fbw [746.329604ms]
Dec 14 15:49:43.636: INFO: Created: latency-svc-zrd2v
Dec 14 15:49:43.673: INFO: Got endpoints: latency-svc-f2mr6 [742.706763ms]
Dec 14 15:49:43.686: INFO: Created: latency-svc-6r4dc
Dec 14 15:49:43.724: INFO: Got endpoints: latency-svc-wsgrb [748.627016ms]
Dec 14 15:49:43.737: INFO: Created: latency-svc-ths4j
Dec 14 15:49:43.777: INFO: Got endpoints: latency-svc-s5wvp [745.786958ms]
Dec 14 15:49:43.793: INFO: Created: latency-svc-vc664
Dec 14 15:49:43.828: INFO: Got endpoints: latency-svc-vkfhz [747.641777ms]
Dec 14 15:49:43.842: INFO: Created: latency-svc-jnmf5
Dec 14 15:49:43.875: INFO: Got endpoints: latency-svc-mzkg2 [748.666096ms]
Dec 14 15:49:43.896: INFO: Created: latency-svc-85kgr
Dec 14 15:49:43.931: INFO: Got endpoints: latency-svc-cr2gr [754.985796ms]
Dec 14 15:49:43.948: INFO: Created: latency-svc-2mxvg
Dec 14 15:49:43.978: INFO: Got endpoints: latency-svc-jggqb [751.416429ms]
Dec 14 15:49:43.998: INFO: Created: latency-svc-qjxkp
Dec 14 15:49:44.030: INFO: Got endpoints: latency-svc-jcpfp [747.57102ms]
Dec 14 15:49:44.050: INFO: Created: latency-svc-nz9k8
Dec 14 15:49:44.078: INFO: Got endpoints: latency-svc-r9l2f [747.208392ms]
Dec 14 15:49:44.138: INFO: Got endpoints: latency-svc-b844s [758.757678ms]
Dec 14 15:49:44.181: INFO: Got endpoints: latency-svc-mks7t [755.717518ms]
Dec 14 15:49:44.229: INFO: Got endpoints: latency-svc-djbtl [755.958812ms]
Dec 14 15:49:44.278: INFO: Got endpoints: latency-svc-xr6gt [754.809451ms]
Dec 14 15:49:44.327: INFO: Got endpoints: latency-svc-dtqt2 [746.538012ms]
Dec 14 15:49:44.377: INFO: Got endpoints: latency-svc-zrd2v [754.058747ms]
Dec 14 15:49:44.426: INFO: Got endpoints: latency-svc-6r4dc [752.950428ms]
Dec 14 15:49:44.509: INFO: Got endpoints: latency-svc-ths4j [784.320043ms]
Dec 14 15:49:44.541: INFO: Got endpoints: latency-svc-vc664 [764.084953ms]
Dec 14 15:49:44.575: INFO: Got endpoints: latency-svc-jnmf5 [746.540194ms]
Dec 14 15:49:44.625: INFO: Got endpoints: latency-svc-85kgr [750.203646ms]
Dec 14 15:49:44.676: INFO: Got endpoints: latency-svc-2mxvg [745.055471ms]
Dec 14 15:49:44.729: INFO: Got endpoints: latency-svc-qjxkp [750.669374ms]
Dec 14 15:49:44.775: INFO: Got endpoints: latency-svc-nz9k8 [744.613012ms]
Dec 14 15:49:44.775: INFO: Latencies: [41.105914ms 68.749436ms 93.790137ms 109.530986ms 112.214296ms 114.902708ms 117.615067ms 120.532758ms 120.886741ms 123.739822ms 128.51351ms 135.034212ms 144.212226ms 152.432439ms 154.5683ms 156.204535ms 156.619825ms 167.255238ms 173.907752ms 202.57301ms 202.997447ms 223.082192ms 236.037446ms 258.705735ms 265.416596ms 268.057426ms 277.074746ms 281.336073ms 294.069121ms 299.981673ms 301.444941ms 301.84281ms 305.019271ms 306.230533ms 312.156291ms 314.37044ms 314.939132ms 316.51104ms 318.04354ms 326.241964ms 327.27926ms 338.46989ms 344.526066ms 346.537196ms 347.848747ms 354.664738ms 370.673924ms 382.145697ms 389.747461ms 431.824825ms 469.177206ms 469.916275ms 485.789098ms 510.326463ms 558.02885ms 599.125891ms 641.546746ms 668.353112ms 711.703523ms 716.011939ms 718.798042ms 723.063727ms 727.370148ms 727.526842ms 734.210522ms 734.399461ms 737.458452ms 738.74567ms 739.517549ms 740.356695ms 740.787468ms 741.537824ms 741.557958ms 741.581021ms 742.075665ms 742.294406ms 742.521398ms 742.706763ms 743.645283ms 743.87207ms 744.311913ms 744.613012ms 744.67185ms 745.055471ms 745.223621ms 745.396302ms 745.593908ms 745.607332ms 745.660994ms 745.786958ms 745.899345ms 746.016006ms 746.329604ms 746.527676ms 746.538012ms 746.540194ms 746.557392ms 746.644374ms 746.701813ms 746.706612ms 746.713991ms 746.95745ms 747.014985ms 747.155059ms 747.208392ms 747.247941ms 747.538453ms 747.57102ms 747.641777ms 747.754655ms 748.142024ms 748.15997ms 748.184882ms 748.223875ms 748.296659ms 748.309953ms 748.459445ms 748.463585ms 748.535366ms 748.627016ms 748.666096ms 748.790358ms 748.791504ms 748.917182ms 748.92142ms 748.945764ms 748.983327ms 749.013008ms 749.124534ms 749.295318ms 749.411386ms 749.458428ms 749.470231ms 749.59196ms 749.78325ms 749.864686ms 750.007783ms 750.194538ms 750.203646ms 750.250716ms 750.251869ms 750.255913ms 750.338673ms 750.50903ms 750.552078ms 750.605054ms 750.61333ms 750.669374ms 750.83129ms 750.868984ms 751.02587ms 751.416429ms 751.45703ms 751.495029ms 751.59104ms 751.962283ms 751.970552ms 752.111666ms 752.240645ms 752.302309ms 752.773846ms 752.849679ms 752.950428ms 752.988968ms 753.123989ms 753.497958ms 753.783589ms 754.058747ms 754.101704ms 754.632272ms 754.652749ms 754.668168ms 754.809451ms 754.91342ms 754.985796ms 755.00796ms 755.068688ms 755.314426ms 755.534294ms 755.543594ms 755.717518ms 755.958812ms 756.048287ms 756.378187ms 756.440125ms 756.570092ms 756.737702ms 756.792956ms 757.332079ms 757.446741ms 758.757678ms 759.898173ms 764.084953ms 764.921802ms 768.11306ms 775.636212ms 777.701847ms 779.79666ms 784.320043ms 789.301526ms]
Dec 14 15:49:44.775: INFO: 50 %ile: 746.713991ms
Dec 14 15:49:44.775: INFO: 90 %ile: 755.717518ms
Dec 14 15:49:44.775: INFO: 99 %ile: 784.320043ms
Dec 14 15:49:44.776: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Dec 14 15:49:44.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3196" for this suite.

• [SLOW TEST:9.817 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":57,"skipped":1223,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:44.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Dec 14 15:49:44.861: INFO: Pod name sample-pod: Found 0 pods out of 3
Dec 14 15:49:49.869: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Dec 14 15:49:49.901: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 15:49:49.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8719" for this suite.

• [SLOW TEST:5.219 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":58,"skipped":1229,"failed":0}
SSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:50.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Dec 14 15:49:52.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7650" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":59,"skipped":1232,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:52.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 15:49:52.393: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 15:49:52.411: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 15:49:52.466: INFO: waiting for watch events with expected annotations
Dec 14 15:49:52.466: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Dec 14 15:49:52.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6723" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":60,"skipped":1238,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:49:52.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:50:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8425" for this suite.
STEP: Destroying namespace "nsdeletetest-1824" for this suite.
Dec 14 15:50:05.797: INFO: Namespace nsdeletetest-1824 was already deleted
STEP: Destroying namespace "nsdeletetest-916" for this suite.

• [SLOW TEST:13.213 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":61,"skipped":1256,"failed":0}
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:05.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 14 15:50:05.887: INFO: Waiting up to 5m0s for pod "pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b" in namespace "emptydir-367" to be "Succeeded or Failed"
Dec 14 15:50:05.892: INFO: Pod "pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99438ms
Dec 14 15:50:07.914: INFO: Pod "pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026681665s
Dec 14 15:50:09.930: INFO: Pod "pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042980327s
STEP: Saw pod success
Dec 14 15:50:09.931: INFO: Pod "pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b" satisfied condition "Succeeded or Failed"
Dec 14 15:50:09.937: INFO: Trying to get logs from node queith7zooya-3 pod pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b container test-container: <nil>
STEP: delete the pod
Dec 14 15:50:09.977: INFO: Waiting for pod pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b to disappear
Dec 14 15:50:09.982: INFO: Pod pod-ed7356a1-c70d-4e66-8095-b8e80773fe6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 15:50:09.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-367" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":62,"skipped":1256,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:10.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-d1634d24-0023-4577-9b21-7335657af1c8
STEP: Creating a pod to test consume configMaps
Dec 14 15:50:10.089: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52" in namespace "projected-5572" to be "Succeeded or Failed"
Dec 14 15:50:10.095: INFO: Pod "pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52": Phase="Pending", Reason="", readiness=false. Elapsed: 5.466624ms
Dec 14 15:50:12.113: INFO: Pod "pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023441125s
Dec 14 15:50:14.134: INFO: Pod "pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044588031s
STEP: Saw pod success
Dec 14 15:50:14.134: INFO: Pod "pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52" satisfied condition "Succeeded or Failed"
Dec 14 15:50:14.141: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:50:14.182: INFO: Waiting for pod pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52 to disappear
Dec 14 15:50:14.190: INFO: Pod pod-projected-configmaps-1d3b06c0-0cdd-494d-8966-d35b1ee36d52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 15:50:14.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5572" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":63,"skipped":1271,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:14.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 15:50:14.308: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 15:50:14.316: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 15:50:14.348: INFO: waiting for watch events with expected annotations
Dec 14 15:50:14.348: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Dec 14 15:50:14.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1237" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":64,"skipped":1279,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:14.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-2781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2781 to expose endpoints map[]
Dec 14 15:50:14.506: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Dec 14 15:50:15.528: INFO: successfully validated that service endpoint-test2 in namespace services-2781 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2781
Dec 14 15:50:15.552: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:50:17.567: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2781 to expose endpoints map[pod1:[80]]
Dec 14 15:50:17.591: INFO: successfully validated that service endpoint-test2 in namespace services-2781 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Dec 14 15:50:17.591: INFO: Creating new exec pod
Dec 14 15:50:20.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 15:50:20.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:20.890: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:50:20.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.148 80'
Dec 14 15:50:21.112: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.148 80\nConnection to 10.233.41.148 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:21.112: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2781
Dec 14 15:50:21.127: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:50:23.137: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2781 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 14 15:50:23.165: INFO: successfully validated that service endpoint-test2 in namespace services-2781 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Dec 14 15:50:24.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 15:50:24.376: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:24.376: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:50:24.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.148 80'
Dec 14 15:50:24.615: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.148 80\nConnection to 10.233.41.148 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:24.615: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2781 to expose endpoints map[pod2:[80]]
Dec 14 15:50:25.689: INFO: successfully validated that service endpoint-test2 in namespace services-2781 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Dec 14 15:50:26.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Dec 14 15:50:26.907: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:26.907: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:50:26.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-2781 exec execpodbd5qp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.148 80'
Dec 14 15:50:27.165: INFO: stderr: "+ nc -v -t+  -w 2 10.233.41.148 80echo\n hostName\nConnection to 10.233.41.148 80 port [tcp/http] succeeded!\n"
Dec 14 15:50:27.165: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2781
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2781 to expose endpoints map[]
Dec 14 15:50:28.231: INFO: successfully validated that service endpoint-test2 in namespace services-2781 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:50:28.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2781" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:13.859 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":65,"skipped":1284,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:28.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311
Dec 14 15:50:28.358: INFO: Pod name my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311: Found 0 pods out of 1
Dec 14 15:50:33.372: INFO: Pod name my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311: Found 1 pods out of 1
Dec 14 15:50:33.372: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311" are running
Dec 14 15:50:33.376: INFO: Pod "my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311-zf9fl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:28 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:29 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:29 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:50:28 +0000 UTC Reason: Message:}])
Dec 14 15:50:33.376: INFO: Trying to dial the pod
Dec 14 15:50:38.412: INFO: Controller my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311: Got expected result from replica 1 [my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311-zf9fl]: "my-hostname-basic-6be25995-9dcb-4ebd-b9b8-ca9600f83311-zf9fl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Dec 14 15:50:38.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9182" for this suite.

• [SLOW TEST:10.150 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":66,"skipped":1285,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:38.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 14 15:50:38.506: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 14 15:50:43.524: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Dec 14 15:50:44.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8415" for this suite.

• [SLOW TEST:6.158 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":67,"skipped":1296,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:44.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:50:45.444: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:50:48.495: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:50:48.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:50:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8304" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.364 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":68,"skipped":1296,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:50:51.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-v4kk
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 15:50:52.172: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v4kk" in namespace "subpath-5769" to be "Succeeded or Failed"
Dec 14 15:50:52.185: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Pending", Reason="", readiness=false. Elapsed: 13.564225ms
Dec 14 15:50:54.199: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.026751792s
Dec 14 15:50:56.212: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 4.039960368s
Dec 14 15:50:58.224: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 6.052146093s
Dec 14 15:51:00.243: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 8.070822106s
Dec 14 15:51:02.261: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 10.089589308s
Dec 14 15:51:04.274: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 12.102149533s
Dec 14 15:51:06.287: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 14.115004487s
Dec 14 15:51:08.294: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 16.121755906s
Dec 14 15:51:10.314: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 18.142637412s
Dec 14 15:51:12.328: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=true. Elapsed: 20.156235432s
Dec 14 15:51:14.339: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Running", Reason="", readiness=false. Elapsed: 22.167405753s
Dec 14 15:51:16.357: INFO: Pod "pod-subpath-test-projected-v4kk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.185278884s
STEP: Saw pod success
Dec 14 15:51:16.357: INFO: Pod "pod-subpath-test-projected-v4kk" satisfied condition "Succeeded or Failed"
Dec 14 15:51:16.364: INFO: Trying to get logs from node queith7zooya-3 pod pod-subpath-test-projected-v4kk container test-container-subpath-projected-v4kk: <nil>
STEP: delete the pod
Dec 14 15:51:16.406: INFO: Waiting for pod pod-subpath-test-projected-v4kk to disappear
Dec 14 15:51:16.412: INFO: Pod pod-subpath-test-projected-v4kk no longer exists
STEP: Deleting pod pod-subpath-test-projected-v4kk
Dec 14 15:51:16.412: INFO: Deleting pod "pod-subpath-test-projected-v4kk" in namespace "subpath-5769"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Dec 14 15:51:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5769" for this suite.

• [SLOW TEST:24.466 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":69,"skipped":1312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:51:16.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c6e5f39f-f444-4b59-9fb1-b3852daf151b
STEP: Creating a pod to test consume secrets
Dec 14 15:51:16.516: INFO: Waiting up to 5m0s for pod "pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8" in namespace "secrets-2691" to be "Succeeded or Failed"
Dec 14 15:51:16.520: INFO: Pod "pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895677ms
Dec 14 15:51:18.528: INFO: Pod "pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012325591s
Dec 14 15:51:20.546: INFO: Pod "pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03057495s
STEP: Saw pod success
Dec 14 15:51:20.546: INFO: Pod "pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8" satisfied condition "Succeeded or Failed"
Dec 14 15:51:20.553: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 15:51:20.603: INFO: Waiting for pod pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8 to disappear
Dec 14 15:51:20.609: INFO: Pod pod-secrets-9bf591e2-0299-4c05-89ea-7c245ec3f7e8 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:51:20.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2691" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":70,"skipped":1369,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:51:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:51:20.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a" in namespace "downward-api-1028" to be "Succeeded or Failed"
Dec 14 15:51:20.705: INFO: Pod "downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.579258ms
Dec 14 15:51:22.722: INFO: Pod "downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a": Phase="Running", Reason="", readiness=false. Elapsed: 2.023275534s
Dec 14 15:51:24.737: INFO: Pod "downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038265355s
STEP: Saw pod success
Dec 14 15:51:24.738: INFO: Pod "downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a" satisfied condition "Succeeded or Failed"
Dec 14 15:51:24.744: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a container client-container: <nil>
STEP: delete the pod
Dec 14 15:51:24.791: INFO: Waiting for pod downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a to disappear
Dec 14 15:51:24.796: INFO: Pod downwardapi-volume-5496822c-2fe0-4a00-b0b5-d8cd3ae1331a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:51:24.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1028" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:51:24.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Dec 14 15:51:24.885: INFO: created test-pod-1
Dec 14 15:51:24.891: INFO: created test-pod-2
Dec 14 15:51:24.898: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Dec 14 15:51:24.898: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9842' to be running and ready
Dec 14 15:51:24.945: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 15:51:24.945: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 15:51:24.945: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Dec 14 15:51:24.945: INFO: 0 / 3 pods in namespace 'pods-9842' are running and ready (0 seconds elapsed)
Dec 14 15:51:24.945: INFO: expected 0 pod replicas in namespace 'pods-9842', 0 are Running and Ready.
Dec 14 15:51:24.945: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Dec 14 15:51:24.945: INFO: test-pod-1  queith7zooya-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:51:24 +0000 UTC  }]
Dec 14 15:51:24.945: INFO: test-pod-2  queith7zooya-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:51:24 +0000 UTC  }]
Dec 14 15:51:24.945: INFO: test-pod-3  queith7zooya-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 15:51:24 +0000 UTC  }]
Dec 14 15:51:24.945: INFO: 
Dec 14 15:51:26.976: INFO: 3 / 3 pods in namespace 'pods-9842' are running and ready (2 seconds elapsed)
Dec 14 15:51:26.976: INFO: expected 0 pod replicas in namespace 'pods-9842', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Dec 14 15:51:27.015: INFO: Pod quantity 3 is different from expected quantity 0
Dec 14 15:51:28.035: INFO: Pod quantity 3 is different from expected quantity 0
Dec 14 15:51:29.030: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 15:51:30.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9842" for this suite.

• [SLOW TEST:5.231 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":72,"skipped":1494,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:51:30.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-1853
Dec 14 15:51:30.116: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:51:32.132: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 15:51:32.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 15:51:32.410: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 15:51:32.410: INFO: stdout: "iptables"
Dec 14 15:51:32.410: INFO: proxyMode: iptables
Dec 14 15:51:32.452: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 15:51:32.457: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-1853
STEP: creating replication controller affinity-clusterip-timeout in namespace services-1853
I1214 15:51:32.485419      14 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-1853, replica count: 3
I1214 15:51:35.538413      14 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 15:51:35.557: INFO: Creating new exec pod
Dec 14 15:51:38.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Dec 14 15:51:38.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 15:51:38.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:51:38.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.63.231 80'
Dec 14 15:51:39.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.63.231 80\nConnection to 10.233.63.231 80 port [tcp/http] succeeded!\n"
Dec 14 15:51:39.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 15:51:39.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.63.231:80/ ; done'
Dec 14 15:51:39.486: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n"
Dec 14 15:51:39.486: INFO: stdout: "\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc\naffinity-clusterip-timeout-gqsmc"
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.486: INFO: Received response from host: affinity-clusterip-timeout-gqsmc
Dec 14 15:51:39.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.63.231:80/'
Dec 14 15:51:39.702: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n"
Dec 14 15:51:39.702: INFO: stdout: "affinity-clusterip-timeout-gqsmc"
Dec 14 15:51:59.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.63.231:80/'
Dec 14 15:51:59.949: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n"
Dec 14 15:51:59.949: INFO: stdout: "affinity-clusterip-timeout-gqsmc"
Dec 14 15:52:19.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1853 exec execpod-affinityl54dq -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.63.231:80/'
Dec 14 15:52:20.195: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.63.231:80/\n"
Dec 14 15:52:20.195: INFO: stdout: "affinity-clusterip-timeout-vhmfg"
Dec 14 15:52:20.195: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-1853, will wait for the garbage collector to delete the pods
Dec 14 15:52:20.313: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 14.369869ms
Dec 14 15:52:20.415: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.158003ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 15:52:22.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1853" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:52.619 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":73,"skipped":1504,"failed":0}
S
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:22.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 14 15:52:25.290: INFO: Successfully updated pod "adopt-release-2bgkf"
STEP: Checking that the Job readopts the Pod
Dec 14 15:52:25.292: INFO: Waiting up to 15m0s for pod "adopt-release-2bgkf" in namespace "job-6011" to be "adopted"
Dec 14 15:52:25.305: INFO: Pod "adopt-release-2bgkf": Phase="Running", Reason="", readiness=true. Elapsed: 13.372664ms
Dec 14 15:52:27.334: INFO: Pod "adopt-release-2bgkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.042384673s
Dec 14 15:52:27.334: INFO: Pod "adopt-release-2bgkf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 14 15:52:27.857: INFO: Successfully updated pod "adopt-release-2bgkf"
STEP: Checking that the Job releases the Pod
Dec 14 15:52:27.857: INFO: Waiting up to 15m0s for pod "adopt-release-2bgkf" in namespace "job-6011" to be "released"
Dec 14 15:52:27.863: INFO: Pod "adopt-release-2bgkf": Phase="Running", Reason="", readiness=true. Elapsed: 6.579172ms
Dec 14 15:52:29.874: INFO: Pod "adopt-release-2bgkf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017723289s
Dec 14 15:52:29.875: INFO: Pod "adopt-release-2bgkf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Dec 14 15:52:29.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6011" for this suite.

• [SLOW TEST:7.209 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":74,"skipped":1505,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:29.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-bb83bb65-1f49-4278-bb04-fc3d03526611
STEP: Creating the pod
Dec 14 15:52:29.961: INFO: The status of Pod pod-projected-configmaps-76b90470-ce40-4309-8c9b-3c77c2594c20 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:52:31.976: INFO: The status of Pod pod-projected-configmaps-76b90470-ce40-4309-8c9b-3c77c2594c20 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-bb83bb65-1f49-4278-bb04-fc3d03526611
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3368" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":75,"skipped":1524,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Dec 14 15:52:34.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-4600 cluster-info'
Dec 14 15:52:34.255: INFO: stderr: ""
Dec 14 15:52:34.255: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4600" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":76,"skipped":1526,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-f66e4bd8-4b74-4981-8571-78515e31a9d2
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4048" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":77,"skipped":1530,"failed":0}
SSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2429" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":78,"skipped":1536,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8879" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":79,"skipped":1551,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:52:34.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6413" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":80,"skipped":1552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:34.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:52:34.780: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 14 15:52:39.794: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 15:52:39.794: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 15:52:39.828: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-160  356b89b7-fc2b-4e72-8a0d-f7a37682c78f 8740 1 2022-12-14 15:52:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-12-14 15:52:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00200aae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 15:52:39.832: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 15:52:39.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-160" for this suite.

• [SLOW TEST:5.136 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":81,"skipped":1587,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:39.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:52:39.916: INFO: The status of Pod busybox-readonly-fs02fe8970-4643-4b07-80b2-01f38f44bbc7 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:52:41.927: INFO: The status of Pod busybox-readonly-fs02fe8970-4643-4b07-80b2-01f38f44bbc7 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Dec 14 15:52:41.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-352" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":82,"skipped":1588,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:41.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:52:42.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507" in namespace "projected-9985" to be "Succeeded or Failed"
Dec 14 15:52:42.031: INFO: Pod "downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507": Phase="Pending", Reason="", readiness=false. Elapsed: 6.682386ms
Dec 14 15:52:44.069: INFO: Pod "downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044502793s
Dec 14 15:52:46.083: INFO: Pod "downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058961222s
STEP: Saw pod success
Dec 14 15:52:46.083: INFO: Pod "downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507" satisfied condition "Succeeded or Failed"
Dec 14 15:52:46.090: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507 container client-container: <nil>
STEP: delete the pod
Dec 14 15:52:46.127: INFO: Waiting for pod downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507 to disappear
Dec 14 15:52:46.130: INFO: Pod downwardapi-volume-dd189862-3058-4418-9222-dda656b7d507 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 15:52:46.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9985" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1593,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:46.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:52:46.182: INFO: Creating ReplicaSet my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4
Dec 14 15:52:46.194: INFO: Pod name my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4: Found 0 pods out of 1
Dec 14 15:52:51.211: INFO: Pod name my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4: Found 1 pods out of 1
Dec 14 15:52:51.211: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4" is running
Dec 14 15:52:51.218: INFO: Pod "my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4-h2kmz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:52:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:52:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:52:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-12-14 15:52:46 +0000 UTC Reason: Message:}])
Dec 14 15:52:51.218: INFO: Trying to dial the pod
Dec 14 15:52:56.253: INFO: Controller my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4: Got expected result from replica 1 [my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4-h2kmz]: "my-hostname-basic-df0cef88-d82d-4959-b337-57505d9d0ce4-h2kmz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 15:52:56.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2563" for this suite.

• [SLOW TEST:10.133 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":84,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:52:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Dec 14 15:52:56.332: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 15:53:56.381: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:53:56.389: INFO: Starting informer...
STEP: Starting pod...
Dec 14 15:53:56.629: INFO: Pod is running on queith7zooya-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 14 15:53:56.661: INFO: Pod wasn't evicted. Proceeding
Dec 14 15:53:56.661: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 14 15:55:11.729: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:55:11.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2400" for this suite.

• [SLOW TEST:135.486 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":85,"skipped":1626,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-bb9eab93-9104-40b1-9d74-39e2b47e174a
STEP: Creating a pod to test consume configMaps
Dec 14 15:55:11.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265" in namespace "projected-609" to be "Succeeded or Failed"
Dec 14 15:55:11.866: INFO: Pod "pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265": Phase="Pending", Reason="", readiness=false. Elapsed: 4.620805ms
Dec 14 15:55:13.884: INFO: Pod "pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023512888s
Dec 14 15:55:15.895: INFO: Pod "pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033546575s
STEP: Saw pod success
Dec 14 15:55:15.895: INFO: Pod "pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265" satisfied condition "Succeeded or Failed"
Dec 14 15:55:15.902: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:55:15.978: INFO: Waiting for pod pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265 to disappear
Dec 14 15:55:15.982: INFO: Pod pod-projected-configmaps-826d46d4-f808-41e8-b492-b0100c1be265 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 15:55:15.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-609" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":86,"skipped":1674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:16.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 15:55:16.054: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 15:55:16.066: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 15:55:16.071: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-1 before test
Dec 14 15:55:16.092: INFO: kube-flannel-ds-9s7dq from kube-flannel started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:55:16.092: INFO: kube-addon-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 15:55:16.092: INFO: kube-apiserver-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 15:55:16.092: INFO: kube-controller-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 15:55:16.092: INFO: kube-proxy-zp6s8 from kube-system started at 2022-12-14 15:34:55 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:55:16.092: INFO: kube-scheduler-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 15:55:16.092: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:55:16.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:55:16.092: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 15:55:16.092: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-2 before test
Dec 14 15:55:16.108: INFO: kube-flannel-ds-2jdlw from kube-flannel started at 2022-12-14 15:34:53 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:55:16.108: INFO: coredns-57575c5f89-h2qxf from kube-system started at 2022-12-14 15:34:51 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container coredns ready: true, restart count 0
Dec 14 15:55:16.108: INFO: kube-addon-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 15:55:16.108: INFO: kube-apiserver-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 15:55:16.108: INFO: kube-controller-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 15:55:16.108: INFO: kube-proxy-b847h from kube-system started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:55:16.108: INFO: kube-scheduler-queith7zooya-2 from kube-system started at 2022-12-14 15:29:31 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 15:55:16.108: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-rhr5t from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:55:16.108: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:55:16.108: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 15:55:16.108: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-3 before test
Dec 14 15:55:16.135: INFO: kube-flannel-ds-rfzcl from kube-flannel started at 2022-12-14 15:53:57 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.135: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 15:55:16.135: INFO: coredns-57575c5f89-qk6wh from kube-system started at 2022-12-14 15:53:56 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.135: INFO: 	Container coredns ready: true, restart count 0
Dec 14 15:55:16.135: INFO: kube-proxy-ms4r6 from kube-system started at 2022-12-14 15:34:52 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.135: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 15:55:16.135: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:35:54 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.135: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 15:55:16.135: INFO: sonobuoy-e2e-job-d6f98c81f7634948 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:55:16.135: INFO: 	Container e2e ready: true, restart count 0
Dec 14 15:55:16.135: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:55:16.136: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-dc4lx from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 15:55:16.136: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 15:55:16.136: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 15:55:16.136: INFO: taint-eviction-4 from taint-single-pod-2400 started at 2022-12-14 15:53:56 +0000 UTC (1 container statuses recorded)
Dec 14 15:55:16.136: INFO: 	Container pause ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node queith7zooya-1
STEP: verifying the node has the label node queith7zooya-2
STEP: verifying the node has the label node queith7zooya-3
Dec 14 15:55:16.255: INFO: Pod kube-flannel-ds-2jdlw requesting resource cpu=100m on Node queith7zooya-2
Dec 14 15:55:16.255: INFO: Pod kube-flannel-ds-9s7dq requesting resource cpu=100m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-flannel-ds-rfzcl requesting resource cpu=100m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod coredns-57575c5f89-h2qxf requesting resource cpu=100m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod coredns-57575c5f89-qk6wh requesting resource cpu=100m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod kube-addon-manager-queith7zooya-1 requesting resource cpu=5m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-addon-manager-queith7zooya-2 requesting resource cpu=5m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod kube-apiserver-queith7zooya-1 requesting resource cpu=250m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-apiserver-queith7zooya-2 requesting resource cpu=250m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod kube-controller-manager-queith7zooya-1 requesting resource cpu=200m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-controller-manager-queith7zooya-2 requesting resource cpu=200m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod kube-proxy-b847h requesting resource cpu=0m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod kube-proxy-ms4r6 requesting resource cpu=0m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod kube-proxy-zp6s8 requesting resource cpu=0m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-scheduler-queith7zooya-1 requesting resource cpu=100m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod kube-scheduler-queith7zooya-2 requesting resource cpu=100m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod sonobuoy requesting resource cpu=0m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod sonobuoy-e2e-job-d6f98c81f7634948 requesting resource cpu=0m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2 requesting resource cpu=0m on Node queith7zooya-1
Dec 14 15:55:16.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-dc4lx requesting resource cpu=0m on Node queith7zooya-3
Dec 14 15:55:16.256: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-rhr5t requesting resource cpu=0m on Node queith7zooya-2
Dec 14 15:55:16.256: INFO: Pod taint-eviction-4 requesting resource cpu=0m on Node queith7zooya-3
STEP: Starting Pods to consume most of the cluster CPU.
Dec 14 15:55:16.256: INFO: Creating a pod which consumes cpu=661m on Node queith7zooya-1
Dec 14 15:55:16.269: INFO: Creating a pod which consumes cpu=591m on Node queith7zooya-2
Dec 14 15:55:16.285: INFO: Creating a pod which consumes cpu=980m on Node queith7zooya-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0.1730b3f29b0e1e52], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6752/filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0 to queith7zooya-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0.1730b3f2b70532e9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0.1730b3f2bfc6affc], Reason = [Created], Message = [Created container filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0.1730b3f2c1da99bd], Reason = [Started], Message = [Started container filler-pod-03ca259f-54f2-4ad8-8ff4-2060f0821fa0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400.1730b3f2997f520b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6752/filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400 to queith7zooya-2]
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400.1730b3f2e1c21a52], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-n6vpd" : failed to sync configmap cache: timed out waiting for the condition]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400.1730b3f30ff5d014], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400.1730b3f315f14e1a], Reason = [Created], Message = [Created container filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400.1730b3f3170f61cb], Reason = [Started], Message = [Started container filler-pod-6c02bbf8-834a-4956-83eb-188f4e14b400]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1.1730b3f2980c1cfc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6752/filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1 to queith7zooya-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1.1730b3f2b51f4a6c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.7"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1.1730b3f31ae5dca4], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.7" in 1.707482506s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1.1730b3f32218fc01], Reason = [Created], Message = [Created container filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1.1730b3f323406bcb], Reason = [Started], Message = [Started container filler-pod-e4bb4e67-8ae8-4b3a-97cb-664c2ddc34c1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1730b3f38b1e9551], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.]
STEP: removing the label node off the node queith7zooya-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node queith7zooya-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node queith7zooya-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:55:21.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6752" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.472 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":87,"skipped":1733,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:21.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-e2204985-0e14-41d2-bf7f-d92d4d44e7a5
STEP: Creating a pod to test consume configMaps
Dec 14 15:55:21.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf" in namespace "configmap-5507" to be "Succeeded or Failed"
Dec 14 15:55:21.556: INFO: Pod "pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.250335ms
Dec 14 15:55:23.579: INFO: Pod "pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0280503s
Dec 14 15:55:25.595: INFO: Pod "pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044105808s
STEP: Saw pod success
Dec 14 15:55:25.596: INFO: Pod "pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf" satisfied condition "Succeeded or Failed"
Dec 14 15:55:25.602: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf container agnhost-container: <nil>
STEP: delete the pod
Dec 14 15:55:25.638: INFO: Waiting for pod pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf to disappear
Dec 14 15:55:25.646: INFO: Pod pod-configmaps-bedb4756-e9e3-461c-8515-0d939801c3cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 15:55:25.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1733,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:25.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:55:25.714: INFO: Waiting up to 5m0s for pod "downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894" in namespace "projected-8805" to be "Succeeded or Failed"
Dec 14 15:55:25.719: INFO: Pod "downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061899ms
Dec 14 15:55:27.735: INFO: Pod "downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021027314s
Dec 14 15:55:29.749: INFO: Pod "downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035397898s
STEP: Saw pod success
Dec 14 15:55:29.749: INFO: Pod "downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894" satisfied condition "Succeeded or Failed"
Dec 14 15:55:29.759: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894 container client-container: <nil>
STEP: delete the pod
Dec 14 15:55:29.793: INFO: Waiting for pod downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894 to disappear
Dec 14 15:55:29.798: INFO: Pod downwardapi-volume-143fbaa7-b7a8-4fb2-be93-55d029408894 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 15:55:29.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8805" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:29.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 15:55:43.947: INFO: DNS probes using dns-8029/dns-test-62637d38-f058-4586-9af6-256f857fc37c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 15:55:43.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8029" for this suite.

• [SLOW TEST:14.185 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":90,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:55:44.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 15:55:44.676: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 15:55:47.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:55:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-179" for this suite.
STEP: Destroying namespace "webhook-179-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:16.095 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":91,"skipped":1806,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:00.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 15:56:16.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3795" for this suite.

• [SLOW TEST:16.360 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":92,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:16.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-b90a64dc-1dfa-426e-8d1c-74e88b62d0ee
STEP: Creating a pod to test consume secrets
Dec 14 15:56:16.518: INFO: Waiting up to 5m0s for pod "pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a" in namespace "secrets-8266" to be "Succeeded or Failed"
Dec 14 15:56:16.525: INFO: Pod "pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.830909ms
Dec 14 15:56:18.538: INFO: Pod "pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020189311s
Dec 14 15:56:20.548: INFO: Pod "pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030442635s
STEP: Saw pod success
Dec 14 15:56:20.549: INFO: Pod "pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a" satisfied condition "Succeeded or Failed"
Dec 14 15:56:20.554: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 15:56:20.588: INFO: Waiting for pod pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a to disappear
Dec 14 15:56:20.595: INFO: Pod pod-secrets-c823ced2-00fd-4baa-8907-1b696141422a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 15:56:20.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8266" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":1847,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 14 15:56:20.674: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4320  9822439d-56f5-420c-a90a-1e9728587abd 9590 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 15:56:20.675: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4320  9822439d-56f5-420c-a90a-1e9728587abd 9591 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 14 15:56:20.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4320  9822439d-56f5-420c-a90a-1e9728587abd 9592 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 15:56:20.698: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4320  9822439d-56f5-420c-a90a-1e9728587abd 9593 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Dec 14 15:56:20.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4320" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":94,"skipped":1868,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:20.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:56:20.756: INFO: Creating deployment "webserver-deployment"
Dec 14 15:56:20.763: INFO: Waiting for observed generation 1
Dec 14 15:56:22.784: INFO: Waiting for all required pods to come up
Dec 14 15:56:22.793: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 14 15:56:24.812: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 14 15:56:24.824: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 14 15:56:24.846: INFO: Updating deployment webserver-deployment
Dec 14 15:56:24.846: INFO: Waiting for observed generation 2
Dec 14 15:56:26.865: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 14 15:56:26.871: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 14 15:56:26.875: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 15:56:26.891: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 14 15:56:26.891: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 14 15:56:26.895: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 14 15:56:26.902: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 14 15:56:26.902: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 14 15:56:26.919: INFO: Updating deployment webserver-deployment
Dec 14 15:56:26.919: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 14 15:56:26.928: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 14 15:56:28.939: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 15:56:28.956: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1495  3a695b1a-2c3a-4de3-9e9f-9ea31f40941d 9875 3 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a3a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 15:56:26 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2022-12-14 15:56:27 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 14 15:56:28.967: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-1495  67cf4d3e-886c-4503-8f86-2846cd38d2ad 9872 3 2022-12-14 15:56:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3a695b1a-2c3a-4de3-9e9f-9ea31f40941d 0xc000dfec87 0xc000dfec88}] []  [{kube-controller-manager Update apps/v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a695b1a-2c3a-4de3-9e9f-9ea31f40941d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dfed28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 15:56:28.967: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 14 15:56:28.967: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-1495  f3b2732e-3651-4a24-9e71-93905cb1ae7e 9866 3 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3a695b1a-2c3a-4de3-9e9f-9ea31f40941d 0xc000dfeb77 0xc000dfeb78}] []  [{kube-controller-manager Update apps/v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a695b1a-2c3a-4de3-9e9f-9ea31f40941d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 15:56:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dfec18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 14 15:56:28.979: INFO: Pod "webserver-deployment-55df494869-2cg22" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2cg22 webserver-deployment-55df494869- deployment-1495  88b13293-c660-4f22-b2d5-2478a137b5b2 9859 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000dffa60 0xc000dffa61}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnbnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnbnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.980: INFO: Pod "webserver-deployment-55df494869-2s9q9" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2s9q9 webserver-deployment-55df494869- deployment-1495  4f3289d6-08bd-419b-b872-979715ecbafa 9954 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000dffc80 0xc000dffc81}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xrc2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xrc2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.980: INFO: Pod "webserver-deployment-55df494869-4fbdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4fbdt webserver-deployment-55df494869- deployment-1495  fc5c8f09-f597-4e4c-b2e5-476307607232 9862 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000dffe47 0xc000dffe48}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rrvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rrvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.981: INFO: Pod "webserver-deployment-55df494869-625pp" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-625pp webserver-deployment-55df494869- deployment-1495  1414d027-0612-4fa1-85e5-1e29b6b62b5c 9879 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000dfffb0 0xc000dfffb1}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hs2mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hs2mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.981: INFO: Pod "webserver-deployment-55df494869-6nscl" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-6nscl webserver-deployment-55df494869- deployment-1495  f7b4cc48-6719-459c-b0ee-dd3f4427b542 9723 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd6317 0xc000fd6318}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7g8bc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7g8bc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.17,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://02c445dc5df5ac456f0c8597b41feb4cb0fffca7fa3fd2ed477dca6aec681d68,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.981: INFO: Pod "webserver-deployment-55df494869-7cq7l" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7cq7l webserver-deployment-55df494869- deployment-1495  bee12a73-ba0d-4c9e-a134-c78d2622e86d 9728 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd6500 0xc000fd6501}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9sjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9sjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.16,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c97816b826427308bad3e30ce3d603731674106ac68839c9f86ecba11ac47116,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.982: INFO: Pod "webserver-deployment-55df494869-8jlrd" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8jlrd webserver-deployment-55df494869- deployment-1495  8d204eb5-936e-4111-b47c-1806c33cde5b 9871 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd66e0 0xc000fd66e1}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njfxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njfxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.983: INFO: Pod "webserver-deployment-55df494869-h5z6b" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-h5z6b webserver-deployment-55df494869- deployment-1495  0a95930f-7b39-4153-830e-77de8ad98390 9869 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd68c7 0xc000fd68c8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmn5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmn5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.984: INFO: Pod "webserver-deployment-55df494869-hz6xr" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-hz6xr webserver-deployment-55df494869- deployment-1495  93e55998-ed0f-4377-a965-f1666fc74c56 9696 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd6c37 0xc000fd6c38}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzgxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzgxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.20,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://6fc074b47f7c78cc022b39fb8526f14f4b9a0526733978ef8250f0d72857115c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.984: INFO: Pod "webserver-deployment-55df494869-j87sp" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-j87sp webserver-deployment-55df494869- deployment-1495  a4d89d25-b54f-4d0b-963f-dbde7a423b14 9725 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7040 0xc000fd7041}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5h7bq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5h7bq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:10.233.65.18,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://05a1cd6ce65b7806e8703e061e080256cc3bd3963f058e78d81a673d5a117cf3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.984: INFO: Pod "webserver-deployment-55df494869-kcq25" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kcq25 webserver-deployment-55df494869- deployment-1495  681420a7-8dc5-49f8-9a9f-7741b518708a 9874 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7220 0xc000fd7221}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kv5sh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kv5sh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.985: INFO: Pod "webserver-deployment-55df494869-kpzgk" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kpzgk webserver-deployment-55df494869- deployment-1495  13d69e0f-c3cb-4b91-898a-524093de18bb 9692 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd73e7 0xc000fd73e8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pr2tw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pr2tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.19,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b6052e4194afb2a72c4ddc44f652375aece0c998518b797d7df5c662908f38ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.985: INFO: Pod "webserver-deployment-55df494869-nw2d8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-nw2d8 webserver-deployment-55df494869- deployment-1495  831136a5-4144-46ae-89cd-18ecb607ceb7 9716 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7600 0xc000fd7601}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cfklc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfklc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.90,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d9982227b1ff37cc730a123cdd554f14b9f782ea644d6be6431eda94d6e95079,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.985: INFO: Pod "webserver-deployment-55df494869-p7f9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-p7f9p webserver-deployment-55df494869- deployment-1495  1a624b28-3c03-4948-b805-0404c4d76094 9849 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd77e0 0xc000fd77e1}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxstb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxstb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.986: INFO: Pod "webserver-deployment-55df494869-qdqv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-qdqv6 webserver-deployment-55df494869- deployment-1495  56d9efe4-4ec5-4b3b-b0aa-9e0c9d171ff7 9826 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd79a7 0xc000fd79a8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmk4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmk4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.987: INFO: Pod "webserver-deployment-55df494869-qfzrm" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-qfzrm webserver-deployment-55df494869- deployment-1495  fd8c1417-d02a-4a24-8673-1f8485e0e032 9851 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7b87 0xc000fd7b88}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhpwr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhpwr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.987: INFO: Pod "webserver-deployment-55df494869-s2m6d" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-s2m6d webserver-deployment-55df494869- deployment-1495  7e505913-90c6-4acc-bf87-2e0cdb802855 9705 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7d57 0xc000fd7d58}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cx7n9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cx7n9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.91,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2ee9f3d7147bc676fac388a5a75a444312335a8d0c50e4855663eac2efc75c03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.988: INFO: Pod "webserver-deployment-55df494869-s5wq7" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-s5wq7 webserver-deployment-55df494869- deployment-1495  e15e6868-1370-4217-86c5-b0f3de71aab9 9885 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000fd7f40 0xc000fd7f41}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9xcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9xcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.990: INFO: Pod "webserver-deployment-55df494869-v69xj" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-v69xj webserver-deployment-55df494869- deployment-1495  52259748-5323-41cb-8843-3cf2a03c5ec4 9694 0 2022-12-14 15:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000aa8107 0xc000aa8108}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:10.233.64.18,StartTime:2022-12-14 15:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 15:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://853af00536736906b4f8def548ca61456c5240018816905fa1f62a573ca26786,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.991: INFO: Pod "webserver-deployment-55df494869-vllr2" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vllr2 webserver-deployment-55df494869- deployment-1495  262b7ad8-fab2-495d-9c9b-8066cc2068ee 9835 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 f3b2732e-3651-4a24-9e71-93905cb1ae7e 0xc000aa82f0 0xc000aa82f1}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3b2732e-3651-4a24-9e71-93905cb1ae7e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl4tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl4tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.991: INFO: Pod "webserver-deployment-57ccb67bb8-2f4t9" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-2f4t9 webserver-deployment-57ccb67bb8- deployment-1495  9d3a0ec4-7f3a-4fcb-83f3-19cbbdd47599 9895 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa84b7 0xc000aa84b8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hqqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hqqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.991: INFO: Pod "webserver-deployment-57ccb67bb8-65mpd" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-65mpd webserver-deployment-57ccb67bb8- deployment-1495  98acb137-092d-408d-a9a2-16102fca4f9a 9755 0 2022-12-14 15:56:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa86b7 0xc000aa86b8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwlzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwlzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.991: INFO: Pod "webserver-deployment-57ccb67bb8-6g78s" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-6g78s webserver-deployment-57ccb67bb8- deployment-1495  dce525c0-3a69-47cb-ab45-5bc4dd85f67d 9762 0 2022-12-14 15:56:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa88a7 0xc000aa88a8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5q2xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5q2xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.992: INFO: Pod "webserver-deployment-57ccb67bb8-b4hjf" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-b4hjf webserver-deployment-57ccb67bb8- deployment-1495  fec2bb51-c9d9-41c2-8717-83a814be3d67 9867 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa8a97 0xc000aa8a98}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twqxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twqxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.992: INFO: Pod "webserver-deployment-57ccb67bb8-csl7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-csl7b webserver-deployment-57ccb67bb8- deployment-1495  517d7e77-0acf-45db-86a8-ca07a30647b6 9850 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa8c10 0xc000aa8c11}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tj455,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tj455,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.992: INFO: Pod "webserver-deployment-57ccb67bb8-kn84z" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-kn84z webserver-deployment-57ccb67bb8- deployment-1495  0769a244-e987-4929-8613-82b2b762dd92 9782 0 2022-12-14 15:56:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa8e07 0xc000aa8e08}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlsj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlsj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.993: INFO: Pod "webserver-deployment-57ccb67bb8-lvlr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-lvlr5 webserver-deployment-57ccb67bb8- deployment-1495  3879c7e1-870f-45b2-b974-182007a97b57 9780 0 2022-12-14 15:56:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa8ff7 0xc000aa8ff8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq55m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq55m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.994: INFO: Pod "webserver-deployment-57ccb67bb8-mw4td" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mw4td webserver-deployment-57ccb67bb8- deployment-1495  c5ef96b9-0b2e-4307-845a-22e085e238d6 9953 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa91f7 0xc000aa91f8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lv2qt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lv2qt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.994: INFO: Pod "webserver-deployment-57ccb67bb8-qmqcq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-qmqcq webserver-deployment-57ccb67bb8- deployment-1495  ef60ba65-7f15-468b-8173-7e9f3009ccff 9893 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa93e7 0xc000aa93e8}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mq5gh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mq5gh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.995: INFO: Pod "webserver-deployment-57ccb67bb8-tn7jg" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-tn7jg webserver-deployment-57ccb67bb8- deployment-1495  8bbf4186-869b-40b6-8e9d-f03d732894ce 9753 0 2022-12-14 15:56:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc000aa9777 0xc000aa9778}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf2vx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf2vx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 15:56:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.995: INFO: Pod "webserver-deployment-57ccb67bb8-vdnkd" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-vdnkd webserver-deployment-57ccb67bb8- deployment-1495  d8c582ad-aa99-441c-80df-815d93a3af51 9955 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc00040c247 0xc00040c248}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ft8d9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ft8d9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.209,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.996: INFO: Pod "webserver-deployment-57ccb67bb8-wchn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wchn6 webserver-deployment-57ccb67bb8- deployment-1495  30a0ea58-e46d-45ec-bda5-c0aaa89b01de 9840 0 2022-12-14 15:56:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc00040c757 0xc00040c758}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v6txk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v6txk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 15:56:28.996: INFO: Pod "webserver-deployment-57ccb67bb8-zq7jt" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-zq7jt webserver-deployment-57ccb67bb8- deployment-1495  60f32f21-670e-437e-baba-357ef56b911a 9881 0 2022-12-14 15:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 67cf4d3e-886c-4503-8f86-2846cd38d2ad 0xc00040c977 0xc00040c978}] []  [{kube-controller-manager Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67cf4d3e-886c-4503-8f86-2846cd38d2ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 15:56:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmd9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmd9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 15:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.111,PodIP:,StartTime:2022-12-14 15:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 15:56:28.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1495" for this suite.

• [SLOW TEST:8.320 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":95,"skipped":1880,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 14 15:56:29.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 15:56:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 15:56:47.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2368" for this suite.

• [SLOW TEST:18.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":96,"skipped":1891,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:47.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:56:47.806: INFO: The status of Pod pod-secrets-a1bf6fc6-b29b-42b1-bed5-beb497827afa is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:56:49.821: INFO: The status of Pod pod-secrets-a1bf6fc6-b29b-42b1-bed5-beb497827afa is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Dec 14 15:56:49.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3470" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":97,"skipped":1901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:49.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Dec 14 15:56:49.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8133" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":98,"skipped":1935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:56:50.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 15:57:06.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-319" for this suite.

• [SLOW TEST:16.315 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":99,"skipped":1968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:57:06.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Dec 14 15:57:06.389: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Dec 14 15:57:08.405: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Dec 14 15:57:09.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6573" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":100,"skipped":1992,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:57:09.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Dec 14 15:57:09.507: INFO: Waiting up to 5m0s for pod "var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867" in namespace "var-expansion-5087" to be "Succeeded or Failed"
Dec 14 15:57:09.511: INFO: Pod "var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963387ms
Dec 14 15:57:11.527: INFO: Pod "var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019631414s
Dec 14 15:57:13.544: INFO: Pod "var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0365219s
STEP: Saw pod success
Dec 14 15:57:13.544: INFO: Pod "var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867" satisfied condition "Succeeded or Failed"
Dec 14 15:57:13.551: INFO: Trying to get logs from node queith7zooya-3 pod var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867 container dapi-container: <nil>
STEP: delete the pod
Dec 14 15:57:13.603: INFO: Waiting for pod var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867 to disappear
Dec 14 15:57:13.607: INFO: Pod var-expansion-d6483e73-4adc-40c9-a5cd-7af2337ef867 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 15:57:13.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5087" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":101,"skipped":2010,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:57:13.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 15:57:13.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5" in namespace "downward-api-1579" to be "Succeeded or Failed"
Dec 14 15:57:13.689: INFO: Pod "downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.419603ms
Dec 14 15:57:15.702: INFO: Pod "downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5": Phase="Running", Reason="", readiness=false. Elapsed: 2.021225737s
Dec 14 15:57:17.718: INFO: Pod "downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037060104s
STEP: Saw pod success
Dec 14 15:57:17.718: INFO: Pod "downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5" satisfied condition "Succeeded or Failed"
Dec 14 15:57:17.723: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5 container client-container: <nil>
STEP: delete the pod
Dec 14 15:57:17.752: INFO: Waiting for pod downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5 to disappear
Dec 14 15:57:17.756: INFO: Pod downwardapi-volume-45697912-c13a-4894-bf9e-52204c6585e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 15:57:17.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1579" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":102,"skipped":2012,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:57:17.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Dec 14 15:57:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8571" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":103,"skipped":2022,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:57:17.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 15:57:18.040: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 15:58:18.087: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:58:18.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 15:58:18.166: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Dec 14 15:58:18.170: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Dec 14 15:58:18.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6441" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Dec 14 15:58:18.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7908" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.334 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":104,"skipped":2036,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:58:18.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 15:59:18.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2011" for this suite.

• [SLOW TEST:60.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":2045,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:59:18.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Dec 14 15:59:20.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1056" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":106,"skipped":2056,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:59:20.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6249.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6249.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.152_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6249.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6249.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6249.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6249.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6249.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 152.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.152_udp@PTR;check="$$(dig +tcp +noall +answer +search 152.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.152_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 15:59:22.678: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.683: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.695: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.721: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.726: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.730: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.734: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:22.749: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:27.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.779: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.805: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.815: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.819: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:27.840: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:32.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.778: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.784: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.825: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.843: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.848: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:32.883: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:37.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.770: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.778: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.817: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.839: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.845: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:37.892: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:42.766: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.773: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.780: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.785: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.817: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.834: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:42.868: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:47.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.768: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.779: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.815: INFO: Unable to read jessie_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.829: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.834: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:47.856: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_udp@dns-test-service.dns-6249.svc.cluster.local jessie_tcp@dns-test-service.dns-6249.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:52.757: INFO: Unable to read wheezy_udp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:52.768: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:52.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:52.777: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local from pod dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a: the server could not find the requested resource (get pods dns-test-25392b0c-e76e-4400-801a-80680332fd5a)
Dec 14 15:59:52.843: INFO: Lookups using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a failed for: [wheezy_udp@dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@dns-test-service.dns-6249.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6249.svc.cluster.local]

Dec 14 15:59:57.861: INFO: DNS probes using dns-6249/dns-test-25392b0c-e76e-4400-801a-80680332fd5a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 15:59:58.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6249" for this suite.

• [SLOW TEST:37.541 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":107,"skipped":2059,"failed":0}
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 15:59:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-9cf101a2-f716-495c-b68e-e89d887ac3e5
STEP: Creating a pod to test consume secrets
Dec 14 15:59:58.170: INFO: Waiting up to 5m0s for pod "pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046" in namespace "secrets-6479" to be "Succeeded or Failed"
Dec 14 15:59:58.180: INFO: Pod "pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046": Phase="Pending", Reason="", readiness=false. Elapsed: 9.192928ms
Dec 14 16:00:00.195: INFO: Pod "pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024365924s
Dec 14 16:00:02.216: INFO: Pod "pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045989362s
STEP: Saw pod success
Dec 14 16:00:02.217: INFO: Pod "pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046" satisfied condition "Succeeded or Failed"
Dec 14 16:00:02.224: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046 container secret-env-test: <nil>
STEP: delete the pod
Dec 14 16:00:02.269: INFO: Waiting for pod pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046 to disappear
Dec 14 16:00:02.274: INFO: Pod pod-secrets-aab5f55f-4b27-466f-a1eb-e689c24a0046 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:00:02.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6479" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":108,"skipped":2059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Dec 14 16:00:02.357: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:00:07.374: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 16:00:07.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-709" for this suite.

• [SLOW TEST:5.158 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":109,"skipped":2103,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:07.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:00:07.522: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:00:12.544: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Dec 14 16:00:12.560: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Dec 14 16:00:12.579: INFO: observed ReplicaSet test-rs in namespace replicaset-3501 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:00:12.636: INFO: observed ReplicaSet test-rs in namespace replicaset-3501 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:00:12.672: INFO: observed ReplicaSet test-rs in namespace replicaset-3501 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:00:12.726: INFO: observed ReplicaSet test-rs in namespace replicaset-3501 with ReadyReplicas 1, AvailableReplicas 1
Dec 14 16:00:13.914: INFO: observed ReplicaSet test-rs in namespace replicaset-3501 with ReadyReplicas 2, AvailableReplicas 2
Dec 14 16:00:14.546: INFO: observed Replicaset test-rs in namespace replicaset-3501 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 16:00:14.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3501" for this suite.

• [SLOW TEST:7.111 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":110,"skipped":2112,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:14.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 16:00:14.612: INFO: Waiting up to 5m0s for pod "pod-5132eaf4-2e1d-4135-bafb-c0c867669041" in namespace "emptydir-2905" to be "Succeeded or Failed"
Dec 14 16:00:14.615: INFO: Pod "pod-5132eaf4-2e1d-4135-bafb-c0c867669041": Phase="Pending", Reason="", readiness=false. Elapsed: 3.607247ms
Dec 14 16:00:16.635: INFO: Pod "pod-5132eaf4-2e1d-4135-bafb-c0c867669041": Phase="Running", Reason="", readiness=false. Elapsed: 2.023219504s
Dec 14 16:00:18.652: INFO: Pod "pod-5132eaf4-2e1d-4135-bafb-c0c867669041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040114187s
STEP: Saw pod success
Dec 14 16:00:18.652: INFO: Pod "pod-5132eaf4-2e1d-4135-bafb-c0c867669041" satisfied condition "Succeeded or Failed"
Dec 14 16:00:18.657: INFO: Trying to get logs from node queith7zooya-3 pod pod-5132eaf4-2e1d-4135-bafb-c0c867669041 container test-container: <nil>
STEP: delete the pod
Dec 14 16:00:18.695: INFO: Waiting for pod pod-5132eaf4-2e1d-4135-bafb-c0c867669041 to disappear
Dec 14 16:00:18.700: INFO: Pod pod-5132eaf4-2e1d-4135-bafb-c0c867669041 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:00:18.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2905" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":2119,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:18.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Dec 14 16:00:18.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6437" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":112,"skipped":2129,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:18.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 16:00:18.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6604" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":113,"skipped":2139,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:18.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-48b8cabc-a254-4ea9-aabd-45c8b0d0650c
STEP: Creating a pod to test consume configMaps
Dec 14 16:00:18.966: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707" in namespace "projected-9894" to be "Succeeded or Failed"
Dec 14 16:00:18.971: INFO: Pod "pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707": Phase="Pending", Reason="", readiness=false. Elapsed: 5.616118ms
Dec 14 16:00:20.988: INFO: Pod "pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022171663s
Dec 14 16:00:23.006: INFO: Pod "pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040493438s
STEP: Saw pod success
Dec 14 16:00:23.006: INFO: Pod "pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707" satisfied condition "Succeeded or Failed"
Dec 14 16:00:23.013: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:00:23.043: INFO: Waiting for pod pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707 to disappear
Dec 14 16:00:23.048: INFO: Pod pod-projected-configmaps-63307065-5912-4c82-8f75-632458980707 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 16:00:23.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9894" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":2143,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:23.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-f32011c1-4a8e-421c-bed1-ba1af5ff4c2b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 16:00:25.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4159" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":115,"skipped":2148,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:25.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Dec 14 16:00:31.423: INFO: 81 pods remaining
Dec 14 16:00:31.423: INFO: 81 pods has nil DeletionTimestamp
Dec 14 16:00:31.423: INFO: 
Dec 14 16:00:32.517: INFO: 74 pods remaining
Dec 14 16:00:32.517: INFO: 72 pods has nil DeletionTimestamp
Dec 14 16:00:32.517: INFO: 
Dec 14 16:00:33.399: INFO: 60 pods remaining
Dec 14 16:00:33.399: INFO: 60 pods has nil DeletionTimestamp
Dec 14 16:00:33.399: INFO: 
Dec 14 16:00:34.400: INFO: 41 pods remaining
Dec 14 16:00:34.400: INFO: 41 pods has nil DeletionTimestamp
Dec 14 16:00:34.400: INFO: 
Dec 14 16:00:35.417: INFO: 33 pods remaining
Dec 14 16:00:35.417: INFO: 32 pods has nil DeletionTimestamp
Dec 14 16:00:35.417: INFO: 
Dec 14 16:00:36.453: INFO: 20 pods remaining
Dec 14 16:00:36.453: INFO: 20 pods has nil DeletionTimestamp
Dec 14 16:00:36.453: INFO: 
Dec 14 16:00:37.392: INFO: 0 pods remaining
Dec 14 16:00:37.392: INFO: 0 pods has nil DeletionTimestamp
Dec 14 16:00:37.392: INFO: 
STEP: Gathering metrics
Dec 14 16:00:38.413: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 16:00:38.695: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 16:00:38.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4578" for this suite.

• [SLOW TEST:13.478 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":116,"skipped":2153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:00:38.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-40aa6ef5-e64f-44e1-8b73-716f47501afc
STEP: Creating secret with name s-test-opt-upd-e1d3e59e-3c23-4ff8-845e-827367a2ffb3
STEP: Creating the pod
Dec 14 16:00:38.912: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:00:40.921: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:00:42.927: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:00:44.922: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:00:46.928: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:00:48.929: INFO: The status of Pod pod-secrets-41cccfca-ac5a-4f4c-b0a3-1b41ca195352 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-40aa6ef5-e64f-44e1-8b73-716f47501afc
STEP: Updating secret s-test-opt-upd-e1d3e59e-3c23-4ff8-845e-827367a2ffb3
STEP: Creating secret with name s-test-opt-create-d38b4146-6d8a-42dc-847a-2fa402c6764b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:02:09.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9707" for this suite.

• [SLOW TEST:91.203 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":117,"skipped":2202,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:02:09.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 14 16:02:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:02:13.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:02:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1449" for this suite.

• [SLOW TEST:20.455 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":118,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:02:30.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Dec 14 16:02:30.470: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Dec 14 16:02:32.500: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Dec 14 16:02:34.542: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Dec 14 16:02:36.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9694" for this suite.

• [SLOW TEST:6.182 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":119,"skipped":2228,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:02:36.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Dec 14 16:02:36.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:02:56.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3972" for this suite.

• [SLOW TEST:20.152 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":120,"skipped":2229,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:02:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-eb612677-21b3-4d38-b4cd-885e50723269 in namespace container-probe-7074
Dec 14 16:02:58.809: INFO: Started pod liveness-eb612677-21b3-4d38-b4cd-885e50723269 in namespace container-probe-7074
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:02:58.815: INFO: Initial restart count of pod liveness-eb612677-21b3-4d38-b4cd-885e50723269 is 0
Dec 14 16:03:18.992: INFO: Restart count of pod container-probe-7074/liveness-eb612677-21b3-4d38-b4cd-885e50723269 is now 1 (20.176283693s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:03:19.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7074" for this suite.

• [SLOW TEST:22.301 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":121,"skipped":2239,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:03:19.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7793
Dec 14 16:03:19.077: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:03:21.097: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Dec 14 16:03:21.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Dec 14 16:03:21.565: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Dec 14 16:03:21.566: INFO: stdout: "iptables"
Dec 14 16:03:21.566: INFO: proxyMode: iptables
Dec 14 16:03:21.591: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Dec 14 16:03:21.596: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7793
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7793
I1214 16:03:21.643687      14 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7793, replica count: 3
I1214 16:03:24.694979      14 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:03:24.716: INFO: Creating new exec pod
Dec 14 16:03:27.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Dec 14 16:03:28.052: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Dec 14 16:03:28.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:03:28.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.34.121 80'
Dec 14 16:03:28.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.34.121 80\nConnection to 10.233.34.121 80 port [tcp/http] succeeded!\n"
Dec 14 16:03:28.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:03:28.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.209 30844'
Dec 14 16:03:28.519: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.209 30844\nConnection to 192.168.121.209 30844 port [tcp/*] succeeded!\n"
Dec 14 16:03:28.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:03:28.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 30844'
Dec 14 16:03:28.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 30844\nConnection to 192.168.121.248 30844 port [tcp/*] succeeded!\n"
Dec 14 16:03:28.728: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:03:28.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.209:30844/ ; done'
Dec 14 16:03:29.095: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n"
Dec 14 16:03:29.095: INFO: stdout: "\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk\naffinity-nodeport-timeout-q9kjk"
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.095: INFO: Received response from host: affinity-nodeport-timeout-q9kjk
Dec 14 16:03:29.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.209:30844/'
Dec 14 16:03:29.452: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n"
Dec 14 16:03:29.452: INFO: stdout: "affinity-nodeport-timeout-q9kjk"
Dec 14 16:03:49.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-7793 exec execpod-affinitytkx4x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.121.209:30844/'
Dec 14 16:03:49.737: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.121.209:30844/\n"
Dec 14 16:03:49.737: INFO: stdout: "affinity-nodeport-timeout-9t69t"
Dec 14 16:03:49.737: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7793, will wait for the garbage collector to delete the pods
Dec 14 16:03:49.840: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 16.809234ms
Dec 14 16:03:49.940: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.833397ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:03:51.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7793" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:32.865 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":122,"skipped":2240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:03:51.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:03:51.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 create -f -'
Dec 14 16:03:52.851: INFO: stderr: ""
Dec 14 16:03:52.851: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Dec 14 16:03:52.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 create -f -'
Dec 14 16:03:53.151: INFO: stderr: ""
Dec 14 16:03:53.151: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 16:03:54.163: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:03:54.163: INFO: Found 0 / 1
Dec 14 16:03:55.163: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:03:55.163: INFO: Found 1 / 1
Dec 14 16:03:55.163: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 16:03:55.167: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:03:55.167: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:03:55.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 describe pod agnhost-primary-24ntd'
Dec 14 16:03:55.395: INFO: stderr: ""
Dec 14 16:03:55.395: INFO: stdout: "Name:         agnhost-primary-24ntd\nNamespace:    kubectl-6528\nPriority:     0\nNode:         queith7zooya-3/192.168.121.248\nStart Time:   Wed, 14 Dec 2022 16:03:52 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.233.66.154\nIPs:\n  IP:           10.233.66.154\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://92bea8e8d4f52b3f2cf5d1c32054cf4298806f8921dbe5f00c7c4480c6af4fb4\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Dec 2022 16:03:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qt9rn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qt9rn:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-6528/agnhost-primary-24ntd to queith7zooya-3\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Dec 14 16:03:55.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 describe rc agnhost-primary'
Dec 14 16:03:55.626: INFO: stderr: ""
Dec 14 16:03:55.626: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6528\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-24ntd\n"
Dec 14 16:03:55.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 describe service agnhost-primary'
Dec 14 16:03:55.800: INFO: stderr: ""
Dec 14 16:03:55.800: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6528\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.36.133\nIPs:               10.233.36.133\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.154:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 14 16:03:55.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 describe node queith7zooya-1'
Dec 14 16:03:56.035: INFO: stderr: ""
Dec 14 16:03:56.035: INFO: stdout: "Name:               queith7zooya-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=queith7zooya-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"76:4a:ae:91:84:9d\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.209\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Dec 2022 15:26:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  queith7zooya-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 14 Dec 2022 16:03:47 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 14 Dec 2022 15:34:59 +0000   Wed, 14 Dec 2022 15:34:59 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 14 Dec 2022 16:00:43 +0000   Wed, 14 Dec 2022 15:25:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 14 Dec 2022 16:00:43 +0000   Wed, 14 Dec 2022 15:25:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 14 Dec 2022 16:00:43 +0000   Wed, 14 Dec 2022 15:25:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 14 Dec 2022 16:00:43 +0000   Wed, 14 Dec 2022 15:29:12 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.209\n  Hostname:    queith7zooya-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  122749536Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8140760Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  119410748528\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3291096Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3490f37cf2554f6791e06ea981b4f482\n  System UUID:                3490f37c-f255-4f67-91e0-6ea981b4f482\n  Boot ID:                    9c80e865-8135-4c51-a8a2-11d93a94f002\n  Kernel Version:             5.15.0-56-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.24.3\n  Kubelet Version:            v1.24.9\n  Kube-Proxy Version:         v1.24.9\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-9s7dq                                      100m (6%)     100m (6%)   50Mi (1%)        50Mi (1%)      29m\n  kube-system                 kube-addon-manager-queith7zooya-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         29m\n  kube-system                 kube-apiserver-queith7zooya-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-controller-manager-queith7zooya-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-zp6s8                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-scheduler-queith7zooya-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         29m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         27m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                655m (40%)  100m (6%)\n  memory             100Mi (3%)  50Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 37m                kube-proxy       \n  Normal  Starting                 29m                kube-proxy       \n  Normal  Starting                 28m                kube-proxy       \n  Normal  NodeHasSufficientMemory  38m (x6 over 38m)  kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    38m (x5 over 38m)  kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     38m (x5 over 38m)  kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  Starting                 37m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  37m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    37m                kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     37m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  37m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                37m                kubelet          Node queith7zooya-1 status is now: NodeReady\n  Normal  RegisteredNode           37m                node-controller  Node queith7zooya-1 event: Registered Node queith7zooya-1 in Controller\n  Normal  Starting                 37m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  37m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    37m                kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     37m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             37m                kubelet          Node queith7zooya-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  37m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                37m                kubelet          Node queith7zooya-1 status is now: NodeReady\n  Normal  NodeReady                34m                kubelet          Node queith7zooya-1 status is now: NodeReady\n  Normal  NodeHasSufficientMemory  34m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    34m                kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     34m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             34m                kubelet          Node queith7zooya-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  34m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 34m                kubelet          Starting kubelet.\n  Normal  NodeNotReady             34m                kubelet          Node queith7zooya-1 status is now: NodeNotReady\n  Normal  NodeHasSufficientMemory  34m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    34m                kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 34m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientPID     34m                kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  34m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                34m                kubelet          Node queith7zooya-1 status is now: NodeReady\n  Normal  RegisteredNode           30m                node-controller  Node queith7zooya-1 event: Registered Node queith7zooya-1 in Controller\n  Normal  RegisteredNode           29m                node-controller  Node queith7zooya-1 event: Registered Node queith7zooya-1 in Controller\n  Normal  NodeHasSufficientMemory  29m (x8 over 29m)  kubelet          Node queith7zooya-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    29m (x8 over 29m)  kubelet          Node queith7zooya-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     29m (x7 over 29m)  kubelet          Node queith7zooya-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  29m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 29m                kubelet          Starting kubelet.\n"
Dec 14 16:03:56.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6528 describe namespace kubectl-6528'
Dec 14 16:03:56.156: INFO: stderr: ""
Dec 14 16:03:56.156: INFO: stdout: "Name:         kubectl-6528\nLabels:       e2e-framework=kubectl\n              e2e-run=e50888bf-6310-4046-976d-a2bdad20bdfe\n              kubernetes.io/metadata.name=kubectl-6528\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:03:56.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6528" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":123,"skipped":2273,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:03:56.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 16:04:00.296: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Dec 14 16:04:00.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2154" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":124,"skipped":2286,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-597.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-597.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:04:04.458: INFO: DNS probes using dns-597/dns-test-a34070a0-1b40-487b-841a-8a5a34a07bf7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:04:04.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-597" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":125,"skipped":2288,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:04.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:04:04.606: INFO: created pod pod-service-account-defaultsa
Dec 14 16:04:04.606: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 14 16:04:04.613: INFO: created pod pod-service-account-mountsa
Dec 14 16:04:04.613: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 14 16:04:04.628: INFO: created pod pod-service-account-nomountsa
Dec 14 16:04:04.628: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 14 16:04:04.640: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 14 16:04:04.640: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 14 16:04:04.656: INFO: created pod pod-service-account-mountsa-mountspec
Dec 14 16:04:04.656: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 14 16:04:04.689: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 14 16:04:04.689: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 14 16:04:04.719: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 14 16:04:04.719: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 14 16:04:04.739: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 14 16:04:04.740: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 14 16:04:04.774: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 14 16:04:04.774: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 16:04:04.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9463" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":126,"skipped":2296,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Dec 14 16:04:04.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7511" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":127,"skipped":2327,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:04.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-1243
STEP: creating service affinity-nodeport-transition in namespace services-1243
STEP: creating replication controller affinity-nodeport-transition in namespace services-1243
I1214 16:04:05.024780      14 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1243, replica count: 3
I1214 16:04:08.077522      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 16:04:11.077994      14 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:04:11.109: INFO: Creating new exec pod
Dec 14 16:04:14.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Dec 14 16:04:14.410: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Dec 14 16:04:14.410: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:04:14.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.12.174 80'
Dec 14 16:04:14.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.12.174 80\nConnection to 10.233.12.174 80 port [tcp/http] succeeded!\n"
Dec 14 16:04:14.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:04:14.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.111 32470'
Dec 14 16:04:14.781: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.111 32470\nConnection to 192.168.121.111 32470 port [tcp/*] succeeded!\n"
Dec 14 16:04:14.781: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:04:14.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 32470'
Dec 14 16:04:14.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 32470\nConnection to 192.168.121.248 32470 port [tcp/*] succeeded!\n"
Dec 14 16:04:14.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:04:15.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.209:32470/ ; done'
Dec 14 16:04:15.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n"
Dec 14 16:04:15.631: INFO: stdout: "\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-zrc86\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-n8926\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq"
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-zrc86
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-n8926
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.631: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:15.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1243 exec execpod-affinitybtmfh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.209:32470/ ; done'
Dec 14 16:04:16.239: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32470/\n"
Dec 14 16:04:16.239: INFO: stdout: "\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq\naffinity-nodeport-transition-t9lbq"
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.239: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.240: INFO: Received response from host: affinity-nodeport-transition-t9lbq
Dec 14 16:04:16.240: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1243, will wait for the garbage collector to delete the pods
Dec 14 16:04:16.339: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.675154ms
Dec 14 16:04:16.440: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.287157ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:04:18.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1243" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:14.002 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":128,"skipped":2346,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4655
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 16:04:18.947: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 16:04:19.020: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:04:21.029: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:04:23.032: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:04:25.032: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:04:27.037: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:04:29.029: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:04:31.038: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 16:04:31.049: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 16:04:31.060: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 16:04:33.117: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 16:04:33.117: INFO: Breadth first check of 10.233.64.64 on host 192.168.121.209...
Dec 14 16:04:33.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.65:9080/dial?request=hostname&protocol=udp&host=10.233.64.64&port=8081&tries=1'] Namespace:pod-network-test-4655 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:04:33.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:04:33.134: INFO: ExecWithOptions: Clientset creation
Dec 14 16:04:33.134: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4655/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.64.65%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.64%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:04:33.273: INFO: Waiting for responses: map[]
Dec 14 16:04:33.273: INFO: reached 10.233.64.64 after 0/1 tries
Dec 14 16:04:33.273: INFO: Breadth first check of 10.233.65.61 on host 192.168.121.111...
Dec 14 16:04:33.280: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.65:9080/dial?request=hostname&protocol=udp&host=10.233.65.61&port=8081&tries=1'] Namespace:pod-network-test-4655 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:04:33.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:04:33.281: INFO: ExecWithOptions: Clientset creation
Dec 14 16:04:33.281: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4655/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.64.65%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.61%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:04:33.389: INFO: Waiting for responses: map[]
Dec 14 16:04:33.389: INFO: reached 10.233.65.61 after 0/1 tries
Dec 14 16:04:33.389: INFO: Breadth first check of 10.233.66.160 on host 192.168.121.248...
Dec 14 16:04:33.395: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.65:9080/dial?request=hostname&protocol=udp&host=10.233.66.160&port=8081&tries=1'] Namespace:pod-network-test-4655 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:04:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:04:33.397: INFO: ExecWithOptions: Clientset creation
Dec 14 16:04:33.397: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4655/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.64.65%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.160%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Dec 14 16:04:33.501: INFO: Waiting for responses: map[]
Dec 14 16:04:33.501: INFO: reached 10.233.66.160 after 0/1 tries
Dec 14 16:04:33.501: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Dec 14 16:04:33.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4655" for this suite.

• [SLOW TEST:14.610 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2355,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:33.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 16:04:33.591: INFO: Waiting up to 5m0s for pod "pod-1b25f59c-adae-4b6d-a24a-366985ee598a" in namespace "emptydir-4027" to be "Succeeded or Failed"
Dec 14 16:04:33.598: INFO: Pod "pod-1b25f59c-adae-4b6d-a24a-366985ee598a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.265488ms
Dec 14 16:04:35.624: INFO: Pod "pod-1b25f59c-adae-4b6d-a24a-366985ee598a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032750235s
Dec 14 16:04:37.642: INFO: Pod "pod-1b25f59c-adae-4b6d-a24a-366985ee598a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049954271s
STEP: Saw pod success
Dec 14 16:04:37.642: INFO: Pod "pod-1b25f59c-adae-4b6d-a24a-366985ee598a" satisfied condition "Succeeded or Failed"
Dec 14 16:04:37.647: INFO: Trying to get logs from node queith7zooya-2 pod pod-1b25f59c-adae-4b6d-a24a-366985ee598a container test-container: <nil>
STEP: delete the pod
Dec 14 16:04:37.689: INFO: Waiting for pod pod-1b25f59c-adae-4b6d-a24a-366985ee598a to disappear
Dec 14 16:04:37.697: INFO: Pod pod-1b25f59c-adae-4b6d-a24a-366985ee598a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:04:37.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4027" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":130,"skipped":2373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:37.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec 14 16:04:38.885: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 16:04:39.058: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 16:04:39.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3953" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":131,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:39.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:04:39.217: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f" in namespace "security-context-test-5409" to be "Succeeded or Failed"
Dec 14 16:04:39.224: INFO: Pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.829403ms
Dec 14 16:04:41.234: INFO: Pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017374674s
Dec 14 16:04:43.247: INFO: Pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030042969s
Dec 14 16:04:45.256: INFO: Pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039498395s
Dec 14 16:04:45.256: INFO: Pod "alpine-nnp-false-9b026314-e23c-44d3-8054-dc37fabb795f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 16:04:45.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5409" for this suite.

• [SLOW TEST:6.210 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2527,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:04:45.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 16:04:45.364: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:05:45.412: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Dec 14 16:05:45.480: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 16:05:45.493: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 16:05:45.529: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 16:05:45.539: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 14 16:05:45.574: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 14 16:05:45.584: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:06:01.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2893" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.515 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":133,"skipped":2538,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:06:01.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-205, will wait for the garbage collector to delete the pods
Dec 14 16:06:05.988: INFO: Deleting Job.batch foo took: 14.704976ms
Dec 14 16:06:06.089: INFO: Terminating Job.batch foo pods took: 100.992005ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Dec 14 16:06:38.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-205" for this suite.

• [SLOW TEST:37.013 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":134,"skipped":2563,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:06:38.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 16:06:38.875: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 16:06:38.889: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 16:06:38.894: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-1 before test
Dec 14 16:06:38.906: INFO: kube-flannel-ds-9s7dq from kube-flannel started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.906: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:06:38.907: INFO: kube-addon-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.907: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:06:38.908: INFO: kube-apiserver-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.908: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:06:38.908: INFO: kube-controller-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.908: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 16:06:38.908: INFO: kube-proxy-zp6s8 from kube-system started at 2022-12-14 15:34:55 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.908: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:06:38.909: INFO: kube-scheduler-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.909: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 16:06:38.909: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:06:38.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:06:38.909: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:06:38.909: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-2 before test
Dec 14 16:06:38.922: INFO: kube-flannel-ds-2jdlw from kube-flannel started at 2022-12-14 15:34:53 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.922: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:06:38.922: INFO: coredns-57575c5f89-h2qxf from kube-system started at 2022-12-14 15:34:51 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.922: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:06:38.922: INFO: kube-addon-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.922: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:06:38.922: INFO: kube-apiserver-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.922: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:06:38.923: INFO: kube-controller-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.923: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 16:06:38.923: INFO: kube-proxy-b847h from kube-system started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.923: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:06:38.923: INFO: kube-scheduler-queith7zooya-2 from kube-system started at 2022-12-14 15:29:31 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.923: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 16:06:38.923: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-rhr5t from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:06:38.923: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:06:38.923: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:06:38.923: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-3 before test
Dec 14 16:06:38.933: INFO: kube-flannel-ds-rfzcl from kube-flannel started at 2022-12-14 15:53:57 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:06:38.933: INFO: coredns-57575c5f89-qk6wh from kube-system started at 2022-12-14 15:53:56 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:06:38.933: INFO: kube-proxy-ms4r6 from kube-system started at 2022-12-14 15:34:52 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:06:38.933: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:35:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 16:06:38.933: INFO: sonobuoy-e2e-job-d6f98c81f7634948 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container e2e ready: true, restart count 0
Dec 14 16:06:38.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:06:38.933: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-dc4lx from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:06:38.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:06:38.933: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-efd3795c-0175-4ff3-bc31-94e4e302c749 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-efd3795c-0175-4ff3-bc31-94e4e302c749 off the node queith7zooya-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-efd3795c-0175-4ff3-bc31-94e4e302c749
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:06:43.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4174" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":135,"skipped":2566,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:06:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-3a786086-820d-435f-bcf5-540398639ba2
STEP: Creating a pod to test consume configMaps
Dec 14 16:06:43.165: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431" in namespace "configmap-3807" to be "Succeeded or Failed"
Dec 14 16:06:43.169: INFO: Pod "pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431": Phase="Pending", Reason="", readiness=false. Elapsed: 4.485445ms
Dec 14 16:06:45.179: INFO: Pod "pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013929511s
Dec 14 16:06:47.191: INFO: Pod "pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02620878s
STEP: Saw pod success
Dec 14 16:06:47.191: INFO: Pod "pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431" satisfied condition "Succeeded or Failed"
Dec 14 16:06:47.196: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 14 16:06:47.305: INFO: Waiting for pod pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431 to disappear
Dec 14 16:06:47.310: INFO: Pod pod-configmaps-ca4dd9fb-7a67-4bc3-a83e-bc0a05b01431 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 16:06:47.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3807" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2583,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:06:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 16:06:47.397: INFO: Waiting up to 5m0s for pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18" in namespace "emptydir-5582" to be "Succeeded or Failed"
Dec 14 16:06:47.402: INFO: Pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680758ms
Dec 14 16:06:49.412: INFO: Pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014539042s
Dec 14 16:06:51.427: INFO: Pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029736305s
Dec 14 16:06:53.435: INFO: Pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037786108s
STEP: Saw pod success
Dec 14 16:06:53.435: INFO: Pod "pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18" satisfied condition "Succeeded or Failed"
Dec 14 16:06:53.441: INFO: Trying to get logs from node queith7zooya-3 pod pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18 container test-container: <nil>
STEP: delete the pod
Dec 14 16:06:53.468: INFO: Waiting for pod pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18 to disappear
Dec 14 16:06:53.473: INFO: Pod pod-7bd41735-4b9c-4edc-aefd-37c7dd345f18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:06:53.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5582" for this suite.

• [SLOW TEST:6.155 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:06:53.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Dec 14 16:08:54.110: INFO: Successfully updated pod "var-expansion-a85d85e8-f7b4-4de2-aa78-8ac16e262c2f"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Dec 14 16:08:56.135: INFO: Deleting pod "var-expansion-a85d85e8-f7b4-4de2-aa78-8ac16e262c2f" in namespace "var-expansion-5295"
Dec 14 16:08:56.149: INFO: Wait up to 5m0s for pod "var-expansion-a85d85e8-f7b4-4de2-aa78-8ac16e262c2f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 16:09:28.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5295" for this suite.

• [SLOW TEST:154.706 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":138,"skipped":2637,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:28.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Dec 14 16:09:28.283: INFO: The status of Pod labelsupdatef54e0a65-121f-47a7-ad2f-55e2f086bc31 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:09:30.304: INFO: The status of Pod labelsupdatef54e0a65-121f-47a7-ad2f-55e2f086bc31 is Running (Ready = true)
Dec 14 16:09:30.866: INFO: Successfully updated pod "labelsupdatef54e0a65-121f-47a7-ad2f-55e2f086bc31"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 16:09:34.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7910" for this suite.

• [SLOW TEST:6.726 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":139,"skipped":2637,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:34.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Dec 14 16:09:34.963: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 14 16:09:34.974: INFO: Waiting for terminating namespaces to be deleted...
Dec 14 16:09:34.978: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-1 before test
Dec 14 16:09:34.987: INFO: kube-flannel-ds-9s7dq from kube-flannel started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.987: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:09:34.987: INFO: kube-addon-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.987: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:09:34.987: INFO: kube-apiserver-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.988: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:09:34.988: INFO: kube-controller-manager-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.988: INFO: 	Container kube-controller-manager ready: true, restart count 2
Dec 14 16:09:34.988: INFO: kube-proxy-zp6s8 from kube-system started at 2022-12-14 15:34:55 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.988: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:09:34.988: INFO: kube-scheduler-queith7zooya-1 from kube-system started at 2022-12-14 15:34:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.988: INFO: 	Container kube-scheduler ready: true, restart count 2
Dec 14 16:09:34.988: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-7dqd2 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:09:34.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:09:34.988: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:09:34.988: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-2 before test
Dec 14 16:09:34.998: INFO: kube-flannel-ds-2jdlw from kube-flannel started at 2022-12-14 15:34:53 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.998: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:09:34.998: INFO: coredns-57575c5f89-h2qxf from kube-system started at 2022-12-14 15:34:51 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.998: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:09:34.998: INFO: kube-addon-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.998: INFO: 	Container kube-addon-manager ready: true, restart count 1
Dec 14 16:09:34.998: INFO: kube-apiserver-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.998: INFO: 	Container kube-apiserver ready: true, restart count 1
Dec 14 16:09:34.998: INFO: kube-controller-manager-queith7zooya-2 from kube-system started at 2022-12-14 15:33:24 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.998: INFO: 	Container kube-controller-manager ready: true, restart count 1
Dec 14 16:09:34.999: INFO: kube-proxy-b847h from kube-system started at 2022-12-14 15:34:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.999: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:09:34.999: INFO: kube-scheduler-queith7zooya-2 from kube-system started at 2022-12-14 15:29:31 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:34.999: INFO: 	Container kube-scheduler ready: true, restart count 1
Dec 14 16:09:34.999: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-rhr5t from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:09:34.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:09:34.999: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 14 16:09:34.999: INFO: 
Logging pods the apiserver thinks is on node queith7zooya-3 before test
Dec 14 16:09:35.010: INFO: kube-flannel-ds-rfzcl from kube-flannel started at 2022-12-14 15:53:57 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 14 16:09:35.010: INFO: coredns-57575c5f89-qk6wh from kube-system started at 2022-12-14 15:53:56 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container coredns ready: true, restart count 0
Dec 14 16:09:35.010: INFO: kube-proxy-ms4r6 from kube-system started at 2022-12-14 15:34:52 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 14 16:09:35.010: INFO: labelsupdatef54e0a65-121f-47a7-ad2f-55e2f086bc31 from projected-7910 started at 2022-12-14 16:09:28 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container client-container ready: true, restart count 0
Dec 14 16:09:35.010: INFO: sonobuoy from sonobuoy started at 2022-12-14 15:35:54 +0000 UTC (1 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 14 16:09:35.010: INFO: sonobuoy-e2e-job-d6f98c81f7634948 from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container e2e ready: true, restart count 0
Dec 14 16:09:35.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:09:35.010: INFO: sonobuoy-systemd-logs-daemon-set-7b8e5c1a93484a84-dc4lx from sonobuoy started at 2022-12-14 15:36:05 +0000 UTC (2 container statuses recorded)
Dec 14 16:09:35.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 14 16:09:35.010: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1730b4ba89ed9a52], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:09:36.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8385" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":140,"skipped":2653,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-39a510c6-cd32-49da-b43a-19ee6700cb1b
STEP: Creating a pod to test consume configMaps
Dec 14 16:09:36.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b" in namespace "projected-3087" to be "Succeeded or Failed"
Dec 14 16:09:36.128: INFO: Pod "pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812894ms
Dec 14 16:09:38.145: INFO: Pod "pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021993239s
Dec 14 16:09:40.157: INFO: Pod "pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033333022s
STEP: Saw pod success
Dec 14 16:09:40.157: INFO: Pod "pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b" satisfied condition "Succeeded or Failed"
Dec 14 16:09:40.161: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:09:40.186: INFO: Waiting for pod pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b to disappear
Dec 14 16:09:40.191: INFO: Pod pod-projected-configmaps-d79fa6d2-2f15-4522-a0fd-df028f1d712b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 16:09:40.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3087" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":141,"skipped":2659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:40.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Dec 14 16:09:40.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9205 create -f -'
Dec 14 16:09:40.734: INFO: stderr: ""
Dec 14 16:09:40.734: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 16:09:41.746: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:09:41.746: INFO: Found 0 / 1
Dec 14 16:09:42.745: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:09:42.745: INFO: Found 1 / 1
Dec 14 16:09:42.745: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 14 16:09:42.751: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:09:42.751: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:09:42.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9205 patch pod agnhost-primary-2bqcm -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 14 16:09:42.936: INFO: stderr: ""
Dec 14 16:09:42.936: INFO: stdout: "pod/agnhost-primary-2bqcm patched\n"
STEP: checking annotations
Dec 14 16:09:42.942: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:09:42.942: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:09:42.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9205" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":142,"skipped":2711,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Dec 14 16:09:45.023: INFO: pods: 0 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec 14 16:09:47.207: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Dec 14 16:09:49.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8351" for this suite.

• [SLOW TEST:6.331 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":143,"skipped":2720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:49.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:49.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9398
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Dec 14 16:09:53.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5514" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Dec 14 16:09:53.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9398" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":144,"skipped":2762,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:09:53.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 14 16:09:53.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 14 16:10:09.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:10:13.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:10:29.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7540" for this suite.

• [SLOW TEST:36.135 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":145,"skipped":2772,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:10:29.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:10:31.753: INFO: Deleting pod "var-expansion-a4d9d994-7051-4cb5-9a3b-ac3ec166ec8a" in namespace "var-expansion-3833"
Dec 14 16:10:31.776: INFO: Wait up to 5m0s for pod "var-expansion-a4d9d994-7051-4cb5-9a3b-ac3ec166ec8a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 16:10:33.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3833" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":146,"skipped":2776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:10:33.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-d21789a6-966c-415b-b2be-f98ad17613f9
STEP: Creating a pod to test consume secrets
Dec 14 16:10:33.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de" in namespace "projected-2105" to be "Succeeded or Failed"
Dec 14 16:10:33.919: INFO: Pod "pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.229697ms
Dec 14 16:10:35.935: INFO: Pod "pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021904951s
Dec 14 16:10:37.963: INFO: Pod "pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050005032s
STEP: Saw pod success
Dec 14 16:10:37.963: INFO: Pod "pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de" satisfied condition "Succeeded or Failed"
Dec 14 16:10:37.969: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:10:38.003: INFO: Waiting for pod pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de to disappear
Dec 14 16:10:38.007: INFO: Pod pod-projected-secrets-8c51ed27-2a8d-4abc-9b74-1589644882de no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:10:38.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2105" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":147,"skipped":2855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:10:38.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:10:39.612: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:10:42.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:10:42.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2188-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:10:46.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2349" for this suite.
STEP: Destroying namespace "webhook-2349-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.300 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":148,"skipped":2879,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:10:46.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 16:10:46.417: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:11:46.469: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Dec 14 16:11:46.510: INFO: Created pod: pod0-0-sched-preemption-low-priority
Dec 14 16:11:46.520: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Dec 14 16:11:46.566: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Dec 14 16:11:46.585: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Dec 14 16:11:46.613: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Dec 14 16:11:46.627: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:11:52.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-829" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:66.564 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":149,"skipped":2889,"failed":0}
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:11:52.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Dec 14 16:11:52.947: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 16:11:58.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9928" for this suite.

• [SLOW TEST:5.720 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":150,"skipped":2892,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:11:58.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:12:00.263: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:12:03.319: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:12:03.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1964-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:12:06.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1810" for this suite.
STEP: Destroying namespace "webhook-1810-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":151,"skipped":2904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:12:07.332: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:12:10.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:12:10.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5087" for this suite.
STEP: Destroying namespace "webhook-5087-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":152,"skipped":2929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:10.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Dec 14 16:12:10.615: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 16:12:14.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9868" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":153,"skipped":2996,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:14.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-f58d3d75-389c-4c59-829c-4b3bdbdedc50
STEP: Creating a pod to test consume secrets
Dec 14 16:12:14.783: INFO: Waiting up to 5m0s for pod "pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990" in namespace "secrets-5675" to be "Succeeded or Failed"
Dec 14 16:12:14.792: INFO: Pod "pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516141ms
Dec 14 16:12:16.814: INFO: Pod "pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990": Phase="Running", Reason="", readiness=false. Elapsed: 2.031230774s
Dec 14 16:12:18.833: INFO: Pod "pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049463433s
STEP: Saw pod success
Dec 14 16:12:18.833: INFO: Pod "pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990" satisfied condition "Succeeded or Failed"
Dec 14 16:12:18.838: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:12:18.901: INFO: Waiting for pod pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990 to disappear
Dec 14 16:12:18.905: INFO: Pod pod-secrets-b77c53e4-cf6a-4f4d-aec1-2abe25d17990 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:12:18.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5675" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":154,"skipped":2999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:18.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-0c556307-5ea7-42d4-9a21-18bcb9e19c30
STEP: Creating a pod to test consume configMaps
Dec 14 16:12:18.979: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb" in namespace "projected-4142" to be "Succeeded or Failed"
Dec 14 16:12:18.986: INFO: Pod "pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.734392ms
Dec 14 16:12:21.003: INFO: Pod "pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0238383s
Dec 14 16:12:23.024: INFO: Pod "pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044893192s
STEP: Saw pod success
Dec 14 16:12:23.024: INFO: Pod "pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb" satisfied condition "Succeeded or Failed"
Dec 14 16:12:23.030: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:12:23.064: INFO: Waiting for pod pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb to disappear
Dec 14 16:12:23.068: INFO: Pod pod-projected-configmaps-3c9e6acd-5cfc-41d0-8abf-f6e94f4cb4eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 16:12:23.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4142" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":155,"skipped":3032,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:23.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Dec 14 16:12:35.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5978" for this suite.

• [SLOW TEST:12.147 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":156,"skipped":3050,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:35.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Dec 14 16:12:37.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7433" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":157,"skipped":3054,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:37.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8836
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8836
I1214 16:12:37.621097      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8836, replica count: 2
Dec 14 16:12:40.672: INFO: Creating new exec pod
I1214 16:12:40.672696      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:12:43.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8836 exec execpodt7nht -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 16:12:44.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 16:12:44.035: INFO: stdout: "externalname-service-8cprh"
Dec 14 16:12:44.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8836 exec execpodt7nht -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.0.141 80'
Dec 14 16:12:44.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.0.141 80\nConnection to 10.233.0.141 80 port [tcp/http] succeeded!\n"
Dec 14 16:12:44.248: INFO: stdout: ""
Dec 14 16:12:45.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8836 exec execpodt7nht -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.0.141 80'
Dec 14 16:12:45.616: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.0.141 80\nConnection to 10.233.0.141 80 port [tcp/http] succeeded!\n"
Dec 14 16:12:45.617: INFO: stdout: "externalname-service-8cprh"
Dec 14 16:12:45.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8836 exec execpodt7nht -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 30619'
Dec 14 16:12:45.967: INFO: stderr: "+ nc -v -t -w 2 192.168.121.248 30619\n+ echo hostName\nConnection to 192.168.121.248 30619 port [tcp/*] succeeded!\n"
Dec 14 16:12:45.967: INFO: stdout: "externalname-service-k2x96"
Dec 14 16:12:45.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8836 exec execpodt7nht -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.209 30619'
Dec 14 16:12:46.225: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.209 30619\nConnection to 192.168.121.209 30619 port [tcp/*] succeeded!\n"
Dec 14 16:12:46.225: INFO: stdout: "externalname-service-k2x96"
Dec 14 16:12:46.225: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:12:46.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8836" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.775 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":158,"skipped":3055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:46.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 14 16:12:46.382: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15653 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:12:46.383: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15654 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:12:46.384: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15655 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 14 16:12:56.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15706 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:12:56.463: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15707 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:12:56.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6468  edc23015-3bee-4563-87d8-98ccd01a952b 15708 0 2022-12-14 16:12:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-12-14 16:12:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Dec 14 16:12:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6468" for this suite.

• [SLOW TEST:10.216 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":159,"skipped":3085,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:12:56.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-b5ac5c04-03e9-42ca-ba43-a5d08efe9845
STEP: Creating a pod to test consume secrets
Dec 14 16:12:56.586: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6" in namespace "projected-7100" to be "Succeeded or Failed"
Dec 14 16:12:56.600: INFO: Pod "pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.970957ms
Dec 14 16:12:58.614: INFO: Pod "pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027838252s
Dec 14 16:13:00.631: INFO: Pod "pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044291923s
STEP: Saw pod success
Dec 14 16:13:00.631: INFO: Pod "pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6" satisfied condition "Succeeded or Failed"
Dec 14 16:13:00.636: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:13:00.668: INFO: Waiting for pod pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6 to disappear
Dec 14 16:13:00.672: INFO: Pod pod-projected-secrets-d3eb9ccd-5da3-4b9d-bbdf-242bf419dff6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:13:00.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7100" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":160,"skipped":3091,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:13:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Dec 14 16:13:00.742: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3816 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:13:00.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3816" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":161,"skipped":3095,"failed":0}

------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:13:00.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Dec 14 16:15:01.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-138" for this suite.

• [SLOW TEST:120.158 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":162,"skipped":3095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:15:01.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Dec 14 16:15:01.079: INFO: Waiting up to 5m0s for pod "var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198" in namespace "var-expansion-393" to be "Succeeded or Failed"
Dec 14 16:15:01.084: INFO: Pod "var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198": Phase="Pending", Reason="", readiness=false. Elapsed: 5.103819ms
Dec 14 16:15:03.100: INFO: Pod "var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021298364s
Dec 14 16:15:05.111: INFO: Pod "var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031700822s
STEP: Saw pod success
Dec 14 16:15:05.111: INFO: Pod "var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198" satisfied condition "Succeeded or Failed"
Dec 14 16:15:05.115: INFO: Trying to get logs from node queith7zooya-3 pod var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198 container dapi-container: <nil>
STEP: delete the pod
Dec 14 16:15:05.329: INFO: Waiting for pod var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198 to disappear
Dec 14 16:15:05.335: INFO: Pod var-expansion-d51ab9a4-a499-4bc2-9416-ee272a92c198 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 16:15:05.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-393" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:15:05.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 14 16:15:05.754: INFO: Pod name wrapped-volume-race-006e3dfe-7263-4940-8369-427004a02939: Found 0 pods out of 5
Dec 14 16:15:10.773: INFO: Pod name wrapped-volume-race-006e3dfe-7263-4940-8369-427004a02939: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-006e3dfe-7263-4940-8369-427004a02939 in namespace emptydir-wrapper-8845, will wait for the garbage collector to delete the pods
Dec 14 16:15:20.900: INFO: Deleting ReplicationController wrapped-volume-race-006e3dfe-7263-4940-8369-427004a02939 took: 13.695171ms
Dec 14 16:15:21.000: INFO: Terminating ReplicationController wrapped-volume-race-006e3dfe-7263-4940-8369-427004a02939 pods took: 100.418969ms
STEP: Creating RC which spawns configmap-volume pods
Dec 14 16:15:24.134: INFO: Pod name wrapped-volume-race-8d0b7123-1f4e-4c38-a47c-3c606f74c747: Found 0 pods out of 5
Dec 14 16:15:29.150: INFO: Pod name wrapped-volume-race-8d0b7123-1f4e-4c38-a47c-3c606f74c747: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8d0b7123-1f4e-4c38-a47c-3c606f74c747 in namespace emptydir-wrapper-8845, will wait for the garbage collector to delete the pods
Dec 14 16:15:39.282: INFO: Deleting ReplicationController wrapped-volume-race-8d0b7123-1f4e-4c38-a47c-3c606f74c747 took: 12.960594ms
Dec 14 16:15:39.382: INFO: Terminating ReplicationController wrapped-volume-race-8d0b7123-1f4e-4c38-a47c-3c606f74c747 pods took: 100.467569ms
STEP: Creating RC which spawns configmap-volume pods
Dec 14 16:15:43.129: INFO: Pod name wrapped-volume-race-f20c9099-e78b-4690-af22-70358e3d8f65: Found 0 pods out of 5
Dec 14 16:15:48.157: INFO: Pod name wrapped-volume-race-f20c9099-e78b-4690-af22-70358e3d8f65: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f20c9099-e78b-4690-af22-70358e3d8f65 in namespace emptydir-wrapper-8845, will wait for the garbage collector to delete the pods
Dec 14 16:15:58.275: INFO: Deleting ReplicationController wrapped-volume-race-f20c9099-e78b-4690-af22-70358e3d8f65 took: 16.517594ms
Dec 14 16:15:58.376: INFO: Terminating ReplicationController wrapped-volume-race-f20c9099-e78b-4690-af22-70358e3d8f65 pods took: 100.828065ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Dec 14 16:16:02.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8845" for this suite.

• [SLOW TEST:56.958 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":164,"skipped":3228,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:02.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:16:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 16:16:02.370: INFO: The status of Pod pod-logs-websocket-4fffcd35-1397-40b2-b546-6235f4dff085 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:16:04.380: INFO: The status of Pod pod-logs-websocket-4fffcd35-1397-40b2-b546-6235f4dff085 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 16:16:04.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3314" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":165,"skipped":3244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:04.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-09b5ae02-6e2c-417c-acb0-e44c7bb11548
STEP: Creating a pod to test consume secrets
Dec 14 16:16:04.483: INFO: Waiting up to 5m0s for pod "pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d" in namespace "secrets-9371" to be "Succeeded or Failed"
Dec 14 16:16:04.488: INFO: Pod "pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877689ms
Dec 14 16:16:06.507: INFO: Pod "pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023804236s
Dec 14 16:16:08.516: INFO: Pod "pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033432483s
STEP: Saw pod success
Dec 14 16:16:08.516: INFO: Pod "pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d" satisfied condition "Succeeded or Failed"
Dec 14 16:16:08.520: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:16:08.562: INFO: Waiting for pod pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d to disappear
Dec 14 16:16:08.566: INFO: Pod pod-secrets-8b6880c2-5e9c-4adb-a96a-a4dbe5af402d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:16:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9371" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":166,"skipped":3288,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:08.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Dec 14 16:16:08.699: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:16:10.711: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Dec 14 16:16:10.762: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:16:12.778: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 16:16:13.000: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 16:16:13.005: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 16:16:15.006: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 16:16:15.016: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 14 16:16:17.006: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 14 16:16:17.022: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Dec 14 16:16:17.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8559" for this suite.

• [SLOW TEST:8.451 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":3295,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:17.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-140 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-140;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-140 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-140;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-140.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-140.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-140.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-140.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-140.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-140.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-140.svc;check="$$(dig +notcp +noall +answer +search 14.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.14_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-140 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-140;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-140 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-140;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-140.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-140.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-140.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-140.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-140.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-140.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-140.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-140.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-140.svc;check="$$(dig +notcp +noall +answer +search 14.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.14_udp@PTR;check="$$(dig +tcp +noall +answer +search 14.20.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.20.14_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:16:21.246: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.262: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.301: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.326: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.344: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.350: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.358: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.369: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.400: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.407: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.417: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.423: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.428: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.434: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.440: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.447: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:21.470: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:26.485: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.493: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.504: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.510: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.517: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.529: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.537: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.570: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.576: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.581: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.587: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.591: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.601: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.605: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:26.625: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:31.480: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.489: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.505: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.510: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.520: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.535: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.597: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.604: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.609: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.617: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.630: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.636: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:31.668: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:36.481: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.495: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.502: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.508: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.520: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.526: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.605: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.635: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.642: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.654: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.660: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.697: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:36.721: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:41.480: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.488: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.494: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.502: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.507: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.520: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.531: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.536: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.598: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.607: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.622: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.631: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.639: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.659: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:41.722: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:46.481: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.486: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.494: INFO: Unable to read wheezy_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.501: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.506: INFO: Unable to read wheezy_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.514: INFO: Unable to read wheezy_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.523: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.562: INFO: Unable to read jessie_udp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.570: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.575: INFO: Unable to read jessie_udp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.580: INFO: Unable to read jessie_tcp@dns-test-service.dns-140 from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.585: INFO: Unable to read jessie_udp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.596: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-140.svc from pod dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b: the server could not find the requested resource (get pods dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b)
Dec 14 16:16:46.626: INFO: Lookups using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-140 wheezy_tcp@dns-test-service.dns-140 wheezy_udp@dns-test-service.dns-140.svc wheezy_tcp@dns-test-service.dns-140.svc wheezy_udp@_http._tcp.dns-test-service.dns-140.svc wheezy_tcp@_http._tcp.dns-test-service.dns-140.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-140 jessie_tcp@dns-test-service.dns-140 jessie_udp@dns-test-service.dns-140.svc jessie_tcp@dns-test-service.dns-140.svc jessie_udp@_http._tcp.dns-test-service.dns-140.svc jessie_tcp@_http._tcp.dns-test-service.dns-140.svc]

Dec 14 16:16:51.604: INFO: DNS probes using dns-140/dns-test-340566e2-c68f-4c2e-b8df-f888cda85a1b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:16:51.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-140" for this suite.

• [SLOW TEST:34.796 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":168,"skipped":3300,"failed":0}
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Dec 14 16:16:51.886: INFO: Creating e2e-svc-a-l4mfb
Dec 14 16:16:51.907: INFO: Creating e2e-svc-b-cmnqp
Dec 14 16:16:51.926: INFO: Creating e2e-svc-c-qvlsr
STEP: deleting service collection
Dec 14 16:16:52.046: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:16:52.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5236" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":169,"skipped":3300,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Dec 14 16:16:52.139: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Dec 14 16:16:52.145: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 16:16:52.145: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Dec 14 16:16:52.157: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Dec 14 16:16:52.157: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Dec 14 16:16:52.172: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Dec 14 16:16:52.172: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Dec 14 16:16:59.252: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Dec 14 16:16:59.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3315" for this suite.

• [SLOW TEST:7.221 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":170,"skipped":3321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:16:59.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:16:59.337: INFO: Creating simple deployment test-new-deployment
Dec 14 16:16:59.353: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:17:01.427: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-249  3cfe7e96-9e76-45ad-9074-114c4b468cba 17270 3 2022-12-14 16:16:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-12-14 16:16:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:17:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042bcff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:17:00 +0000 UTC,LastTransitionTime:2022-12-14 16:17:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2022-12-14 16:17:00 +0000 UTC,LastTransitionTime:2022-12-14 16:16:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:17:01.437: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-249  b078918d-7b93-4739-8022-4f5aec3747ea 17274 3 2022-12-14 16:16:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3cfe7e96-9e76-45ad-9074-114c4b468cba 0xc0009a23c7 0xc0009a23c8}] []  [{kube-controller-manager Update apps/v1 2022-12-14 16:16:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3cfe7e96-9e76-45ad-9074-114c4b468cba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:17:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a27a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:17:01.454: INFO: Pod "test-new-deployment-55df494869-s2ph7" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-s2ph7 test-new-deployment-55df494869- deployment-249  4bace219-9863-4ea9-8ed3-c3db8c044d13 17264 0 2022-12-14 16:16:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 b078918d-7b93-4739-8022-4f5aec3747ea 0xc0042bd3d7 0xc0042bd3d8}] []  [{kube-controller-manager Update v1 2022-12-14 16:16:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b078918d-7b93-4739-8022-4f5aec3747ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:17:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.205\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ft2rf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ft2rf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:16:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:17:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:17:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:16:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.205,StartTime:2022-12-14 16:16:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:17:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://dba17bf1054905ed0ffb3bb006d78b8ed3206ec6dba0a2dc7b5986e2e47ca6c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.205,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:17:01.454: INFO: Pod "test-new-deployment-55df494869-zlswm" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-zlswm test-new-deployment-55df494869- deployment-249  2e6bc338-612f-4d93-a4e5-147a95fa32ca 17273 0 2022-12-14 16:17:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 b078918d-7b93-4739-8022-4f5aec3747ea 0xc0042bd5c7 0xc0042bd5c8}] []  [{kube-controller-manager Update v1 2022-12-14 16:17:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b078918d-7b93-4739-8022-4f5aec3747ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpdsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpdsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:17:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 16:17:01.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-249" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":171,"skipped":3359,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:01.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-6353
STEP: creating replication controller nodeport-test in namespace services-6353
I1214 16:17:01.581415      14 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-6353, replica count: 2
I1214 16:17:04.637361      14 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:17:04.637: INFO: Creating new exec pod
Dec 14 16:17:07.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-6353 exec execpodcsrc7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Dec 14 16:17:08.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 14 16:17:08.010: INFO: stdout: "nodeport-test-cw2r6"
Dec 14 16:17:08.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-6353 exec execpodcsrc7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.63.18 80'
Dec 14 16:17:08.229: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.63.18 80\nConnection to 10.233.63.18 80 port [tcp/http] succeeded!\n"
Dec 14 16:17:08.229: INFO: stdout: "nodeport-test-jbrpw"
Dec 14 16:17:08.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-6353 exec execpodcsrc7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 30149'
Dec 14 16:17:08.432: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 30149\nConnection to 192.168.121.248 30149 port [tcp/*] succeeded!\n"
Dec 14 16:17:08.432: INFO: stdout: "nodeport-test-cw2r6"
Dec 14 16:17:08.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-6353 exec execpodcsrc7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.111 30149'
Dec 14 16:17:08.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.111 30149\nConnection to 192.168.121.111 30149 port [tcp/*] succeeded!\n"
Dec 14 16:17:08.652: INFO: stdout: "nodeport-test-jbrpw"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:17:08.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6353" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.195 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":172,"skipped":3366,"failed":0}
SSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:08.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec 14 16:17:08.731: INFO: Waiting up to 5m0s for pod "security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5" in namespace "security-context-3644" to be "Succeeded or Failed"
Dec 14 16:17:08.737: INFO: Pod "security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.655879ms
Dec 14 16:17:10.751: INFO: Pod "security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019103168s
Dec 14 16:17:12.762: INFO: Pod "security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030141305s
STEP: Saw pod success
Dec 14 16:17:12.762: INFO: Pod "security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5" satisfied condition "Succeeded or Failed"
Dec 14 16:17:12.766: INFO: Trying to get logs from node queith7zooya-3 pod security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5 container test-container: <nil>
STEP: delete the pod
Dec 14 16:17:12.801: INFO: Waiting for pod security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5 to disappear
Dec 14 16:17:12.805: INFO: Pod security-context-d02fd942-1a11-4cf2-9066-d4ca471514f5 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 16:17:12.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3644" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":173,"skipped":3372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:12.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Dec 14 16:17:12.870: INFO: Waiting up to 5m0s for pod "downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d" in namespace "downward-api-4181" to be "Succeeded or Failed"
Dec 14 16:17:12.874: INFO: Pod "downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.98625ms
Dec 14 16:17:14.888: INFO: Pod "downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017493806s
Dec 14 16:17:16.905: INFO: Pod "downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034272919s
STEP: Saw pod success
Dec 14 16:17:16.905: INFO: Pod "downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d" satisfied condition "Succeeded or Failed"
Dec 14 16:17:16.912: INFO: Trying to get logs from node queith7zooya-3 pod downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d container dapi-container: <nil>
STEP: delete the pod
Dec 14 16:17:16.949: INFO: Waiting for pod downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d to disappear
Dec 14 16:17:16.955: INFO: Pod downward-api-40dac80f-4379-456f-b1ab-32fdb979ee9d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Dec 14 16:17:16.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4181" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":174,"skipped":3408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:16.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Dec 14 16:17:17.044: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Dec 14 16:17:17.079: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Dec 14 16:17:17.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1796" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":175,"skipped":3469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:17.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Dec 14 16:17:37.432: INFO: EndpointSlice for Service endpointslice-3062/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Dec 14 16:17:47.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3062" for this suite.

• [SLOW TEST:30.319 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":176,"skipped":3495,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:47.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Dec 14 16:17:47.544: INFO: Waiting up to 5m0s for pod "downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98" in namespace "downward-api-9271" to be "Succeeded or Failed"
Dec 14 16:17:47.552: INFO: Pod "downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98": Phase="Pending", Reason="", readiness=false. Elapsed: 8.347494ms
Dec 14 16:17:49.562: INFO: Pod "downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017804324s
Dec 14 16:17:51.574: INFO: Pod "downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030602006s
STEP: Saw pod success
Dec 14 16:17:51.575: INFO: Pod "downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98" satisfied condition "Succeeded or Failed"
Dec 14 16:17:51.579: INFO: Trying to get logs from node queith7zooya-3 pod downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98 container dapi-container: <nil>
STEP: delete the pod
Dec 14 16:17:51.619: INFO: Waiting for pod downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98 to disappear
Dec 14 16:17:51.624: INFO: Pod downward-api-c28cf6dd-6cd8-40d6-ac80-9878a2081a98 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Dec 14 16:17:51.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9271" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":177,"skipped":3506,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:51.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Dec 14 16:17:51.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 14 16:17:51.882: INFO: stderr: ""
Dec 14 16:17:51.882: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Dec 14 16:17:51.884: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 14 16:17:51.885: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3892" to be "running and ready, or succeeded"
Dec 14 16:17:51.902: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.818284ms
Dec 14 16:17:53.916: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.030302422s
Dec 14 16:17:53.916: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 14 16:17:53.916: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 14 16:17:53.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator'
Dec 14 16:17:54.136: INFO: stderr: ""
Dec 14 16:17:54.136: INFO: stdout: "I1214 16:17:52.581119       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/ztfm 592\nI1214 16:17:52.780462       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/2r8r 223\nI1214 16:17:52.980706       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/8ttx 562\nI1214 16:17:53.181176       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/j864 500\nI1214 16:17:53.380402       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/ttdj 297\nI1214 16:17:53.580766       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/wjm 382\nI1214 16:17:53.781119       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4lpm 373\nI1214 16:17:53.980883       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/fcs7 562\n"
STEP: limiting log lines
Dec 14 16:17:54.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator --tail=1'
Dec 14 16:17:54.276: INFO: stderr: ""
Dec 14 16:17:54.276: INFO: stdout: "I1214 16:17:54.181264       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/grp 414\n"
Dec 14 16:17:54.276: INFO: got output "I1214 16:17:54.181264       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/grp 414\n"
STEP: limiting log bytes
Dec 14 16:17:54.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator --limit-bytes=1'
Dec 14 16:17:54.399: INFO: stderr: ""
Dec 14 16:17:54.399: INFO: stdout: "I"
Dec 14 16:17:54.399: INFO: got output "I"
STEP: exposing timestamps
Dec 14 16:17:54.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator --tail=1 --timestamps'
Dec 14 16:17:54.537: INFO: stderr: ""
Dec 14 16:17:54.537: INFO: stdout: "2022-12-14T16:17:54.381008322Z I1214 16:17:54.380943       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/2p8 496\n"
Dec 14 16:17:54.537: INFO: got output "2022-12-14T16:17:54.381008322Z I1214 16:17:54.380943       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/2p8 496\n"
STEP: restricting to a time range
Dec 14 16:17:57.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator --since=1s'
Dec 14 16:17:57.289: INFO: stderr: ""
Dec 14 16:17:57.290: INFO: stdout: "I1214 16:17:56.380441       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7xm9 500\nI1214 16:17:56.581119       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/zs59 265\nI1214 16:17:56.780314       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/j9w8 207\nI1214 16:17:56.980814       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/txzz 541\nI1214 16:17:57.181088       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/bfx 590\n"
Dec 14 16:17:57.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 logs logs-generator logs-generator --since=24h'
Dec 14 16:17:57.494: INFO: stderr: ""
Dec 14 16:17:57.494: INFO: stdout: "I1214 16:17:52.581119       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/ztfm 592\nI1214 16:17:52.780462       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/2r8r 223\nI1214 16:17:52.980706       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/8ttx 562\nI1214 16:17:53.181176       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/j864 500\nI1214 16:17:53.380402       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/ttdj 297\nI1214 16:17:53.580766       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/wjm 382\nI1214 16:17:53.781119       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4lpm 373\nI1214 16:17:53.980883       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/fcs7 562\nI1214 16:17:54.181264       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/grp 414\nI1214 16:17:54.380943       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/2p8 496\nI1214 16:17:54.580473       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/q2j 218\nI1214 16:17:54.781065       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/jxrm 312\nI1214 16:17:54.980480       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/jn5 481\nI1214 16:17:55.181056       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/4t2j 360\nI1214 16:17:55.381193       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bhzn 429\nI1214 16:17:55.580451       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/lpl 233\nI1214 16:17:55.781759       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/vm6 372\nI1214 16:17:55.980483       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/h9xg 506\nI1214 16:17:56.181070       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/gcjq 543\nI1214 16:17:56.380441       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/7xm9 500\nI1214 16:17:56.581119       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/zs59 265\nI1214 16:17:56.780314       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/j9w8 207\nI1214 16:17:56.980814       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/txzz 541\nI1214 16:17:57.181088       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/bfx 590\nI1214 16:17:57.380459       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/c45 241\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Dec 14 16:17:57.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-3892 delete pod logs-generator'
Dec 14 16:17:58.335: INFO: stderr: ""
Dec 14 16:17:58.335: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:17:58.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3892" for this suite.

• [SLOW TEST:6.714 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":178,"skipped":3509,"failed":0}
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:17:58.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 16:18:02.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-644" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":179,"skipped":3516,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:18:02.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Dec 14 16:18:02.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1859" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":180,"skipped":3530,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:18:02.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Dec 14 16:18:02.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:18:28.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3095" for this suite.

• [SLOW TEST:25.536 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":181,"skipped":3535,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:18:28.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Dec 14 16:18:30.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8944" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3550,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:18:30.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:18:30.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Dec 14 16:18:33.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-1854 --namespace=crd-publish-openapi-1854 create -f -'
Dec 14 16:18:34.742: INFO: stderr: ""
Dec 14 16:18:34.742: INFO: stdout: "e2e-test-crd-publish-openapi-1499-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 16:18:34.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-1854 --namespace=crd-publish-openapi-1854 delete e2e-test-crd-publish-openapi-1499-crds test-cr'
Dec 14 16:18:35.160: INFO: stderr: ""
Dec 14 16:18:35.160: INFO: stdout: "e2e-test-crd-publish-openapi-1499-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 14 16:18:35.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-1854 --namespace=crd-publish-openapi-1854 apply -f -'
Dec 14 16:18:35.518: INFO: stderr: ""
Dec 14 16:18:35.518: INFO: stdout: "e2e-test-crd-publish-openapi-1499-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 14 16:18:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-1854 --namespace=crd-publish-openapi-1854 delete e2e-test-crd-publish-openapi-1499-crds test-cr'
Dec 14 16:18:35.802: INFO: stderr: ""
Dec 14 16:18:35.802: INFO: stdout: "e2e-test-crd-publish-openapi-1499-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 16:18:35.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-1854 explain e2e-test-crd-publish-openapi-1499-crds'
Dec 14 16:18:36.163: INFO: stderr: ""
Dec 14 16:18:36.163: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1499-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:18:38.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1854" for this suite.

• [SLOW TEST:8.752 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":183,"skipped":3550,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:18:39.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2723
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 16:18:39.054: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 16:18:39.163: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:18:41.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:18:43.191: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:18:45.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:18:47.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:18:49.169: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 16:18:51.177: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 16:18:51.186: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 16:18:51.194: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 16:18:53.208: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 16:18:55.206: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 16:18:57.207: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 16:18:59.201: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 16:19:01.238: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 16:19:03.290: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 16:19:03.291: INFO: Going to poll 10.233.64.78 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:19:03.296: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2723 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:19:03.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:19:03.299: INFO: ExecWithOptions: Clientset creation
Dec 14 16:19:03.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2723/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:19:04.531: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 16:19:04.531: INFO: Going to poll 10.233.65.80 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:19:04.540: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.80 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2723 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:19:04.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:19:04.541: INFO: ExecWithOptions: Clientset creation
Dec 14 16:19:04.541: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2723/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.80+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:19:05.650: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 16:19:05.650: INFO: Going to poll 10.233.66.215 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Dec 14 16:19:05.658: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2723 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:19:05.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:19:05.660: INFO: ExecWithOptions: Clientset creation
Dec 14 16:19:05.660: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2723/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.215+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:19:06.771: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Dec 14 16:19:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2723" for this suite.

• [SLOW TEST:27.794 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":184,"skipped":3558,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:19:06.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Dec 14 16:19:06.841: INFO: namespace kubectl-6685
Dec 14 16:19:06.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6685 create -f -'
Dec 14 16:19:07.870: INFO: stderr: ""
Dec 14 16:19:07.870: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Dec 14 16:19:08.890: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:19:08.891: INFO: Found 1 / 1
Dec 14 16:19:08.891: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 14 16:19:08.898: INFO: Selector matched 1 pods for map[app:agnhost]
Dec 14 16:19:08.898: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 14 16:19:08.898: INFO: wait on agnhost-primary startup in kubectl-6685 
Dec 14 16:19:08.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6685 logs agnhost-primary-h77vc agnhost-primary'
Dec 14 16:19:09.046: INFO: stderr: ""
Dec 14 16:19:09.046: INFO: stdout: "Paused\n"
STEP: exposing RC
Dec 14 16:19:09.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6685 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Dec 14 16:19:09.205: INFO: stderr: ""
Dec 14 16:19:09.205: INFO: stdout: "service/rm2 exposed\n"
Dec 14 16:19:09.221: INFO: Service rm2 in namespace kubectl-6685 found.
STEP: exposing service
Dec 14 16:19:11.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6685 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Dec 14 16:19:11.463: INFO: stderr: ""
Dec 14 16:19:11.463: INFO: stdout: "service/rm3 exposed\n"
Dec 14 16:19:11.468: INFO: Service rm3 in namespace kubectl-6685 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:19:13.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6685" for this suite.

• [SLOW TEST:6.705 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":185,"skipped":3560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:19:13.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-80c9b097-d383-4613-a287-f1cce83fabea
STEP: Creating configMap with name cm-test-opt-upd-06ed1b0f-2a84-4d7b-b48e-049bc7b8b585
STEP: Creating the pod
Dec 14 16:19:13.592: INFO: The status of Pod pod-projected-configmaps-b8a6c4b0-610d-4c32-a55e-1ba6636fe27e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:19:15.602: INFO: The status of Pod pod-projected-configmaps-b8a6c4b0-610d-4c32-a55e-1ba6636fe27e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:19:17.608: INFO: The status of Pod pod-projected-configmaps-b8a6c4b0-610d-4c32-a55e-1ba6636fe27e is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-80c9b097-d383-4613-a287-f1cce83fabea
STEP: Updating configmap cm-test-opt-upd-06ed1b0f-2a84-4d7b-b48e-049bc7b8b585
STEP: Creating configMap with name cm-test-opt-create-18ed8512-68fe-4d15-b2e3-5a454f40544d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Dec 14 16:20:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7485" for this suite.

• [SLOW TEST:80.918 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":186,"skipped":3582,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:20:34.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Dec 14 16:20:34.508: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:20:36.521: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Dec 14 16:20:36.550: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:20:38.561: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec 14 16:20:38.578: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:20:38.601: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 16:20:40.601: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:20:40.610: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 14 16:20:42.602: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 14 16:20:42.616: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Dec 14 16:20:42.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3737" for this suite.

• [SLOW TEST:8.246 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3597,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:20:42.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:20:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:20:46.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7102" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":188,"skipped":3603,"failed":0}
SSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:20:46.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Dec 14 16:20:46.167: INFO: Major version: 1
STEP: Confirm minor version
Dec 14 16:20:46.167: INFO: cleanMinorVersion: 24
Dec 14 16:20:46.167: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Dec 14 16:20:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-4151" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":189,"skipped":3606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:20:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3130
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Dec 14 16:20:46.239: INFO: Found 0 stateful pods, waiting for 3
Dec 14 16:20:56.261: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:20:56.261: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:20:56.261: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:20:56.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-3130 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:20:56.564: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:20:56.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:20:56.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Dec 14 16:21:06.638: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 14 16:21:16.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-3130 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:21:17.029: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:21:17.029: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:21:17.029: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:21:27.086: INFO: Waiting for StatefulSet statefulset-3130/ss2 to complete update
Dec 14 16:21:27.086: INFO: Waiting for Pod statefulset-3130/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Dec 14 16:21:27.086: INFO: Waiting for Pod statefulset-3130/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Dec 14 16:21:37.138: INFO: Waiting for StatefulSet statefulset-3130/ss2 to complete update
Dec 14 16:21:37.139: INFO: Waiting for Pod statefulset-3130/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Dec 14 16:21:47.122: INFO: Waiting for StatefulSet statefulset-3130/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 14 16:21:57.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-3130 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:21:57.421: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:21:57.421: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:21:57.421: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:22:07.486: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 14 16:22:17.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-3130 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:22:17.781: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:22:17.781: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:22:17.781: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 16:22:27.851: INFO: Deleting all statefulset in ns statefulset-3130
Dec 14 16:22:27.857: INFO: Scaling statefulset ss2 to 0
Dec 14 16:22:37.914: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:22:37.921: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 16:22:37.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3130" for this suite.

• [SLOW TEST:111.780 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":190,"skipped":3642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:37.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Dec 14 16:22:38.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-5970 create -f -'
Dec 14 16:22:38.446: INFO: stderr: ""
Dec 14 16:22:38.446: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Dec 14 16:22:38.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-5970 diff -f -'
Dec 14 16:22:40.026: INFO: rc: 1
Dec 14 16:22:40.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-5970 delete -f -'
Dec 14 16:22:40.139: INFO: stderr: ""
Dec 14 16:22:40.139: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:22:40.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5970" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":191,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:40.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-9bb74381-1bf3-4622-ad40-79b6c53596ed
STEP: Creating a pod to test consume configMaps
Dec 14 16:22:40.214: INFO: Waiting up to 5m0s for pod "pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1" in namespace "configmap-3451" to be "Succeeded or Failed"
Dec 14 16:22:40.219: INFO: Pod "pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101148ms
Dec 14 16:22:42.232: INFO: Pod "pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018002237s
Dec 14 16:22:44.244: INFO: Pod "pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030173952s
STEP: Saw pod success
Dec 14 16:22:44.245: INFO: Pod "pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1" satisfied condition "Succeeded or Failed"
Dec 14 16:22:44.252: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:22:44.295: INFO: Waiting for pod pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1 to disappear
Dec 14 16:22:44.304: INFO: Pod pod-configmaps-b887664a-94b4-4504-ad80-04aad6cfd1b1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 16:22:44.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3451" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3706,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:44.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:22:44.368: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 14 16:22:44.390: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 16:22:49.402: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 16:22:49.402: INFO: Creating deployment "test-rolling-update-deployment"
Dec 14 16:22:49.412: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 14 16:22:49.422: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 14 16:22:51.436: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 14 16:22:51.440: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:22:51.457: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4113  6066e3fd-2a35-42b1-ae87-354b945ecfd1 18976 1 2022-12-14 16:22:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-12-14 16:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044faef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 16:22:49 +0000 UTC,LastTransitionTime:2022-12-14 16:22:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2022-12-14 16:22:51 +0000 UTC,LastTransitionTime:2022-12-14 16:22:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 16:22:51.463: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-4113  4deb99fa-f373-4e3b-955e-ecbf30f442fa 18966 1 2022-12-14 16:22:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6066e3fd-2a35-42b1-ae87-354b945ecfd1 0xc0044fb3e7 0xc0044fb3e8}] []  [{kube-controller-manager Update apps/v1 2022-12-14 16:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6066e3fd-2a35-42b1-ae87-354b945ecfd1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:22:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044fb498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:22:51.464: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 14 16:22:51.464: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4113  55f4496f-4a23-4e2f-9995-be7ec58164ff 18975 2 2022-12-14 16:22:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6066e3fd-2a35-42b1-ae87-354b945ecfd1 0xc0044fb2b7 0xc0044fb2b8}] []  [{e2e.test Update apps/v1 2022-12-14 16:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:22:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6066e3fd-2a35-42b1-ae87-354b945ecfd1\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:22:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044fb378 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:22:51.471: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-zvktx" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-zvktx test-rolling-update-deployment-67c8f74c6c- deployment-4113  6acd3c5a-1d2b-46d5-b17b-f445c8eccdf1 18965 0 2022-12-14 16:22:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c 4deb99fa-f373-4e3b-955e-ecbf30f442fa 0xc000fd7217 0xc000fd7218}] []  [{kube-controller-manager Update v1 2022-12-14 16:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4deb99fa-f373-4e3b-955e-ecbf30f442fa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:22:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtv92,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtv92,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:22:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:22:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:22:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.225,StartTime:2022-12-14 16:22:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 16:22:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://1f27773dfc17fc1576606235af6b4ef10ffe6bf9ed95ff6490711645f10a2276,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 16:22:51.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4113" for this suite.

• [SLOW TEST:7.166 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":193,"skipped":3708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:51.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 16:22:51.588: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Dec 14 16:22:51.593: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 16:22:51.621: INFO: waiting for watch events with expected annotations
Dec 14 16:22:51.621: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Dec 14 16:22:51.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7032" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":194,"skipped":3740,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:51.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:22:51.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3612" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":195,"skipped":3743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:51.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8379
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8379
I1214 16:22:51.969523      14 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8379, replica count: 2
Dec 14 16:22:55.023: INFO: Creating new exec pod
I1214 16:22:55.023056      14 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:22:58.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8379 exec execpodfrdmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Dec 14 16:22:58.312: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 14 16:22:58.312: INFO: stdout: "externalname-service-dbl9n"
Dec 14 16:22:58.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8379 exec execpodfrdmv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.38.249 80'
Dec 14 16:22:58.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.38.249 80\nConnection to 10.233.38.249 80 port [tcp/http] succeeded!\n"
Dec 14 16:22:58.504: INFO: stdout: "externalname-service-dbl9n"
Dec 14 16:22:58.504: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:22:58.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8379" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.695 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":196,"skipped":3780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:22:58.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Dec 14 16:28:00.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6903" for this suite.

• [SLOW TEST:302.190 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":197,"skipped":3827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:00.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Dec 14 16:28:00.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5104" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":198,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:00.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Dec 14 16:28:01.040: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:28:03.061: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Dec 14 16:28:03.085: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:28:05.114: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 14 16:28:05.121: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.123: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.123: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 16:28:05.445: INFO: Exec stderr: ""
Dec 14 16:28:05.445: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.449: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.449: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 16:28:05.574: INFO: Exec stderr: ""
Dec 14 16:28:05.574: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.576: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.577: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 16:28:05.708: INFO: Exec stderr: ""
Dec 14 16:28:05.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.710: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.710: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 16:28:05.795: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 14 16:28:05.796: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.797: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 16:28:05.921: INFO: Exec stderr: ""
Dec 14 16:28:05.921: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:05.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:05.923: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:05.923: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Dec 14 16:28:06.053: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 14 16:28:06.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:06.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:06.055: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:06.055: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 16:28:06.171: INFO: Exec stderr: ""
Dec 14 16:28:06.171: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:06.173: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:06.173: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Dec 14 16:28:06.272: INFO: Exec stderr: ""
Dec 14 16:28:06.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:06.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:06.274: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:06.274: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 16:28:06.387: INFO: Exec stderr: ""
Dec 14 16:28:06.388: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4615 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:28:06.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:28:06.390: INFO: ExecWithOptions: Clientset creation
Dec 14 16:28:06.390: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4615/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Dec 14 16:28:06.489: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Dec 14 16:28:06.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4615" for this suite.

• [SLOW TEST:5.540 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3852,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:06.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-1465/secret-test-6c3c266b-3bfa-4ba4-9fb9-0f347cf7ca11
STEP: Creating a pod to test consume secrets
Dec 14 16:28:06.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de" in namespace "secrets-1465" to be "Succeeded or Failed"
Dec 14 16:28:06.600: INFO: Pod "pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.253204ms
Dec 14 16:28:08.613: INFO: Pod "pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017231398s
Dec 14 16:28:10.634: INFO: Pod "pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038735292s
STEP: Saw pod success
Dec 14 16:28:10.634: INFO: Pod "pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de" satisfied condition "Succeeded or Failed"
Dec 14 16:28:10.638: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de container env-test: <nil>
STEP: delete the pod
Dec 14 16:28:10.687: INFO: Waiting for pod pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de to disappear
Dec 14 16:28:10.691: INFO: Pod pod-configmaps-373cadb5-6e8a-490a-974b-d28c48e4d2de no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:28:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1465" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":200,"skipped":3858,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:10.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:28:10.752: INFO: Creating pod...
Dec 14 16:28:12.844: INFO: Creating service...
Dec 14 16:28:12.859: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=DELETE
Dec 14 16:28:12.870: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:28:12.870: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=OPTIONS
Dec 14 16:28:12.878: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:28:12.878: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=PATCH
Dec 14 16:28:12.888: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:28:12.888: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=POST
Dec 14 16:28:12.895: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:28:12.895: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=PUT
Dec 14 16:28:12.901: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 16:28:12.901: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=DELETE
Dec 14 16:28:12.911: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Dec 14 16:28:12.911: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=OPTIONS
Dec 14 16:28:12.919: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Dec 14 16:28:12.920: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=PATCH
Dec 14 16:28:12.926: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Dec 14 16:28:12.926: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=POST
Dec 14 16:28:12.933: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Dec 14 16:28:12.933: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=PUT
Dec 14 16:28:12.939: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Dec 14 16:28:12.939: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=GET
Dec 14 16:28:12.943: INFO: http.Client request:GET StatusCode:301
Dec 14 16:28:12.943: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=GET
Dec 14 16:28:12.954: INFO: http.Client request:GET StatusCode:301
Dec 14 16:28:12.954: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/pods/agnhost/proxy?method=HEAD
Dec 14 16:28:12.962: INFO: http.Client request:HEAD StatusCode:301
Dec 14 16:28:12.962: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7269/services/e2e-proxy-test-service/proxy?method=HEAD
Dec 14 16:28:12.973: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Dec 14 16:28:12.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7269" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":201,"skipped":3861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:12.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Dec 14 16:28:13.054: INFO: The status of Pod labelsupdatef2ce3784-a1b9-4297-84d3-921079e77197 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:28:15.066: INFO: The status of Pod labelsupdatef2ce3784-a1b9-4297-84d3-921079e77197 is Running (Ready = true)
Dec 14 16:28:15.609: INFO: Successfully updated pod "labelsupdatef2ce3784-a1b9-4297-84d3-921079e77197"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 16:28:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7950" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":202,"skipped":3901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:17.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-8p6j
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 16:28:17.753: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8p6j" in namespace "subpath-5953" to be "Succeeded or Failed"
Dec 14 16:28:17.756: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.792562ms
Dec 14 16:28:19.768: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 2.015601646s
Dec 14 16:28:21.780: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 4.027552219s
Dec 14 16:28:23.802: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 6.049193013s
Dec 14 16:28:25.817: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 8.064872375s
Dec 14 16:28:27.855: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 10.102163118s
Dec 14 16:28:29.868: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 12.115662043s
Dec 14 16:28:31.886: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 14.133698017s
Dec 14 16:28:33.904: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 16.151008718s
Dec 14 16:28:35.920: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 18.167028422s
Dec 14 16:28:37.938: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=true. Elapsed: 20.185683723s
Dec 14 16:28:39.948: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Running", Reason="", readiness=false. Elapsed: 22.195570299s
Dec 14 16:28:41.970: INFO: Pod "pod-subpath-test-secret-8p6j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.217544351s
STEP: Saw pod success
Dec 14 16:28:41.971: INFO: Pod "pod-subpath-test-secret-8p6j" satisfied condition "Succeeded or Failed"
Dec 14 16:28:41.977: INFO: Trying to get logs from node queith7zooya-3 pod pod-subpath-test-secret-8p6j container test-container-subpath-secret-8p6j: <nil>
STEP: delete the pod
Dec 14 16:28:42.019: INFO: Waiting for pod pod-subpath-test-secret-8p6j to disappear
Dec 14 16:28:42.024: INFO: Pod pod-subpath-test-secret-8p6j no longer exists
STEP: Deleting pod pod-subpath-test-secret-8p6j
Dec 14 16:28:42.024: INFO: Deleting pod "pod-subpath-test-secret-8p6j" in namespace "subpath-5953"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Dec 14 16:28:42.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5953" for this suite.

• [SLOW TEST:24.375 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":203,"skipped":3934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:42.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:28:42.118: INFO: The status of Pod busybox-scheduling-475fee73-fdde-41af-bb41-55a99f67a10d is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:28:44.126: INFO: The status of Pod busybox-scheduling-475fee73-fdde-41af-bb41-55a99f67a10d is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Dec 14 16:28:44.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8029" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":204,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:44.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 16:28:44.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8" in namespace "projected-6237" to be "Succeeded or Failed"
Dec 14 16:28:44.225: INFO: Pod "downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.600573ms
Dec 14 16:28:46.241: INFO: Pod "downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8": Phase="Running", Reason="", readiness=false. Elapsed: 2.020401482s
Dec 14 16:28:48.253: INFO: Pod "downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032681783s
STEP: Saw pod success
Dec 14 16:28:48.253: INFO: Pod "downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8" satisfied condition "Succeeded or Failed"
Dec 14 16:28:48.260: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8 container client-container: <nil>
STEP: delete the pod
Dec 14 16:28:48.287: INFO: Waiting for pod downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8 to disappear
Dec 14 16:28:48.292: INFO: Pod downwardapi-volume-4e60a5f3-433e-405c-a3be-6b6d99a0bda8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 16:28:48.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6237" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":4014,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:48.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8824
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8824
STEP: creating replication controller externalsvc in namespace services-8824
I1214 16:28:48.417674      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8824, replica count: 2
I1214 16:28:51.470716      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 14 16:28:51.502: INFO: Creating new exec pod
Dec 14 16:28:53.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8824 exec execpod8jflx -- /bin/sh -x -c nslookup clusterip-service.services-8824.svc.cluster.local'
Dec 14 16:28:53.940: INFO: stderr: "+ nslookup clusterip-service.services-8824.svc.cluster.local\n"
Dec 14 16:28:53.940: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-8824.svc.cluster.local\tcanonical name = externalsvc.services-8824.svc.cluster.local.\nName:\texternalsvc.services-8824.svc.cluster.local\nAddress: 10.233.37.30\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8824, will wait for the garbage collector to delete the pods
Dec 14 16:28:54.032: INFO: Deleting ReplicationController externalsvc took: 34.687889ms
Dec 14 16:28:54.133: INFO: Terminating ReplicationController externalsvc pods took: 100.770141ms
Dec 14 16:28:56.372: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:28:56.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8824" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":206,"skipped":4017,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:28:56.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-b34d3c72-5053-4dbc-882f-9882ada19173 in namespace container-probe-9822
Dec 14 16:28:58.494: INFO: Started pod liveness-b34d3c72-5053-4dbc-882f-9882ada19173 in namespace container-probe-9822
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:28:58.499: INFO: Initial restart count of pod liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is 0
Dec 14 16:29:18.648: INFO: Restart count of pod container-probe-9822/liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is now 1 (20.149341325s elapsed)
Dec 14 16:29:38.784: INFO: Restart count of pod container-probe-9822/liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is now 2 (40.284716083s elapsed)
Dec 14 16:29:58.951: INFO: Restart count of pod container-probe-9822/liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is now 3 (1m0.451993234s elapsed)
Dec 14 16:30:19.101: INFO: Restart count of pod container-probe-9822/liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is now 4 (1m20.601767976s elapsed)
Dec 14 16:31:29.565: INFO: Restart count of pod container-probe-9822/liveness-b34d3c72-5053-4dbc-882f-9882ada19173 is now 5 (2m31.065390447s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:31:29.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9822" for this suite.

• [SLOW TEST:153.198 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":4036,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:31:29.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-9fe71a01-5838-47eb-b583-e6861ba7921d
STEP: Creating a pod to test consume secrets
Dec 14 16:31:29.679: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367" in namespace "projected-1200" to be "Succeeded or Failed"
Dec 14 16:31:29.683: INFO: Pod "pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367": Phase="Pending", Reason="", readiness=false. Elapsed: 3.58054ms
Dec 14 16:31:31.696: INFO: Pod "pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017057323s
Dec 14 16:31:33.707: INFO: Pod "pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027480508s
STEP: Saw pod success
Dec 14 16:31:33.707: INFO: Pod "pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367" satisfied condition "Succeeded or Failed"
Dec 14 16:31:33.710: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:31:33.749: INFO: Waiting for pod pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367 to disappear
Dec 14 16:31:33.754: INFO: Pod pod-projected-secrets-df1e8dcd-d66c-4992-8c16-41091d3c0367 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:31:33.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1200" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":208,"skipped":4037,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:31:33.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Dec 14 16:31:33.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5908" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":4098,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:31:33.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Dec 14 16:31:33.958: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Dec 14 16:31:33.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8041" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":210,"skipped":4102,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:31:34.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 14 16:32:14.304: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 16:32:14.439: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 16:32:14.439: INFO: Deleting pod "simpletest.rc-2flhj" in namespace "gc-3481"
Dec 14 16:32:14.469: INFO: Deleting pod "simpletest.rc-2fqfh" in namespace "gc-3481"
Dec 14 16:32:14.535: INFO: Deleting pod "simpletest.rc-45ww7" in namespace "gc-3481"
Dec 14 16:32:14.570: INFO: Deleting pod "simpletest.rc-4c56m" in namespace "gc-3481"
Dec 14 16:32:14.594: INFO: Deleting pod "simpletest.rc-4zs4x" in namespace "gc-3481"
Dec 14 16:32:14.634: INFO: Deleting pod "simpletest.rc-5b8mh" in namespace "gc-3481"
Dec 14 16:32:14.688: INFO: Deleting pod "simpletest.rc-5kgr4" in namespace "gc-3481"
Dec 14 16:32:14.720: INFO: Deleting pod "simpletest.rc-6bks8" in namespace "gc-3481"
Dec 14 16:32:14.758: INFO: Deleting pod "simpletest.rc-6qmxd" in namespace "gc-3481"
Dec 14 16:32:14.803: INFO: Deleting pod "simpletest.rc-7c879" in namespace "gc-3481"
Dec 14 16:32:14.863: INFO: Deleting pod "simpletest.rc-7g4wg" in namespace "gc-3481"
Dec 14 16:32:14.897: INFO: Deleting pod "simpletest.rc-7jsqm" in namespace "gc-3481"
Dec 14 16:32:14.973: INFO: Deleting pod "simpletest.rc-86x8q" in namespace "gc-3481"
Dec 14 16:32:15.017: INFO: Deleting pod "simpletest.rc-8g4sp" in namespace "gc-3481"
Dec 14 16:32:15.063: INFO: Deleting pod "simpletest.rc-8k4sx" in namespace "gc-3481"
Dec 14 16:32:15.109: INFO: Deleting pod "simpletest.rc-8nfk5" in namespace "gc-3481"
Dec 14 16:32:15.160: INFO: Deleting pod "simpletest.rc-8rctt" in namespace "gc-3481"
Dec 14 16:32:15.209: INFO: Deleting pod "simpletest.rc-8rd8z" in namespace "gc-3481"
Dec 14 16:32:15.242: INFO: Deleting pod "simpletest.rc-8vkjd" in namespace "gc-3481"
Dec 14 16:32:15.298: INFO: Deleting pod "simpletest.rc-8zl4x" in namespace "gc-3481"
Dec 14 16:32:15.342: INFO: Deleting pod "simpletest.rc-92xmd" in namespace "gc-3481"
Dec 14 16:32:15.379: INFO: Deleting pod "simpletest.rc-9bqdv" in namespace "gc-3481"
Dec 14 16:32:15.412: INFO: Deleting pod "simpletest.rc-9mdbl" in namespace "gc-3481"
Dec 14 16:32:15.489: INFO: Deleting pod "simpletest.rc-9swn6" in namespace "gc-3481"
Dec 14 16:32:15.570: INFO: Deleting pod "simpletest.rc-b8vh4" in namespace "gc-3481"
Dec 14 16:32:15.621: INFO: Deleting pod "simpletest.rc-bhj7k" in namespace "gc-3481"
Dec 14 16:32:15.665: INFO: Deleting pod "simpletest.rc-bnf6s" in namespace "gc-3481"
Dec 14 16:32:15.695: INFO: Deleting pod "simpletest.rc-bxxxv" in namespace "gc-3481"
Dec 14 16:32:15.739: INFO: Deleting pod "simpletest.rc-c4cdz" in namespace "gc-3481"
Dec 14 16:32:15.768: INFO: Deleting pod "simpletest.rc-c5tm7" in namespace "gc-3481"
Dec 14 16:32:15.811: INFO: Deleting pod "simpletest.rc-c9nl4" in namespace "gc-3481"
Dec 14 16:32:15.871: INFO: Deleting pod "simpletest.rc-cfz48" in namespace "gc-3481"
Dec 14 16:32:15.907: INFO: Deleting pod "simpletest.rc-cgnpr" in namespace "gc-3481"
Dec 14 16:32:15.946: INFO: Deleting pod "simpletest.rc-cps4b" in namespace "gc-3481"
Dec 14 16:32:15.986: INFO: Deleting pod "simpletest.rc-dcc4t" in namespace "gc-3481"
Dec 14 16:32:16.049: INFO: Deleting pod "simpletest.rc-dsbvt" in namespace "gc-3481"
Dec 14 16:32:16.159: INFO: Deleting pod "simpletest.rc-dsgxg" in namespace "gc-3481"
Dec 14 16:32:16.204: INFO: Deleting pod "simpletest.rc-dwrts" in namespace "gc-3481"
Dec 14 16:32:16.255: INFO: Deleting pod "simpletest.rc-dz6l6" in namespace "gc-3481"
Dec 14 16:32:16.318: INFO: Deleting pod "simpletest.rc-f6vsb" in namespace "gc-3481"
Dec 14 16:32:16.408: INFO: Deleting pod "simpletest.rc-ff5kz" in namespace "gc-3481"
Dec 14 16:32:16.434: INFO: Deleting pod "simpletest.rc-fk6pq" in namespace "gc-3481"
Dec 14 16:32:16.513: INFO: Deleting pod "simpletest.rc-fkh6r" in namespace "gc-3481"
Dec 14 16:32:16.699: INFO: Deleting pod "simpletest.rc-fqff5" in namespace "gc-3481"
Dec 14 16:32:16.756: INFO: Deleting pod "simpletest.rc-j4knb" in namespace "gc-3481"
Dec 14 16:32:16.822: INFO: Deleting pod "simpletest.rc-jbtnn" in namespace "gc-3481"
Dec 14 16:32:16.865: INFO: Deleting pod "simpletest.rc-jh246" in namespace "gc-3481"
Dec 14 16:32:16.914: INFO: Deleting pod "simpletest.rc-jrd7x" in namespace "gc-3481"
Dec 14 16:32:16.961: INFO: Deleting pod "simpletest.rc-ktbdc" in namespace "gc-3481"
Dec 14 16:32:16.999: INFO: Deleting pod "simpletest.rc-kv4qr" in namespace "gc-3481"
Dec 14 16:32:17.052: INFO: Deleting pod "simpletest.rc-l8wbx" in namespace "gc-3481"
Dec 14 16:32:17.155: INFO: Deleting pod "simpletest.rc-lcgnn" in namespace "gc-3481"
Dec 14 16:32:17.216: INFO: Deleting pod "simpletest.rc-msfjx" in namespace "gc-3481"
Dec 14 16:32:17.272: INFO: Deleting pod "simpletest.rc-n4lfg" in namespace "gc-3481"
Dec 14 16:32:17.397: INFO: Deleting pod "simpletest.rc-n54t4" in namespace "gc-3481"
Dec 14 16:32:17.490: INFO: Deleting pod "simpletest.rc-nkbrv" in namespace "gc-3481"
Dec 14 16:32:17.550: INFO: Deleting pod "simpletest.rc-nsms6" in namespace "gc-3481"
Dec 14 16:32:17.633: INFO: Deleting pod "simpletest.rc-nvw4w" in namespace "gc-3481"
Dec 14 16:32:17.717: INFO: Deleting pod "simpletest.rc-nz5jx" in namespace "gc-3481"
Dec 14 16:32:17.767: INFO: Deleting pod "simpletest.rc-p278s" in namespace "gc-3481"
Dec 14 16:32:17.840: INFO: Deleting pod "simpletest.rc-p2888" in namespace "gc-3481"
Dec 14 16:32:17.914: INFO: Deleting pod "simpletest.rc-p7fnp" in namespace "gc-3481"
Dec 14 16:32:18.103: INFO: Deleting pod "simpletest.rc-p7xgt" in namespace "gc-3481"
Dec 14 16:32:18.133: INFO: Deleting pod "simpletest.rc-pc2hq" in namespace "gc-3481"
Dec 14 16:32:18.173: INFO: Deleting pod "simpletest.rc-pdw9h" in namespace "gc-3481"
Dec 14 16:32:18.200: INFO: Deleting pod "simpletest.rc-psmcg" in namespace "gc-3481"
Dec 14 16:32:18.242: INFO: Deleting pod "simpletest.rc-pwrz4" in namespace "gc-3481"
Dec 14 16:32:18.304: INFO: Deleting pod "simpletest.rc-q26rr" in namespace "gc-3481"
Dec 14 16:32:18.338: INFO: Deleting pod "simpletest.rc-q4wj4" in namespace "gc-3481"
Dec 14 16:32:18.368: INFO: Deleting pod "simpletest.rc-q8qx9" in namespace "gc-3481"
Dec 14 16:32:18.403: INFO: Deleting pod "simpletest.rc-qdj8h" in namespace "gc-3481"
Dec 14 16:32:18.445: INFO: Deleting pod "simpletest.rc-qhkr6" in namespace "gc-3481"
Dec 14 16:32:18.505: INFO: Deleting pod "simpletest.rc-qmz5f" in namespace "gc-3481"
Dec 14 16:32:18.571: INFO: Deleting pod "simpletest.rc-r7vzm" in namespace "gc-3481"
Dec 14 16:32:18.710: INFO: Deleting pod "simpletest.rc-r9xw5" in namespace "gc-3481"
Dec 14 16:32:18.740: INFO: Deleting pod "simpletest.rc-rckzx" in namespace "gc-3481"
Dec 14 16:32:18.792: INFO: Deleting pod "simpletest.rc-rrwdv" in namespace "gc-3481"
Dec 14 16:32:18.860: INFO: Deleting pod "simpletest.rc-rvjls" in namespace "gc-3481"
Dec 14 16:32:19.017: INFO: Deleting pod "simpletest.rc-s7dq5" in namespace "gc-3481"
Dec 14 16:32:19.037: INFO: Deleting pod "simpletest.rc-s8gqj" in namespace "gc-3481"
Dec 14 16:32:19.051: INFO: Deleting pod "simpletest.rc-s9n5n" in namespace "gc-3481"
Dec 14 16:32:19.092: INFO: Deleting pod "simpletest.rc-spn8z" in namespace "gc-3481"
Dec 14 16:32:19.137: INFO: Deleting pod "simpletest.rc-sppwn" in namespace "gc-3481"
Dec 14 16:32:19.223: INFO: Deleting pod "simpletest.rc-svwwg" in namespace "gc-3481"
Dec 14 16:32:19.276: INFO: Deleting pod "simpletest.rc-t7dn9" in namespace "gc-3481"
Dec 14 16:32:19.354: INFO: Deleting pod "simpletest.rc-tfmz8" in namespace "gc-3481"
Dec 14 16:32:19.441: INFO: Deleting pod "simpletest.rc-vbmcr" in namespace "gc-3481"
Dec 14 16:32:19.478: INFO: Deleting pod "simpletest.rc-vtkdh" in namespace "gc-3481"
Dec 14 16:32:19.530: INFO: Deleting pod "simpletest.rc-vx2bt" in namespace "gc-3481"
Dec 14 16:32:19.588: INFO: Deleting pod "simpletest.rc-vz7g2" in namespace "gc-3481"
Dec 14 16:32:19.645: INFO: Deleting pod "simpletest.rc-w4698" in namespace "gc-3481"
Dec 14 16:32:19.678: INFO: Deleting pod "simpletest.rc-wts4h" in namespace "gc-3481"
Dec 14 16:32:19.737: INFO: Deleting pod "simpletest.rc-wzcfc" in namespace "gc-3481"
Dec 14 16:32:19.797: INFO: Deleting pod "simpletest.rc-xdmkq" in namespace "gc-3481"
Dec 14 16:32:19.841: INFO: Deleting pod "simpletest.rc-z27gz" in namespace "gc-3481"
Dec 14 16:32:19.881: INFO: Deleting pod "simpletest.rc-zwdnd" in namespace "gc-3481"
Dec 14 16:32:19.953: INFO: Deleting pod "simpletest.rc-zzcms" in namespace "gc-3481"
Dec 14 16:32:20.011: INFO: Deleting pod "simpletest.rc-zzdtf" in namespace "gc-3481"
Dec 14 16:32:20.115: INFO: Deleting pod "simpletest.rc-zzgzg" in namespace "gc-3481"
Dec 14 16:32:20.159: INFO: Deleting pod "simpletest.rc-zzxcf" in namespace "gc-3481"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 16:32:20.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3481" for this suite.

• [SLOW TEST:46.211 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":211,"skipped":4119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:32:20.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:32:26.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1088" for this suite.
STEP: Destroying namespace "nsdeletetest-7330" for this suite.
Dec 14 16:32:26.937: INFO: Namespace nsdeletetest-7330 was already deleted
STEP: Destroying namespace "nsdeletetest-7737" for this suite.

• [SLOW TEST:6.695 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":212,"skipped":4215,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:32:26.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Dec 14 16:32:27.027: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:32:29.042: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Dec 14 16:32:29.066: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:32:31.077: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 14 16:32:31.136: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 16:32:31.140: INFO: Pod pod-with-poststart-http-hook still exists
Dec 14 16:32:33.141: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 14 16:32:33.153: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Dec 14 16:32:33.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3818" for this suite.

• [SLOW TEST:6.216 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":213,"skipped":4219,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:32:33.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Dec 14 16:32:33.223: INFO: The status of Pod annotationupdate1df04a76-26d1-4f87-9668-bcf11b415e88 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:32:35.232: INFO: The status of Pod annotationupdate1df04a76-26d1-4f87-9668-bcf11b415e88 is Running (Ready = true)
Dec 14 16:32:35.788: INFO: Successfully updated pod "annotationupdate1df04a76-26d1-4f87-9668-bcf11b415e88"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 16:32:37.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2067" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":4228,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:32:37.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:32:40.013: INFO: DNS probes using dns-test-121a4349-2dc9-4d6f-b94d-4ff4e4a28001 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:32:42.132: INFO: File wheezy_udp@dns-test-service-3.dns-602.svc.cluster.local from pod  dns-602/dns-test-a39f5844-b675-4ff7-b263-671e1ab6e255 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 14 16:32:42.137: INFO: File jessie_udp@dns-test-service-3.dns-602.svc.cluster.local from pod  dns-602/dns-test-a39f5844-b675-4ff7-b263-671e1ab6e255 contains '' instead of 'bar.example.com.'
Dec 14 16:32:42.137: INFO: Lookups using dns-602/dns-test-a39f5844-b675-4ff7-b263-671e1ab6e255 failed for: [wheezy_udp@dns-test-service-3.dns-602.svc.cluster.local jessie_udp@dns-test-service-3.dns-602.svc.cluster.local]

Dec 14 16:32:47.152: INFO: DNS probes using dns-test-a39f5844-b675-4ff7-b263-671e1ab6e255 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-602.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-602.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:32:51.251: INFO: DNS probes using dns-test-290dfcbf-58d8-4286-967f-48ca08ebf4fc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:32:51.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-602" for this suite.

• [SLOW TEST:13.490 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":215,"skipped":4230,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:32:51.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:32:51.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Dec 14 16:32:55.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 create -f -'
Dec 14 16:32:56.501: INFO: stderr: ""
Dec 14 16:32:56.501: INFO: stdout: "e2e-test-crd-publish-openapi-7102-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 16:32:56.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 delete e2e-test-crd-publish-openapi-7102-crds test-cr'
Dec 14 16:32:56.813: INFO: stderr: ""
Dec 14 16:32:56.813: INFO: stdout: "e2e-test-crd-publish-openapi-7102-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 14 16:32:56.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 apply -f -'
Dec 14 16:32:57.167: INFO: stderr: ""
Dec 14 16:32:57.167: INFO: stdout: "e2e-test-crd-publish-openapi-7102-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 14 16:32:57.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 delete e2e-test-crd-publish-openapi-7102-crds test-cr'
Dec 14 16:32:57.325: INFO: stderr: ""
Dec 14 16:32:57.325: INFO: stdout: "e2e-test-crd-publish-openapi-7102-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 14 16:32:57.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-3232 explain e2e-test-crd-publish-openapi-7102-crds'
Dec 14 16:32:58.238: INFO: stderr: ""
Dec 14 16:32:58.238: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7102-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:33:01.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3232" for this suite.

• [SLOW TEST:9.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":216,"skipped":4246,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:33:01.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-00f5fadf-6f7d-423a-b515-2f7b5f5c8727 in namespace container-probe-9194
Dec 14 16:33:03.379: INFO: Started pod busybox-00f5fadf-6f7d-423a-b515-2f7b5f5c8727 in namespace container-probe-9194
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:33:03.383: INFO: Initial restart count of pod busybox-00f5fadf-6f7d-423a-b515-2f7b5f5c8727 is 0
Dec 14 16:33:53.734: INFO: Restart count of pod container-probe-9194/busybox-00f5fadf-6f7d-423a-b515-2f7b5f5c8727 is now 1 (50.350843492s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:33:53.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9194" for this suite.

• [SLOW TEST:52.470 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":217,"skipped":4247,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:33:53.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 16:33:56.879: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Dec 14 16:33:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8386" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":218,"skipped":4264,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:33:56.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-3181ca0d-5cab-48ae-b50d-70a1eee295cb
STEP: Creating a pod to test consume secrets
Dec 14 16:33:56.993: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5" in namespace "projected-7314" to be "Succeeded or Failed"
Dec 14 16:33:57.011: INFO: Pod "pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.698129ms
Dec 14 16:33:59.026: INFO: Pod "pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033055539s
Dec 14 16:34:01.038: INFO: Pod "pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04522795s
STEP: Saw pod success
Dec 14 16:34:01.039: INFO: Pod "pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5" satisfied condition "Succeeded or Failed"
Dec 14 16:34:01.044: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:34:01.121: INFO: Waiting for pod pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5 to disappear
Dec 14 16:34:01.132: INFO: Pod pod-projected-secrets-e3a4d8fe-3476-45e9-8ad4-383eb194dcc5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:34:01.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7314" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":219,"skipped":4264,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:01.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Dec 14 16:34:05.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-939 pod-service-account-9802e1ae-874f-4c28-859c-45e63bf815ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 14 16:34:05.692: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-939 pod-service-account-9802e1ae-874f-4c28-859c-45e63bf815ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 14 16:34:06.028: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-939 pod-service-account-9802e1ae-874f-4c28-859c-45e63bf815ae -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Dec 14 16:34:06.384: INFO: Got root ca configmap in namespace "svcaccounts-939"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 16:34:06.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-939" for this suite.

• [SLOW TEST:5.249 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":220,"skipped":4270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:06.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:34:06.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1016" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":221,"skipped":4308,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:06.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Dec 14 16:34:08.576: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Dec 14 16:34:10.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-936" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":222,"skipped":4328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:10.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Dec 14 16:34:10.725: INFO: The status of Pod pod-hostip-10d3d984-8e75-4b0b-b35e-60d88c2e921e is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:34:12.745: INFO: The status of Pod pod-hostip-10d3d984-8e75-4b0b-b35e-60d88c2e921e is Running (Ready = true)
Dec 14 16:34:12.754: INFO: Pod pod-hostip-10d3d984-8e75-4b0b-b35e-60d88c2e921e has hostIP: 192.168.121.248
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 16:34:12.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5991" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":223,"skipped":4374,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 16:34:16.914: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Dec 14 16:34:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9320" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4378,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Dec 14 16:34:17.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Dec 14 16:34:17.741: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 14 16:34:19.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:21.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:23.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:25.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:27.837: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:29.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:31.839: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:33.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:35.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:37.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 34, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 16:34:40.404: INFO: Waited 532.403549ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Dec 14 16:34:40.616: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Dec 14 16:34:40.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5305" for this suite.

• [SLOW TEST:24.000 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":225,"skipped":4395,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:40.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-35d71260-7979-4910-aae6-13e04d32e413
STEP: Creating a pod to test consume secrets
Dec 14 16:34:41.040: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced" in namespace "projected-4283" to be "Succeeded or Failed"
Dec 14 16:34:41.045: INFO: Pod "pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057279ms
Dec 14 16:34:43.054: INFO: Pod "pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013989691s
Dec 14 16:34:45.069: INFO: Pod "pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028858559s
STEP: Saw pod success
Dec 14 16:34:45.070: INFO: Pod "pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced" satisfied condition "Succeeded or Failed"
Dec 14 16:34:45.077: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:34:45.135: INFO: Waiting for pod pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced to disappear
Dec 14 16:34:45.140: INFO: Pod pod-projected-secrets-39289e42-dbd7-4561-b4cf-be8ed025eced no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:34:45.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4283" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:45.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Dec 14 16:34:45.245: INFO: Waiting up to 5m0s for pod "security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0" in namespace "security-context-6648" to be "Succeeded or Failed"
Dec 14 16:34:45.250: INFO: Pod "security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.803511ms
Dec 14 16:34:47.260: INFO: Pod "security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014627305s
Dec 14 16:34:49.279: INFO: Pod "security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033740991s
STEP: Saw pod success
Dec 14 16:34:49.279: INFO: Pod "security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0" satisfied condition "Succeeded or Failed"
Dec 14 16:34:49.283: INFO: Trying to get logs from node queith7zooya-3 pod security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0 container test-container: <nil>
STEP: delete the pod
Dec 14 16:34:49.340: INFO: Waiting for pod security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0 to disappear
Dec 14 16:34:49.344: INFO: Pod security-context-98cbc237-2d2f-42b7-9062-4d9ec28a6bf0 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 16:34:49.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6648" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":227,"skipped":4419,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:49.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:34:49.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:34:50.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-45" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":228,"skipped":4424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:50.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:34:50.512: INFO: Waiting up to 5m0s for pod "busybox-user-65534-cc6e352b-ea0e-4980-8e02-80cd7c02798e" in namespace "security-context-test-4440" to be "Succeeded or Failed"
Dec 14 16:34:50.516: INFO: Pod "busybox-user-65534-cc6e352b-ea0e-4980-8e02-80cd7c02798e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696916ms
Dec 14 16:34:52.528: INFO: Pod "busybox-user-65534-cc6e352b-ea0e-4980-8e02-80cd7c02798e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015980974s
Dec 14 16:34:54.541: INFO: Pod "busybox-user-65534-cc6e352b-ea0e-4980-8e02-80cd7c02798e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02882555s
Dec 14 16:34:54.542: INFO: Pod "busybox-user-65534-cc6e352b-ea0e-4980-8e02-80cd7c02798e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 16:34:54.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4440" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":229,"skipped":4452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:54.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:34:54.634: INFO: Endpoints addresses: [192.168.121.111 192.168.121.209] , ports: [6443]
Dec 14 16:34:54.634: INFO: EndpointSlices addresses: [192.168.121.111 192.168.121.209] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Dec 14 16:34:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8244" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":230,"skipped":4478,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 14 16:34:54.696: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 16:34:58.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8738" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":4492,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:34:59.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 14 16:35:03.118: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Dec 14 16:35:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3502" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":232,"skipped":4494,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:35:03.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Dec 14 16:35:03.246: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1646  8c020ed3-5947-42e8-8b14-5b2aedee2242 23097 0 2022-12-14 16:35:03 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-12-14 16:35:03 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grrtw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grrtw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 14 16:35:03.257: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:35:05.269: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Dec 14 16:35:05.269: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1646 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:35:05.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:35:05.270: INFO: ExecWithOptions: Clientset creation
Dec 14 16:35:05.271: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1646/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Dec 14 16:35:05.607: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1646 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:35:05.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:35:05.612: INFO: ExecWithOptions: Clientset creation
Dec 14 16:35:05.613: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1646/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 16:35:05.762: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:35:05.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1646" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":233,"skipped":4545,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:35:05.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-mlbn
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 16:35:05.881: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mlbn" in namespace "subpath-9760" to be "Succeeded or Failed"
Dec 14 16:35:05.888: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.811166ms
Dec 14 16:35:07.904: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 2.022720336s
Dec 14 16:35:09.922: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 4.041293748s
Dec 14 16:35:11.936: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 6.054665696s
Dec 14 16:35:13.947: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 8.065717695s
Dec 14 16:35:15.963: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 10.081899958s
Dec 14 16:35:17.978: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 12.096697628s
Dec 14 16:35:19.991: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 14.109853565s
Dec 14 16:35:22.002: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 16.121111408s
Dec 14 16:35:24.011: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 18.13005247s
Dec 14 16:35:26.026: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=true. Elapsed: 20.144721855s
Dec 14 16:35:28.044: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Running", Reason="", readiness=false. Elapsed: 22.162802769s
Dec 14 16:35:30.058: INFO: Pod "pod-subpath-test-configmap-mlbn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.176608293s
STEP: Saw pod success
Dec 14 16:35:30.058: INFO: Pod "pod-subpath-test-configmap-mlbn" satisfied condition "Succeeded or Failed"
Dec 14 16:35:30.062: INFO: Trying to get logs from node queith7zooya-3 pod pod-subpath-test-configmap-mlbn container test-container-subpath-configmap-mlbn: <nil>
STEP: delete the pod
Dec 14 16:35:30.097: INFO: Waiting for pod pod-subpath-test-configmap-mlbn to disappear
Dec 14 16:35:30.101: INFO: Pod pod-subpath-test-configmap-mlbn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mlbn
Dec 14 16:35:30.101: INFO: Deleting pod "pod-subpath-test-configmap-mlbn" in namespace "subpath-9760"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Dec 14 16:35:30.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9760" for this suite.

• [SLOW TEST:24.315 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":234,"skipped":4555,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:35:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-ca09401b-cc90-4c9c-9191-ff2d8341e1ac in namespace container-probe-9980
Dec 14 16:35:32.217: INFO: Started pod busybox-ca09401b-cc90-4c9c-9191-ff2d8341e1ac in namespace container-probe-9980
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:35:32.224: INFO: Initial restart count of pod busybox-ca09401b-cc90-4c9c-9191-ff2d8341e1ac is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:39:34.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9980" for this suite.

• [SLOW TEST:243.995 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":235,"skipped":4574,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:39:34.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:39:35.176: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 16:39:37.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 16, 39, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 39, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 16, 39, 35, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 16, 39, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:39:40.240: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:39:40.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5695" for this suite.
STEP: Destroying namespace "webhook-5695-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.233 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":236,"skipped":4582,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:39:40.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Dec 14 16:39:42.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9388" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":237,"skipped":4584,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:39:42.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Dec 14 16:39:42.610: INFO: observed Pod pod-test in namespace pods-5499 in phase Pending with labels: map[test-pod-static:true] & conditions []
Dec 14 16:39:42.614: INFO: observed Pod pod-test in namespace pods-5499 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC  }]
Dec 14 16:39:42.643: INFO: observed Pod pod-test in namespace pods-5499 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC  }]
Dec 14 16:39:43.737: INFO: Found Pod pod-test in namespace pods-5499 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:39:42 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Dec 14 16:39:43.824: INFO: observed event type MODIFIED
Dec 14 16:39:45.759: INFO: observed event type MODIFIED
Dec 14 16:39:46.757: INFO: observed event type MODIFIED
Dec 14 16:39:46.769: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 16:39:46.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5499" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":238,"skipped":4590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:39:46.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Dec 14 16:39:46.960: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:39:48.970: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.111 on the node which pod1 resides and expect scheduled
Dec 14 16:39:48.989: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:39:51.016: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.111 but use UDP protocol on the node which pod2 resides
Dec 14 16:39:51.050: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:39:53.064: INFO: The status of Pod pod3 is Running (Ready = true)
Dec 14 16:39:53.091: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:39:55.101: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Dec 14 16:39:55.105: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.111 http://127.0.0.1:54323/hostname] Namespace:hostport-9833 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:39:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:39:55.107: INFO: ExecWithOptions: Clientset creation
Dec 14 16:39:55.107: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9833/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.111+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.111, port: 54323
Dec 14 16:39:55.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.111:54323/hostname] Namespace:hostport-9833 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:39:55.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:39:55.248: INFO: ExecWithOptions: Clientset creation
Dec 14 16:39:55.249: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9833/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.111%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.111, port: 54323 UDP
Dec 14 16:39:55.355: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 192.168.121.111 54323] Namespace:hostport-9833 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:39:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:39:55.357: INFO: ExecWithOptions: Clientset creation
Dec 14 16:39:55.357: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9833/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+192.168.121.111+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Dec 14 16:40:00.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9833" for this suite.

• [SLOW TEST:13.631 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":239,"skipped":4612,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:40:00.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4537
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4537
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4537
Dec 14 16:40:00.567: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:40:10.580: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 14 16:40:10.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:40:10.798: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:40:10.798: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:40:10.798: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:40:10.804: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 16:40:20.817: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:40:20.818: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:40:20.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999649s
Dec 14 16:40:21.857: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994480736s
Dec 14 16:40:22.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981422657s
Dec 14 16:40:23.878: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970249436s
Dec 14 16:40:24.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961100831s
Dec 14 16:40:25.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.953421424s
Dec 14 16:40:26.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.93797862s
Dec 14 16:40:27.927: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.927309365s
Dec 14 16:40:28.939: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.911501325s
Dec 14 16:40:29.951: INFO: Verifying statefulset ss doesn't scale past 1 for another 898.833081ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4537
Dec 14 16:40:30.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:40:31.497: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:40:31.497: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:40:31.497: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:40:31.508: INFO: Found 1 stateful pods, waiting for 3
Dec 14 16:40:41.538: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:40:41.538: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:40:41.538: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 14 16:40:41.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:40:41.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:40:41.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:40:41.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:40:41.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:40:42.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:40:42.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:40:42.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:40:42.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:40:42.377: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:40:42.377: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:40:42.377: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:40:42.377: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:40:42.384: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 16:40:52.411: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:40:52.411: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:40:52.411: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:40:52.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999452s
Dec 14 16:40:53.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99508678s
Dec 14 16:40:54.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983898956s
Dec 14 16:40:55.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.971882545s
Dec 14 16:40:56.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.959351316s
Dec 14 16:40:57.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.94574813s
Dec 14 16:40:58.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.936135004s
Dec 14 16:40:59.523: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922216506s
Dec 14 16:41:00.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.909063977s
Dec 14 16:41:01.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 892.781576ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4537
Dec 14 16:41:02.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:41:02.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:41:02.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:41:02.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:41:02.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:41:03.117: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:41:03.117: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:41:03.117: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:41:03.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-4537 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:41:03.298: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:41:03.298: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:41:03.298: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:41:03.298: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 16:41:13.336: INFO: Deleting all statefulset in ns statefulset-4537
Dec 14 16:41:13.341: INFO: Scaling statefulset ss to 0
Dec 14 16:41:13.393: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:41:13.397: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 16:41:13.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4537" for this suite.

• [SLOW TEST:72.935 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":240,"skipped":4616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:41:13.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:41:13.530: INFO: created pod
Dec 14 16:41:13.530: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8349" to be "Succeeded or Failed"
Dec 14 16:41:13.553: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.986012ms
Dec 14 16:41:15.576: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046161978s
Dec 14 16:41:17.583: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052416028s
STEP: Saw pod success
Dec 14 16:41:17.583: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Dec 14 16:41:47.584: INFO: polling logs
Dec 14 16:41:47.621: INFO: Pod logs: 
I1214 16:41:14.535280       1 log.go:195] OK: Got token
I1214 16:41:14.535804       1 log.go:195] validating with in-cluster discovery
I1214 16:41:14.537030       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I1214 16:41:14.537128       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8349:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036673, NotBefore:1671036073, IssuedAt:1671036073, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8349", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"41d3e137-38e4-48fa-b695-d3e4a0403149"}}}
I1214 16:41:14.581344       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I1214 16:41:14.595831       1 log.go:195] OK: Validated signature on JWT
I1214 16:41:14.596019       1 log.go:195] OK: Got valid claims from token!
I1214 16:41:14.596103       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8349:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1671036673, NotBefore:1671036073, IssuedAt:1671036073, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8349", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"41d3e137-38e4-48fa-b695-d3e4a0403149"}}}

Dec 14 16:41:47.621: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 16:41:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8349" for this suite.

• [SLOW TEST:34.219 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":241,"skipped":4639,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:41:47.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 16:41:58.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8544" for this suite.

• [SLOW TEST:11.335 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":242,"skipped":4650,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:41:58.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 16:41:59.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:41:59.090: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:00.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 16:42:00.118: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:01.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:42:01.111: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 14 16:42:01.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:42:01.191: INFO: Node queith7zooya-3 is running 0 daemon pod, expected 1
Dec 14 16:42:02.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 16:42:02.223: INFO: Node queith7zooya-3 is running 0 daemon pod, expected 1
Dec 14 16:42:03.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 16:42:03.206: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4939, will wait for the garbage collector to delete the pods
Dec 14 16:42:03.283: INFO: Deleting DaemonSet.extensions daemon-set took: 13.823367ms
Dec 14 16:42:03.384: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.076045ms
Dec 14 16:42:05.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:05.693: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:42:05.699: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24374"},"items":null}

Dec 14 16:42:05.702: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24374"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:42:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4939" for this suite.

• [SLOW TEST:6.736 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":243,"skipped":4669,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:42:05.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 14 16:42:05.805: INFO: Waiting up to 5m0s for pod "pod-08a004cd-a72e-4b63-aa17-96355bf9e3be" in namespace "emptydir-8603" to be "Succeeded or Failed"
Dec 14 16:42:05.808: INFO: Pod "pod-08a004cd-a72e-4b63-aa17-96355bf9e3be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685294ms
Dec 14 16:42:07.823: INFO: Pod "pod-08a004cd-a72e-4b63-aa17-96355bf9e3be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017768987s
Dec 14 16:42:09.848: INFO: Pod "pod-08a004cd-a72e-4b63-aa17-96355bf9e3be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043208078s
STEP: Saw pod success
Dec 14 16:42:09.848: INFO: Pod "pod-08a004cd-a72e-4b63-aa17-96355bf9e3be" satisfied condition "Succeeded or Failed"
Dec 14 16:42:09.859: INFO: Trying to get logs from node queith7zooya-3 pod pod-08a004cd-a72e-4b63-aa17-96355bf9e3be container test-container: <nil>
STEP: delete the pod
Dec 14 16:42:09.887: INFO: Waiting for pod pod-08a004cd-a72e-4b63-aa17-96355bf9e3be to disappear
Dec 14 16:42:09.890: INFO: Pod pod-08a004cd-a72e-4b63-aa17-96355bf9e3be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:42:09.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8603" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":244,"skipped":4683,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:42:09.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Dec 14 16:42:22.417: INFO: 71 pods remaining
Dec 14 16:42:22.417: INFO: 71 pods has nil DeletionTimestamp
Dec 14 16:42:22.417: INFO: 
STEP: Gathering metrics
Dec 14 16:42:27.864: INFO: The status of Pod kube-controller-manager-queith7zooya-2 is Running (Ready = true)
Dec 14 16:42:28.087: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Dec 14 16:42:28.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-26jvp" in namespace "gc-2421"
Dec 14 16:42:28.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-296ns" in namespace "gc-2421"
Dec 14 16:42:28.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mdhp" in namespace "gc-2421"
Dec 14 16:42:28.316: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qzpk" in namespace "gc-2421"
Dec 14 16:42:28.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b2dk" in namespace "gc-2421"
Dec 14 16:42:28.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hw4n" in namespace "gc-2421"
Dec 14 16:42:28.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nsj2" in namespace "gc-2421"
Dec 14 16:42:28.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-59g7g" in namespace "gc-2421"
Dec 14 16:42:28.589: INFO: Deleting pod "simpletest-rc-to-be-deleted-59sv6" in namespace "gc-2421"
Dec 14 16:42:28.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hhl6" in namespace "gc-2421"
Dec 14 16:42:28.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-5v9k2" in namespace "gc-2421"
Dec 14 16:42:28.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wtgr" in namespace "gc-2421"
Dec 14 16:42:28.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-646hv" in namespace "gc-2421"
Dec 14 16:42:28.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-676nl" in namespace "gc-2421"
Dec 14 16:42:28.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qhjl" in namespace "gc-2421"
Dec 14 16:42:28.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-77ptt" in namespace "gc-2421"
Dec 14 16:42:28.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c2vs" in namespace "gc-2421"
Dec 14 16:42:28.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jcg8" in namespace "gc-2421"
Dec 14 16:42:28.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mhfq" in namespace "gc-2421"
Dec 14 16:42:29.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rc5f" in namespace "gc-2421"
Dec 14 16:42:29.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-87fd8" in namespace "gc-2421"
Dec 14 16:42:29.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-89qbf" in namespace "gc-2421"
Dec 14 16:42:29.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bw4k" in namespace "gc-2421"
Dec 14 16:42:29.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f7g9" in namespace "gc-2421"
Dec 14 16:42:29.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mcc5" in namespace "gc-2421"
Dec 14 16:42:29.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rlgl" in namespace "gc-2421"
Dec 14 16:42:29.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zmds" in namespace "gc-2421"
Dec 14 16:42:29.546: INFO: Deleting pod "simpletest-rc-to-be-deleted-92jjh" in namespace "gc-2421"
Dec 14 16:42:29.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c597" in namespace "gc-2421"
Dec 14 16:42:29.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v5p7" in namespace "gc-2421"
Dec 14 16:42:29.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xlkm" in namespace "gc-2421"
Dec 14 16:42:29.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-b4d5m" in namespace "gc-2421"
Dec 14 16:42:29.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-bclh8" in namespace "gc-2421"
Dec 14 16:42:29.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-c428h" in namespace "gc-2421"
Dec 14 16:42:29.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7mlv" in namespace "gc-2421"
Dec 14 16:42:29.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzbx" in namespace "gc-2421"
Dec 14 16:42:29.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjxch" in namespace "gc-2421"
Dec 14 16:42:30.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7q5t" in namespace "gc-2421"
Dec 14 16:42:30.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxsmn" in namespace "gc-2421"
Dec 14 16:42:30.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-fk75j" in namespace "gc-2421"
Dec 14 16:42:30.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmcp5" in namespace "gc-2421"
Dec 14 16:42:30.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2gvq" in namespace "gc-2421"
Dec 14 16:42:30.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-g78zd" in namespace "gc-2421"
Dec 14 16:42:30.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9677" in namespace "gc-2421"
Dec 14 16:42:30.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbrvf" in namespace "gc-2421"
Dec 14 16:42:30.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd99p" in namespace "gc-2421"
Dec 14 16:42:30.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7fcg" in namespace "gc-2421"
Dec 14 16:42:30.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjn5c" in namespace "gc-2421"
Dec 14 16:42:30.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7hb4" in namespace "gc-2421"
Dec 14 16:42:30.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdg2f" in namespace "gc-2421"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 16:42:30.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2421" for this suite.

• [SLOW TEST:20.607 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":245,"skipped":4691,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:42:30.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Dec 14 16:42:30.626: INFO: created test-podtemplate-1
Dec 14 16:42:30.633: INFO: created test-podtemplate-2
Dec 14 16:42:30.643: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Dec 14 16:42:30.660: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Dec 14 16:42:30.706: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Dec 14 16:42:30.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2725" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":246,"skipped":4692,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:42:30.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:42:30.976: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 14 16:42:31.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:31.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Dec 14 16:42:31.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:31.056: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:32.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:32.070: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:33.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:33.069: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:34.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:34.068: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:35.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:35.065: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:36.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:36.072: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:37.067: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 16:42:37.067: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 14 16:42:37.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 16:42:37.144: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Dec 14 16:42:38.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:38.156: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 14 16:42:38.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:38.185: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:39.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:39.196: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:40.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:40.199: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 16:42:41.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 16:42:41.195: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8014, will wait for the garbage collector to delete the pods
Dec 14 16:42:41.296: INFO: Deleting DaemonSet.extensions daemon-set took: 21.027507ms
Dec 14 16:42:41.397: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.736788ms
Dec 14 16:42:43.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 16:42:43.711: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 16:42:43.718: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26122"},"items":null}

Dec 14 16:42:43.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26122"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:42:43.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8014" for this suite.

• [SLOW TEST:13.035 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":247,"skipped":4702,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:42:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-rgh7
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 16:42:43.883: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rgh7" in namespace "subpath-3475" to be "Succeeded or Failed"
Dec 14 16:42:43.891: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.442937ms
Dec 14 16:42:45.905: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 2.021562604s
Dec 14 16:42:47.912: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 4.028849618s
Dec 14 16:42:49.923: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 6.039961598s
Dec 14 16:42:51.934: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 8.051073464s
Dec 14 16:42:53.950: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 10.066847388s
Dec 14 16:42:55.962: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 12.078511151s
Dec 14 16:42:57.971: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 14.087514117s
Dec 14 16:42:59.992: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 16.108631819s
Dec 14 16:43:02.007: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 18.124178494s
Dec 14 16:43:04.016: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=true. Elapsed: 20.133188356s
Dec 14 16:43:06.033: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Running", Reason="", readiness=false. Elapsed: 22.150019861s
Dec 14 16:43:08.049: INFO: Pod "pod-subpath-test-configmap-rgh7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.166271426s
STEP: Saw pod success
Dec 14 16:43:08.050: INFO: Pod "pod-subpath-test-configmap-rgh7" satisfied condition "Succeeded or Failed"
Dec 14 16:43:08.055: INFO: Trying to get logs from node queith7zooya-3 pod pod-subpath-test-configmap-rgh7 container test-container-subpath-configmap-rgh7: <nil>
STEP: delete the pod
Dec 14 16:43:08.094: INFO: Waiting for pod pod-subpath-test-configmap-rgh7 to disappear
Dec 14 16:43:08.098: INFO: Pod pod-subpath-test-configmap-rgh7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rgh7
Dec 14 16:43:08.099: INFO: Deleting pod "pod-subpath-test-configmap-rgh7" in namespace "subpath-3475"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Dec 14 16:43:08.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3475" for this suite.

• [SLOW TEST:24.333 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":248,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:43:08.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1543.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1543.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1543.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1543.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:43:10.308: INFO: DNS probes using dns-1543/dns-test-fb75b895-06de-4845-9afb-2ef86087fdf7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:43:10.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1543" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":249,"skipped":4771,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:43:10.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-0afdb0bd-b0cb-4d26-950e-f7d608a1b9f7
STEP: Creating a pod to test consume configMaps
Dec 14 16:43:10.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8" in namespace "configmap-1234" to be "Succeeded or Failed"
Dec 14 16:43:10.422: INFO: Pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118621ms
Dec 14 16:43:12.435: INFO: Pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8": Phase="Running", Reason="", readiness=false. Elapsed: 2.016354443s
Dec 14 16:43:14.447: INFO: Pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8": Phase="Running", Reason="", readiness=false. Elapsed: 4.028410798s
Dec 14 16:43:16.454: INFO: Pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035453196s
STEP: Saw pod success
Dec 14 16:43:16.454: INFO: Pod "pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8" satisfied condition "Succeeded or Failed"
Dec 14 16:43:16.457: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:43:16.494: INFO: Waiting for pod pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8 to disappear
Dec 14 16:43:16.496: INFO: Pod pod-configmaps-63c7ecd3-b327-40ea-868f-7296651a78c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 16:43:16.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1234" for this suite.

• [SLOW TEST:6.145 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:43:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 16:43:16.558: INFO: Waiting up to 5m0s for pod "pod-9aabe297-8620-48b5-bde2-f58a8e557df0" in namespace "emptydir-8730" to be "Succeeded or Failed"
Dec 14 16:43:16.567: INFO: Pod "pod-9aabe297-8620-48b5-bde2-f58a8e557df0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117844ms
Dec 14 16:43:18.583: INFO: Pod "pod-9aabe297-8620-48b5-bde2-f58a8e557df0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024135927s
Dec 14 16:43:20.601: INFO: Pod "pod-9aabe297-8620-48b5-bde2-f58a8e557df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04215055s
STEP: Saw pod success
Dec 14 16:43:20.601: INFO: Pod "pod-9aabe297-8620-48b5-bde2-f58a8e557df0" satisfied condition "Succeeded or Failed"
Dec 14 16:43:20.615: INFO: Trying to get logs from node queith7zooya-3 pod pod-9aabe297-8620-48b5-bde2-f58a8e557df0 container test-container: <nil>
STEP: delete the pod
Dec 14 16:43:20.644: INFO: Waiting for pod pod-9aabe297-8620-48b5-bde2-f58a8e557df0 to disappear
Dec 14 16:43:20.648: INFO: Pod pod-9aabe297-8620-48b5-bde2-f58a8e557df0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:43:20.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8730" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":251,"skipped":4800,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:43:20.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:43:22.513: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:43:25.572: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:43:25.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:43:28.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1047" for this suite.
STEP: Destroying namespace "webhook-1047-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.346 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":252,"skipped":4829,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:43:29.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8402.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 14 16:43:33.219: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.225: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.232: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.236: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.241: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.246: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.251: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.255: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:33.255: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:43:38.267: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.271: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.276: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.280: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.287: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.291: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.294: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.298: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:38.299: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:43:43.261: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.266: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.270: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.273: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.277: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.280: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.285: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.289: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:43.290: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:43:48.263: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.270: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.274: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.277: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.282: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.285: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.290: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.296: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:48.296: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:43:53.262: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.267: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.272: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.276: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.280: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.284: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.288: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.293: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:53.293: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:43:58.264: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.269: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.275: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.280: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.292: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.300: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.304: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.309: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local from pod dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c: the server could not find the requested resource (get pods dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c)
Dec 14 16:43:58.309: INFO: Lookups using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8402.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8402.svc.cluster.local jessie_udp@dns-test-service-2.dns-8402.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8402.svc.cluster.local]

Dec 14 16:44:03.305: INFO: DNS probes using dns-8402/dns-test-e2ecfa1c-a70f-4df9-bdc7-e408d0f7b24c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Dec 14 16:44:03.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8402" for this suite.

• [SLOW TEST:34.358 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":253,"skipped":4831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:03.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Dec 14 16:44:03.458: INFO: starting watch
STEP: patching
STEP: updating
Dec 14 16:44:03.473: INFO: waiting for watch events with expected annotations
Dec 14 16:44:03.473: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Dec 14 16:44:03.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-890" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":254,"skipped":4926,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:03.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:44:05.176: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:44:08.246: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:44:08.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1703" for this suite.
STEP: Destroying namespace "webhook-1703-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.266 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":255,"skipped":4929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Dec 14 16:44:08.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-944 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Dec 14 16:44:09.049: INFO: stderr: ""
Dec 14 16:44:09.049: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Dec 14 16:44:09.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-944 delete pods e2e-test-httpd-pod'
Dec 14 16:44:12.404: INFO: stderr: ""
Dec 14 16:44:12.404: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:44:12.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-944" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":256,"skipped":4955,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:12.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Dec 14 16:44:16.543: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3300 PodName:pod-sharedvolume-eb5e671f-5119-4b69-a64d-40cbf28e06a5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 16:44:16.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 16:44:16.545: INFO: ExecWithOptions: Clientset creation
Dec 14 16:44:16.545: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-3300/pods/pod-sharedvolume-eb5e671f-5119-4b69-a64d-40cbf28e06a5/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Dec 14 16:44:16.664: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:44:16.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3300" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":257,"skipped":4960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:16.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 16:44:16.740: INFO: Waiting up to 5m0s for pod "pod-20a34482-2bc2-44d1-95b5-c91f4995b00d" in namespace "emptydir-842" to be "Succeeded or Failed"
Dec 14 16:44:16.744: INFO: Pod "pod-20a34482-2bc2-44d1-95b5-c91f4995b00d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.472831ms
Dec 14 16:44:18.758: INFO: Pod "pod-20a34482-2bc2-44d1-95b5-c91f4995b00d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018001151s
Dec 14 16:44:20.775: INFO: Pod "pod-20a34482-2bc2-44d1-95b5-c91f4995b00d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035245832s
STEP: Saw pod success
Dec 14 16:44:20.776: INFO: Pod "pod-20a34482-2bc2-44d1-95b5-c91f4995b00d" satisfied condition "Succeeded or Failed"
Dec 14 16:44:20.782: INFO: Trying to get logs from node queith7zooya-3 pod pod-20a34482-2bc2-44d1-95b5-c91f4995b00d container test-container: <nil>
STEP: delete the pod
Dec 14 16:44:20.834: INFO: Waiting for pod pod-20a34482-2bc2-44d1-95b5-c91f4995b00d to disappear
Dec 14 16:44:20.838: INFO: Pod pod-20a34482-2bc2-44d1-95b5-c91f4995b00d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 16:44:20.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-842" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":258,"skipped":4991,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:20.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Dec 14 16:44:22.992: INFO: running pods: 0 < 3
Dec 14 16:44:25.005: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Dec 14 16:44:27.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-867" for this suite.

• [SLOW TEST:6.183 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":259,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:27.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:44:28.246: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:44:31.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:44:31.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8675" for this suite.
STEP: Destroying namespace "webhook-8675-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":260,"skipped":5018,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 16:44:31.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca" in namespace "projected-5584" to be "Succeeded or Failed"
Dec 14 16:44:31.530: INFO: Pod "downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.798126ms
Dec 14 16:44:33.539: INFO: Pod "downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca": Phase="Running", Reason="", readiness=false. Elapsed: 2.018573998s
Dec 14 16:44:35.552: INFO: Pod "downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031011586s
STEP: Saw pod success
Dec 14 16:44:35.552: INFO: Pod "downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca" satisfied condition "Succeeded or Failed"
Dec 14 16:44:35.557: INFO: Trying to get logs from node queith7zooya-1 pod downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca container client-container: <nil>
STEP: delete the pod
Dec 14 16:44:35.597: INFO: Waiting for pod downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca to disappear
Dec 14 16:44:35.600: INFO: Pod downwardapi-volume-5eb10714-50a3-40a3-aa78-834b1f8394ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 16:44:35.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5584" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":261,"skipped":5019,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:35.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-3401
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3401 to expose endpoints map[]
Dec 14 16:44:35.687: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Dec 14 16:44:36.700: INFO: successfully validated that service multi-endpoint-test in namespace services-3401 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3401
Dec 14 16:44:36.716: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:44:38.726: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3401 to expose endpoints map[pod1:[100]]
Dec 14 16:44:38.743: INFO: successfully validated that service multi-endpoint-test in namespace services-3401 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3401
Dec 14 16:44:38.759: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:44:40.772: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3401 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 14 16:44:40.806: INFO: successfully validated that service multi-endpoint-test in namespace services-3401 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Dec 14 16:44:40.806: INFO: Creating new exec pod
Dec 14 16:44:43.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-3401 exec execpodk88s2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Dec 14 16:44:44.223: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:44.223: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:44:44.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-3401 exec execpodk88s2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.189 80'
Dec 14 16:44:44.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.189 80\nConnection to 10.233.41.189 80 port [tcp/http] succeeded!\n"
Dec 14 16:44:44.456: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:44:44.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-3401 exec execpodk88s2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Dec 14 16:44:44.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Dec 14 16:44:44.748: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:44:44.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-3401 exec execpodk88s2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.189 81'
Dec 14 16:44:44.949: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.189 81\nConnection to 10.233.41.189 81 port [tcp/*] succeeded!\n"
Dec 14 16:44:44.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3401
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3401 to expose endpoints map[pod2:[101]]
Dec 14 16:44:45.010: INFO: successfully validated that service multi-endpoint-test in namespace services-3401 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3401
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3401 to expose endpoints map[]
Dec 14 16:44:45.064: INFO: successfully validated that service multi-endpoint-test in namespace services-3401 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:44:45.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3401" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.534 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":262,"skipped":5034,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:45.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:44:45.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:44:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7565" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":263,"skipped":5039,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:44:45.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-d8cb2ac4-f4b2-427c-b168-7e311cf33ed2 in namespace container-probe-7597
Dec 14 16:44:48.028: INFO: Started pod liveness-d8cb2ac4-f4b2-427c-b168-7e311cf33ed2 in namespace container-probe-7597
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:44:48.032: INFO: Initial restart count of pod liveness-d8cb2ac4-f4b2-427c-b168-7e311cf33ed2 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:48:49.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7597" for this suite.

• [SLOW TEST:243.992 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":5059,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:48:49.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:48:49.937: INFO: Creating deployment "test-recreate-deployment"
Dec 14 16:48:49.949: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 14 16:48:49.961: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 14 16:48:51.976: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 14 16:48:51.979: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 14 16:48:51.993: INFO: Updating deployment test-recreate-deployment
Dec 14 16:48:51.993: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 16:48:52.200: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9085  e080a38b-7ede-4626-bcfe-b3e9d4de95ed 27431 2 2022-12-14 16:48:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-14 16:48:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004456888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-12-14 16:48:52 +0000 UTC,LastTransitionTime:2022-12-14 16:48:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2022-12-14 16:48:52 +0000 UTC,LastTransitionTime:2022-12-14 16:48:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 14 16:48:52.205: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-9085  84936911-67ed-41da-a4c7-58d5fc192f75 27427 1 2022-12-14 16:48:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e080a38b-7ede-4626-bcfe-b3e9d4de95ed 0xc003ff97e0 0xc003ff97e1}] []  [{kube-controller-manager Update apps/v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e080a38b-7ede-4626-bcfe-b3e9d4de95ed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ff9878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:48:52.205: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 14 16:48:52.206: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-9085  8d70a5e2-31a9-475a-96f9-d7d182d2bf94 27419 2 2022-12-14 16:48:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e080a38b-7ede-4626-bcfe-b3e9d4de95ed 0xc003ff96c7 0xc003ff96c8}] []  [{kube-controller-manager Update apps/v1 2022-12-14 16:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e080a38b-7ede-4626-bcfe-b3e9d4de95ed\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ff9778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 16:48:52.210: INFO: Pod "test-recreate-deployment-cd8586fc7-fbpz6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-fbpz6 test-recreate-deployment-cd8586fc7- deployment-9085  6d67f85e-0987-407e-9016-f7d7460fcc6d 27430 0 2022-12-14 16:48:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 84936911-67ed-41da-a4c7-58d5fc192f75 0xc004456c10 0xc004456c11}] []  [{kube-controller-manager Update v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84936911-67ed-41da-a4c7-58d5fc192f75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 16:48:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6j47d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6j47d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:48:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:48:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 16:48:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:,StartTime:2022-12-14 16:48:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 16:48:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9085" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":265,"skipped":5077,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:48:52.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:48:52.294: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 14 16:48:54.371: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Dec 14 16:48:55.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2282" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":266,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:48:55.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Dec 14 16:48:55.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5706" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":267,"skipped":5117,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:48:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 16:49:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2373" for this suite.

• [SLOW TEST:11.183 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":268,"skipped":5118,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:49:06.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 16:49:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7661" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":269,"skipped":5138,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:49:06.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Dec 14 16:54:06.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2095" for this suite.

• [SLOW TEST:300.116 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":270,"skipped":5141,"failed":0}
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:54:06.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Dec 14 16:54:06.944: INFO: PodSpec: initContainers in spec.initContainers
Dec 14 16:54:48.932: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8fdc30f9-a8ba-4375-a633-8f0190780788", GenerateName:"", Namespace:"init-container-939", SelfLink:"", UID:"d152f713-ef00-4bce-a0c0-e5e1a2082cf0", ResourceVersion:"28143", Generation:0, CreationTimestamp:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"944419245"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e52270), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.December, 14, 16, 54, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004e522a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-w769n", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc002d023a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w769n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w769n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w769n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004130608), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"queith7zooya-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0011fc4d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004130690)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0041306b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0041306b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0041306bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00679c0d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.248", PodIP:"10.233.66.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.107"}}, StartTime:time.Date(2022, time.December, 14, 16, 54, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0011fc5b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0011fc620)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://2f8bcbc53c8b1ae538605707699a03ee1bff9235ce858d0a627960742a29cbb8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d02440), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d02400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc004130734)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Dec 14 16:54:48.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-939" for this suite.

• [SLOW TEST:42.082 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":271,"skipped":5141,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:54:48.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 16:54:49.053: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616" in namespace "downward-api-4197" to be "Succeeded or Failed"
Dec 14 16:54:49.059: INFO: Pod "downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616": Phase="Pending", Reason="", readiness=false. Elapsed: 5.804763ms
Dec 14 16:54:51.070: INFO: Pod "downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017068756s
Dec 14 16:54:53.086: INFO: Pod "downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03243174s
STEP: Saw pod success
Dec 14 16:54:53.086: INFO: Pod "downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616" satisfied condition "Succeeded or Failed"
Dec 14 16:54:53.103: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616 container client-container: <nil>
STEP: delete the pod
Dec 14 16:54:53.423: INFO: Waiting for pod downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616 to disappear
Dec 14 16:54:53.427: INFO: Pod downwardapi-volume-23d76f46-f7b0-4534-8de2-fa827e35e616 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 16:54:53.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4197" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":272,"skipped":5142,"failed":0}
SS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:54:53.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Dec 14 16:54:53.506: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Dec 14 16:54:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5366" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":273,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:54:53.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Dec 14 16:54:53.584: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:54:55.595: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Dec 14 16:54:55.613: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:54:57.628: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Dec 14 16:54:57.652: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:54:57.657: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 16:54:59.657: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:54:59.671: INFO: Pod pod-with-prestop-http-hook still exists
Dec 14 16:55:01.659: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 14 16:55:01.667: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Dec 14 16:55:01.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8429" for this suite.

• [SLOW TEST:8.161 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":5173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:55:01.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5412
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:55:01.751: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:55:11.767: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Dec 14 16:55:11.807: INFO: Found 1 stateful pods, waiting for 2
Dec 14 16:55:21.827: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:55:21.827: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 16:55:21.872: INFO: Deleting all statefulset in ns statefulset-5412
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 16:55:21.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5412" for this suite.

• [SLOW TEST:20.220 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":275,"skipped":5201,"failed":0}
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:55:21.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Dec 14 16:55:21.988: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 16:56:22.071: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:56:22.080: INFO: Starting informer...
STEP: Starting pods...
Dec 14 16:56:22.313: INFO: Pod1 is running on queith7zooya-3. Tainting Node
Dec 14 16:56:24.546: INFO: Pod2 is running on queith7zooya-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 14 16:56:30.837: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 14 16:56:50.857: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Dec 14 16:56:50.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9606" for this suite.

• [SLOW TEST:89.066 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":276,"skipped":5207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:56:50.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:56:51.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 16:56:51.037: INFO: The status of Pod pod-exec-websocket-686296fd-1c71-4c28-ab5d-fe8fb66b82f6 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:56:53.054: INFO: The status of Pod pod-exec-websocket-686296fd-1c71-4c28-ab5d-fe8fb66b82f6 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 16:56:53.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9752" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":277,"skipped":5254,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:56:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:56:53.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:56:59.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5009" for this suite.

• [SLOW TEST:6.432 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":278,"skipped":5263,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:56:59.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Dec 14 16:56:59.669: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-8621 proxy --unix-socket=/tmp/kubectl-proxy-unix2738712290/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:56:59.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8621" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":279,"skipped":5270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:56:59.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 16:57:06.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1590" for this suite.

• [SLOW TEST:7.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":280,"skipped":5313,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:06.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8606
STEP: creating service affinity-nodeport in namespace services-8606
STEP: creating replication controller affinity-nodeport in namespace services-8606
I1214 16:57:07.053999      14 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8606, replica count: 3
I1214 16:57:10.108349      14 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 16:57:10.142: INFO: Creating new exec pod
Dec 14 16:57:13.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8606 exec execpod-affinity484vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Dec 14 16:57:13.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Dec 14 16:57:13.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:57:13.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8606 exec execpod-affinity484vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.17.78 80'
Dec 14 16:57:13.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.17.78 80\nConnection to 10.233.17.78 80 port [tcp/http] succeeded!\n"
Dec 14 16:57:13.942: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:57:13.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8606 exec execpod-affinity484vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.248 32191'
Dec 14 16:57:14.374: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.248 32191\nConnection to 192.168.121.248 32191 port [tcp/*] succeeded!\n"
Dec 14 16:57:14.374: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:57:14.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8606 exec execpod-affinity484vw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.111 32191'
Dec 14 16:57:14.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.111 32191\nConnection to 192.168.121.111 32191 port [tcp/*] succeeded!\n"
Dec 14 16:57:14.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Dec 14 16:57:14.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-8606 exec execpod-affinity484vw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.209:32191/ ; done'
Dec 14 16:57:14.941: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.209:32191/\n"
Dec 14 16:57:14.941: INFO: stdout: "\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg\naffinity-nodeport-xsxtg"
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Received response from host: affinity-nodeport-xsxtg
Dec 14 16:57:14.942: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8606, will wait for the garbage collector to delete the pods
Dec 14 16:57:15.074: INFO: Deleting ReplicationController affinity-nodeport took: 24.574144ms
Dec 14 16:57:15.174: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.696503ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 16:57:17.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8606" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.991 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":281,"skipped":5314,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:17.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 16:57:18.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7434" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":282,"skipped":5320,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:18.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Dec 14 16:57:18.149: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:57:20.157: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 14 16:57:21.193: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 16:57:22.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9576" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":283,"skipped":5340,"failed":0}
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:22.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:57:22.295: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e61533e1-a54f-48d1-9456-088c70cb8b14" in namespace "security-context-test-5397" to be "Succeeded or Failed"
Dec 14 16:57:22.299: INFO: Pod "busybox-readonly-false-e61533e1-a54f-48d1-9456-088c70cb8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167558ms
Dec 14 16:57:24.311: INFO: Pod "busybox-readonly-false-e61533e1-a54f-48d1-9456-088c70cb8b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01621483s
Dec 14 16:57:26.319: INFO: Pod "busybox-readonly-false-e61533e1-a54f-48d1-9456-088c70cb8b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023879859s
Dec 14 16:57:26.319: INFO: Pod "busybox-readonly-false-e61533e1-a54f-48d1-9456-088c70cb8b14" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 16:57:26.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5397" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5342,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 14 16:57:26.404: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28958 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:26.405: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28958 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 14 16:57:26.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28959 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:26.417: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28959 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 14 16:57:26.427: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28960 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:26.428: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28960 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 14 16:57:26.437: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28961 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:26.437: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4145  5dec813b-568b-4795-bc78-b56905e53a8c 28961 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 14 16:57:26.457: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4145  0c8dab9f-c730-49ad-8459-46824214ff69 28962 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:26.458: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4145  0c8dab9f-c730-49ad-8459-46824214ff69 28962 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 14 16:57:36.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4145  0c8dab9f-c730-49ad-8459-46824214ff69 29011 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Dec 14 16:57:36.474: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4145  0c8dab9f-c730-49ad-8459-46824214ff69 29011 0 2022-12-14 16:57:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-12-14 16:57:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Dec 14 16:57:46.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4145" for this suite.

• [SLOW TEST:20.161 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":285,"skipped":5349,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:46.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 16:57:46.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188" in namespace "projected-9549" to be "Succeeded or Failed"
Dec 14 16:57:46.604: INFO: Pod "downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68597ms
Dec 14 16:57:48.614: INFO: Pod "downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017503034s
Dec 14 16:57:50.627: INFO: Pod "downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030489211s
STEP: Saw pod success
Dec 14 16:57:50.627: INFO: Pod "downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188" satisfied condition "Succeeded or Failed"
Dec 14 16:57:50.631: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188 container client-container: <nil>
STEP: delete the pod
Dec 14 16:57:50.670: INFO: Waiting for pod downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188 to disappear
Dec 14 16:57:50.674: INFO: Pod downwardapi-volume-ce9a2d72-f559-4ec3-98e7-02ddf5471188 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 16:57:50.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9549" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":286,"skipped":5370,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:57:50.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-679
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-679
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-679
Dec 14 16:57:50.775: INFO: Found 0 stateful pods, waiting for 1
Dec 14 16:58:00.791: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 14 16:58:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:58:01.012: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:58:01.012: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:58:01.012: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:58:01.019: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 14 16:58:11.044: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:58:11.044: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:58:11.091: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:58:11.091: INFO: ss-0  queith7zooya-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  }]
Dec 14 16:58:11.091: INFO: 
Dec 14 16:58:11.091: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 14 16:58:12.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991180138s
Dec 14 16:58:13.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972980941s
Dec 14 16:58:14.140: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952859632s
Dec 14 16:58:15.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942631028s
Dec 14 16:58:16.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931427216s
Dec 14 16:58:17.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919846262s
Dec 14 16:58:18.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.905265013s
Dec 14 16:58:19.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.893449126s
Dec 14 16:58:20.212: INFO: Verifying statefulset ss doesn't scale past 3 for another 883.274361ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-679
Dec 14 16:58:21.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:58:21.715: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 14 16:58:21.715: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:58:21.715: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:58:21.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:58:21.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 16:58:21.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:58:21.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:58:21.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 14 16:58:22.200: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 14 16:58:22.200: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 14 16:58:22.200: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 14 16:58:22.206: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 14 16:58:32.226: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:58:32.226: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 16:58:32.226: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 14 16:58:32.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:58:32.525: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:58:32.525: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:58:32.525: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:58:32.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:58:32.754: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:58:32.754: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:58:32.754: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:58:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=statefulset-679 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 14 16:58:32.949: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 14 16:58:32.949: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 14 16:58:32.949: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 14 16:58:32.949: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:58:32.955: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 14 16:58:42.983: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:58:42.984: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:58:42.984: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 14 16:58:42.999: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:58:42.999: INFO: ss-0  queith7zooya-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  }]
Dec 14 16:58:42.999: INFO: ss-1  queith7zooya-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  }]
Dec 14 16:58:42.999: INFO: ss-2  queith7zooya-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  }]
Dec 14 16:58:42.999: INFO: 
Dec 14 16:58:42.999: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 16:58:44.013: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 14 16:58:44.013: INFO: ss-0  queith7zooya-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:57:50 +0000 UTC  }]
Dec 14 16:58:44.013: INFO: ss-1  queith7zooya-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  }]
Dec 14 16:58:44.013: INFO: ss-2  queith7zooya-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-12-14 16:58:11 +0000 UTC  }]
Dec 14 16:58:44.013: INFO: 
Dec 14 16:58:44.013: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 14 16:58:45.025: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.980529579s
Dec 14 16:58:46.037: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.968056708s
Dec 14 16:58:47.047: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.957681838s
Dec 14 16:58:48.061: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.947910826s
Dec 14 16:58:49.069: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.933864456s
Dec 14 16:58:50.078: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.925802756s
Dec 14 16:58:51.092: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.915949523s
Dec 14 16:58:52.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 902.200237ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-679
Dec 14 16:58:53.116: INFO: Scaling statefulset ss to 0
Dec 14 16:58:53.131: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 16:58:53.136: INFO: Deleting all statefulset in ns statefulset-679
Dec 14 16:58:53.141: INFO: Scaling statefulset ss to 0
Dec 14 16:58:53.158: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 16:58:53.162: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 16:58:53.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-679" for this suite.

• [SLOW TEST:62.525 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":287,"skipped":5377,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:58:53.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Dec 14 16:58:53.269: INFO: Waiting up to 5m0s for pod "test-pod-6e8476e4-25b3-4004-b839-83f6190628a4" in namespace "svcaccounts-4792" to be "Succeeded or Failed"
Dec 14 16:58:53.273: INFO: Pod "test-pod-6e8476e4-25b3-4004-b839-83f6190628a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956712ms
Dec 14 16:58:55.290: INFO: Pod "test-pod-6e8476e4-25b3-4004-b839-83f6190628a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020977037s
Dec 14 16:58:57.308: INFO: Pod "test-pod-6e8476e4-25b3-4004-b839-83f6190628a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039796614s
STEP: Saw pod success
Dec 14 16:58:57.309: INFO: Pod "test-pod-6e8476e4-25b3-4004-b839-83f6190628a4" satisfied condition "Succeeded or Failed"
Dec 14 16:58:57.315: INFO: Trying to get logs from node queith7zooya-3 pod test-pod-6e8476e4-25b3-4004-b839-83f6190628a4 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 16:58:57.444: INFO: Waiting for pod test-pod-6e8476e4-25b3-4004-b839-83f6190628a4 to disappear
Dec 14 16:58:57.448: INFO: Pod test-pod-6e8476e4-25b3-4004-b839-83f6190628a4 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Dec 14 16:58:57.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4792" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":288,"skipped":5383,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:58:57.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-fb6b4032-69a5-4af7-9545-831248119968
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 16:58:57.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-84" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":289,"skipped":5392,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:58:57.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 16:58:57.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721" in namespace "downward-api-3070" to be "Succeeded or Failed"
Dec 14 16:58:57.598: INFO: Pod "downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721": Phase="Pending", Reason="", readiness=false. Elapsed: 9.229814ms
Dec 14 16:58:59.612: INFO: Pod "downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721": Phase="Running", Reason="", readiness=true. Elapsed: 2.022727484s
Dec 14 16:59:01.629: INFO: Pod "downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039681552s
STEP: Saw pod success
Dec 14 16:59:01.629: INFO: Pod "downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721" satisfied condition "Succeeded or Failed"
Dec 14 16:59:01.636: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721 container client-container: <nil>
STEP: delete the pod
Dec 14 16:59:01.672: INFO: Waiting for pod downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721 to disappear
Dec 14 16:59:01.677: INFO: Pod downwardapi-volume-9f13363f-50cf-4a63-aa29-cb5bba636721 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 16:59:01.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3070" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":290,"skipped":5397,"failed":0}
S
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:01.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-3232
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3232
STEP: Deleting pre-stop pod
Dec 14 16:59:10.851: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Dec 14 16:59:10.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3232" for this suite.

• [SLOW TEST:9.204 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":291,"skipped":5398,"failed":0}
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:10.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Dec 14 16:59:10.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 create -f -'
Dec 14 16:59:12.430: INFO: stderr: ""
Dec 14 16:59:12.430: INFO: stdout: "pod/pause created\n"
Dec 14 16:59:12.430: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 14 16:59:12.430: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1594" to be "running and ready"
Dec 14 16:59:12.450: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.979543ms
Dec 14 16:59:14.464: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033470529s
Dec 14 16:59:14.464: INFO: Pod "pause" satisfied condition "running and ready"
Dec 14 16:59:14.464: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 14 16:59:14.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 label pods pause testing-label=testing-label-value'
Dec 14 16:59:14.589: INFO: stderr: ""
Dec 14 16:59:14.589: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 14 16:59:14.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 get pod pause -L testing-label'
Dec 14 16:59:14.689: INFO: stderr: ""
Dec 14 16:59:14.689: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 14 16:59:14.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 label pods pause testing-label-'
Dec 14 16:59:14.806: INFO: stderr: ""
Dec 14 16:59:14.806: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 14 16:59:14.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 get pod pause -L testing-label'
Dec 14 16:59:14.928: INFO: stderr: ""
Dec 14 16:59:14.928: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Dec 14 16:59:14.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 delete --grace-period=0 --force -f -'
Dec 14 16:59:15.039: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 16:59:15.039: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 14 16:59:15.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 get rc,svc -l name=pause --no-headers'
Dec 14 16:59:15.189: INFO: stderr: "No resources found in kubectl-1594 namespace.\n"
Dec 14 16:59:15.189: INFO: stdout: ""
Dec 14 16:59:15.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1594 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 16:59:15.333: INFO: stderr: ""
Dec 14 16:59:15.333: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 16:59:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1594" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":292,"skipped":5398,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-6c8e00a3-471d-4850-a6a9-3a647ae6ffcb
STEP: Creating a pod to test consume secrets
Dec 14 16:59:15.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5" in namespace "projected-536" to be "Succeeded or Failed"
Dec 14 16:59:15.417: INFO: Pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865389ms
Dec 14 16:59:17.427: INFO: Pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015534824s
Dec 14 16:59:19.444: INFO: Pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032525595s
Dec 14 16:59:21.462: INFO: Pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05024495s
STEP: Saw pod success
Dec 14 16:59:21.462: INFO: Pod "pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5" satisfied condition "Succeeded or Failed"
Dec 14 16:59:21.470: INFO: Trying to get logs from node queith7zooya-3 pod pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 14 16:59:21.605: INFO: Waiting for pod pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5 to disappear
Dec 14 16:59:21.610: INFO: Pod pod-projected-secrets-098928f7-44c6-4c04-aa10-165fde5e48f5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Dec 14 16:59:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-536" for this suite.

• [SLOW TEST:6.279 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5408,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:59:22.225: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:59:25.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:59:25.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5704" for this suite.
STEP: Destroying namespace "webhook-5704-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":294,"skipped":5413,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:25.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:59:25.574: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:59:27.586: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 16:59:29.593: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:31.584: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:33.589: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:35.585: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:37.593: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:39.586: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:41.584: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:43.587: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:45.581: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = false)
Dec 14 16:59:47.590: INFO: The status of Pod test-webserver-fae28f9c-6781-4892-bf77-d877e0950c69 is Running (Ready = true)
Dec 14 16:59:47.597: INFO: Container started at 2022-12-14 16:59:26 +0000 UTC, pod became ready at 2022-12-14 16:59:46 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 16:59:47.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5228" for this suite.

• [SLOW TEST:22.118 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":295,"skipped":5428,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:47.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 16:59:48.353: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 16:59:51.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 16:59:51.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1160-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 16:59:54.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1638" for this suite.
STEP: Destroying namespace "webhook-1638-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.338 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":296,"skipped":5440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 16:59:54.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-ff417995-bee6-47b2-b1eb-250d27ec1350 in namespace container-probe-1937
Dec 14 16:59:57.058: INFO: Started pod test-webserver-ff417995-bee6-47b2-b1eb-250d27ec1350 in namespace container-probe-1937
STEP: checking the pod's current state and verifying that restartCount is present
Dec 14 16:59:57.063: INFO: Initial restart count of pod test-webserver-ff417995-bee6-47b2-b1eb-250d27ec1350 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Dec 14 17:03:58.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1937" for this suite.

• [SLOW TEST:243.834 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":297,"skipped":5507,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:03:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Dec 14 17:03:58.889: INFO: Waiting up to 5m0s for pod "downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d" in namespace "downward-api-5284" to be "Succeeded or Failed"
Dec 14 17:03:58.895: INFO: Pod "downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.125075ms
Dec 14 17:04:00.909: INFO: Pod "downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019646695s
Dec 14 17:04:02.932: INFO: Pod "downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042819871s
STEP: Saw pod success
Dec 14 17:04:02.932: INFO: Pod "downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d" satisfied condition "Succeeded or Failed"
Dec 14 17:04:02.939: INFO: Trying to get logs from node queith7zooya-3 pod downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d container dapi-container: <nil>
STEP: delete the pod
Dec 14 17:04:02.999: INFO: Waiting for pod downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d to disappear
Dec 14 17:04:03.002: INFO: Pod downward-api-8b237a0d-6b47-478f-9023-c1c926d7855d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Dec 14 17:04:03.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5284" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":298,"skipped":5526,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Dec 14 17:04:03.072: INFO: Waiting up to 5m0s for pod "var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b" in namespace "var-expansion-2734" to be "Succeeded or Failed"
Dec 14 17:04:03.077: INFO: Pod "var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.94943ms
Dec 14 17:04:05.091: INFO: Pod "var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019157234s
Dec 14 17:04:07.098: INFO: Pod "var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026721994s
STEP: Saw pod success
Dec 14 17:04:07.099: INFO: Pod "var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b" satisfied condition "Succeeded or Failed"
Dec 14 17:04:07.105: INFO: Trying to get logs from node queith7zooya-3 pod var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b container dapi-container: <nil>
STEP: delete the pod
Dec 14 17:04:07.544: INFO: Waiting for pod var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b to disappear
Dec 14 17:04:07.549: INFO: Pod var-expansion-09f84780-1ed6-4284-962e-1e41c8c4800b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 17:04:07.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2734" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":299,"skipped":5541,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Dec 14 17:04:07.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1797 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Dec 14 17:04:07.779: INFO: stderr: ""
Dec 14 17:04:07.779: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Dec 14 17:04:07.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1797 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Dec 14 17:04:08.379: INFO: stderr: ""
Dec 14 17:04:08.379: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Dec 14 17:04:08.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-1797 delete pods e2e-test-httpd-pod'
Dec 14 17:04:10.818: INFO: stderr: ""
Dec 14 17:04:10.818: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 17:04:10.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1797" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":300,"skipped":5560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:10.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 17:04:10.885: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075" in namespace "projected-8169" to be "Succeeded or Failed"
Dec 14 17:04:10.898: INFO: Pod "downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075": Phase="Pending", Reason="", readiness=false. Elapsed: 12.651068ms
Dec 14 17:04:12.923: INFO: Pod "downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037458982s
Dec 14 17:04:14.958: INFO: Pod "downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072521924s
STEP: Saw pod success
Dec 14 17:04:14.958: INFO: Pod "downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075" satisfied condition "Succeeded or Failed"
Dec 14 17:04:14.971: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075 container client-container: <nil>
STEP: delete the pod
Dec 14 17:04:15.031: INFO: Waiting for pod downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075 to disappear
Dec 14 17:04:15.035: INFO: Pod downwardapi-volume-8be4c763-dd76-4a8b-a72e-845a62fdd075 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 17:04:15.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8169" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":301,"skipped":5588,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:15.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Dec 14 17:04:15.165: INFO: Creating simple deployment test-deployment-7qjt2
Dec 14 17:04:15.209: INFO: deployment "test-deployment-7qjt2" doesn't have the required revision set
Dec 14 17:04:17.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 4, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 4, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 4, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 4, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-7qjt2-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Dec 14 17:04:19.248: INFO: Deployment test-deployment-7qjt2 has Conditions: [{Available True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qjt2-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Dec 14 17:04:19.269: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 4, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 4, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 4, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 4, 15, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7qjt2-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Dec 14 17:04:19.279: INFO: Observed &Deployment event: ADDED
Dec 14 17:04:19.279: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qjt2-688c4d6789"}
Dec 14 17:04:19.279: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.279: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qjt2-688c4d6789"}
Dec 14 17:04:19.279: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 17:04:19.280: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.280: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 17:04:19.280: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qjt2-688c4d6789" is progressing.}
Dec 14 17:04:19.280: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.280: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 17:04:19.280: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qjt2-688c4d6789" has successfully progressed.}
Dec 14 17:04:19.281: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.281: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 17:04:19.281: INFO: Observed Deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qjt2-688c4d6789" has successfully progressed.}
Dec 14 17:04:19.281: INFO: Found Deployment test-deployment-7qjt2 in namespace deployment-2300 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 17:04:19.281: INFO: Deployment test-deployment-7qjt2 has an updated status
STEP: patching the Statefulset Status
Dec 14 17:04:19.281: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 17:04:19.288: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Dec 14 17:04:19.291: INFO: Observed &Deployment event: ADDED
Dec 14 17:04:19.291: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qjt2-688c4d6789"}
Dec 14 17:04:19.291: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.292: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7qjt2-688c4d6789"}
Dec 14 17:04:19.292: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 17:04:19.292: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.292: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Dec 14 17:04:19.292: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:15 +0000 UTC 2022-12-14 17:04:15 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7qjt2-688c4d6789" is progressing.}
Dec 14 17:04:19.293: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.293: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 17:04:19.293: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qjt2-688c4d6789" has successfully progressed.}
Dec 14 17:04:19.293: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.293: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Dec 14 17:04:19.293: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-12-14 17:04:17 +0000 UTC 2022-12-14 17:04:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7qjt2-688c4d6789" has successfully progressed.}
Dec 14 17:04:19.293: INFO: Observed deployment test-deployment-7qjt2 in namespace deployment-2300 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 17:04:19.293: INFO: Observed &Deployment event: MODIFIED
Dec 14 17:04:19.293: INFO: Found deployment test-deployment-7qjt2 in namespace deployment-2300 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 17:04:19.293: INFO: Deployment test-deployment-7qjt2 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 17:04:19.301: INFO: Deployment "test-deployment-7qjt2":
&Deployment{ObjectMeta:{test-deployment-7qjt2  deployment-2300  c8933899-96a4-4f5d-a450-32e7dc7b66ce 30297 1 2022-12-14 17:04:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-12-14 17:04:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:04:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2022-12-14 17:04:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042bc788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 17:04:19.309: INFO: New ReplicaSet "test-deployment-7qjt2-688c4d6789" of Deployment "test-deployment-7qjt2":
&ReplicaSet{ObjectMeta:{test-deployment-7qjt2-688c4d6789  deployment-2300  9c723ad3-39b2-4138-8ec7-366d0c9351a1 30294 1 2022-12-14 17:04:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7qjt2 c8933899-96a4-4f5d-a450-32e7dc7b66ce 0xc00233e510 0xc00233e511}] []  [{kube-controller-manager Update apps/v1 2022-12-14 17:04:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8933899-96a4-4f5d-a450-32e7dc7b66ce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:04:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00233e5b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 17:04:19.314: INFO: Pod "test-deployment-7qjt2-688c4d6789-pchf9" is available:
&Pod{ObjectMeta:{test-deployment-7qjt2-688c4d6789-pchf9 test-deployment-7qjt2-688c4d6789- deployment-2300  a7493624-faa5-4ee5-9f23-49f99308d57e 30293 0 2022-12-14 17:04:15 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [{apps/v1 ReplicaSet test-deployment-7qjt2-688c4d6789 9c723ad3-39b2-4138-8ec7-366d0c9351a1 0xc0042bcb50 0xc0042bcb51}] []  [{kube-controller-manager Update v1 2022-12-14 17:04:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c723ad3-39b2-4138-8ec7-366d0c9351a1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 17:04:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q92nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q92nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:04:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:04:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.137,StartTime:2022-12-14 17:04:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 17:04:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://64486eab61767e8e1fd6fc48afa7e8872de0d17a929419da0a7eff2b3d6a9f52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 17:04:19.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2300" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":302,"skipped":5613,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:19.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Dec 14 17:04:25.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6352" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":303,"skipped":5627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:25.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Dec 14 17:04:25.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 create -f -'
Dec 14 17:04:26.811: INFO: stderr: ""
Dec 14 17:04:26.811: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 17:04:26.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:04:26.963: INFO: stderr: ""
Dec 14 17:04:26.963: INFO: stdout: "update-demo-nautilus-9kdtt update-demo-nautilus-frlrf "
Dec 14 17:04:26.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:27.105: INFO: stderr: ""
Dec 14 17:04:27.105: INFO: stdout: ""
Dec 14 17:04:27.105: INFO: update-demo-nautilus-9kdtt is created but not running
Dec 14 17:04:32.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:04:32.287: INFO: stderr: ""
Dec 14 17:04:32.287: INFO: stdout: "update-demo-nautilus-9kdtt update-demo-nautilus-frlrf "
Dec 14 17:04:32.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:32.531: INFO: stderr: ""
Dec 14 17:04:32.531: INFO: stdout: ""
Dec 14 17:04:32.531: INFO: update-demo-nautilus-9kdtt is created but not running
Dec 14 17:04:37.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:04:37.712: INFO: stderr: ""
Dec 14 17:04:37.712: INFO: stdout: "update-demo-nautilus-9kdtt update-demo-nautilus-frlrf "
Dec 14 17:04:37.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:37.830: INFO: stderr: ""
Dec 14 17:04:37.830: INFO: stdout: "true"
Dec 14 17:04:37.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:04:38.025: INFO: stderr: ""
Dec 14 17:04:38.025: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:04:38.025: INFO: validating pod update-demo-nautilus-9kdtt
Dec 14 17:04:38.040: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:04:38.040: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:04:38.040: INFO: update-demo-nautilus-9kdtt is verified up and running
Dec 14 17:04:38.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-frlrf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:38.235: INFO: stderr: ""
Dec 14 17:04:38.236: INFO: stdout: ""
Dec 14 17:04:38.236: INFO: update-demo-nautilus-frlrf is created but not running
Dec 14 17:04:43.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:04:43.393: INFO: stderr: ""
Dec 14 17:04:43.393: INFO: stdout: "update-demo-nautilus-9kdtt update-demo-nautilus-frlrf "
Dec 14 17:04:43.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:43.547: INFO: stderr: ""
Dec 14 17:04:43.547: INFO: stdout: "true"
Dec 14 17:04:43.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-9kdtt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:04:43.679: INFO: stderr: ""
Dec 14 17:04:43.679: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:04:43.679: INFO: validating pod update-demo-nautilus-9kdtt
Dec 14 17:04:43.688: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:04:43.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:04:43.688: INFO: update-demo-nautilus-9kdtt is verified up and running
Dec 14 17:04:43.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-frlrf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:04:43.812: INFO: stderr: ""
Dec 14 17:04:43.812: INFO: stdout: "true"
Dec 14 17:04:43.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods update-demo-nautilus-frlrf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:04:44.006: INFO: stderr: ""
Dec 14 17:04:44.006: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:04:44.006: INFO: validating pod update-demo-nautilus-frlrf
Dec 14 17:04:44.023: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:04:44.023: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:04:44.023: INFO: update-demo-nautilus-frlrf is verified up and running
STEP: using delete to clean up resources
Dec 14 17:04:44.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 delete --grace-period=0 --force -f -'
Dec 14 17:04:44.189: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:04:44.189: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 17:04:44.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get rc,svc -l name=update-demo --no-headers'
Dec 14 17:04:44.451: INFO: stderr: "No resources found in kubectl-9392 namespace.\n"
Dec 14 17:04:44.451: INFO: stdout: ""
Dec 14 17:04:44.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-9392 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 17:04:44.677: INFO: stderr: ""
Dec 14 17:04:44.677: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 17:04:44.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9392" for this suite.

• [SLOW TEST:19.168 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":304,"skipped":5657,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:44.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1195
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1195
STEP: creating replication controller externalsvc in namespace services-1195
I1214 17:04:44.820617      14 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1195, replica count: 2
I1214 17:04:47.872565      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 17:04:50.873729      14 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 14 17:04:50.933: INFO: Creating new exec pod
Dec 14 17:04:54.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=services-1195 exec execpodq6xs8 -- /bin/sh -x -c nslookup nodeport-service.services-1195.svc.cluster.local'
Dec 14 17:04:55.705: INFO: stderr: "+ nslookup nodeport-service.services-1195.svc.cluster.local\n"
Dec 14 17:04:55.705: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-1195.svc.cluster.local\tcanonical name = externalsvc.services-1195.svc.cluster.local.\nName:\texternalsvc.services-1195.svc.cluster.local\nAddress: 10.233.20.94\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1195, will wait for the garbage collector to delete the pods
Dec 14 17:04:55.777: INFO: Deleting ReplicationController externalsvc took: 12.53683ms
Dec 14 17:04:55.878: INFO: Terminating ReplicationController externalsvc pods took: 101.065664ms
Dec 14 17:04:58.716: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Dec 14 17:04:58.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1195" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:14.052 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":305,"skipped":5666,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:04:58.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Dec 14 17:05:02.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3395" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":306,"skipped":5679,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:02.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-bstc
STEP: Creating a pod to test atomic-volume-subpath
Dec 14 17:05:02.305: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bstc" in namespace "subpath-8786" to be "Succeeded or Failed"
Dec 14 17:05:02.310: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509085ms
Dec 14 17:05:04.324: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 2.018980898s
Dec 14 17:05:06.332: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 4.026484394s
Dec 14 17:05:08.340: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 6.034221975s
Dec 14 17:05:10.351: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 8.045491185s
Dec 14 17:05:12.361: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 10.056113425s
Dec 14 17:05:14.379: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 12.073397351s
Dec 14 17:05:16.393: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 14.087144162s
Dec 14 17:05:18.414: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 16.108441127s
Dec 14 17:05:20.429: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 18.123873237s
Dec 14 17:05:22.441: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=true. Elapsed: 20.135141325s
Dec 14 17:05:24.461: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=false. Elapsed: 22.155214029s
Dec 14 17:05:26.471: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Running", Reason="", readiness=false. Elapsed: 24.165170544s
Dec 14 17:05:28.488: INFO: Pod "pod-subpath-test-downwardapi-bstc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.182878827s
STEP: Saw pod success
Dec 14 17:05:28.488: INFO: Pod "pod-subpath-test-downwardapi-bstc" satisfied condition "Succeeded or Failed"
Dec 14 17:05:28.510: INFO: Trying to get logs from node queith7zooya-3 pod pod-subpath-test-downwardapi-bstc container test-container-subpath-downwardapi-bstc: <nil>
STEP: delete the pod
Dec 14 17:05:28.556: INFO: Waiting for pod pod-subpath-test-downwardapi-bstc to disappear
Dec 14 17:05:28.560: INFO: Pod pod-subpath-test-downwardapi-bstc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bstc
Dec 14 17:05:28.560: INFO: Deleting pod "pod-subpath-test-downwardapi-bstc" in namespace "subpath-8786"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Dec 14 17:05:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8786" for this suite.

• [SLOW TEST:26.385 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":307,"skipped":5682,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:28.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 14 17:05:28.703: INFO: Waiting up to 5m0s for pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4" in namespace "emptydir-6120" to be "Succeeded or Failed"
Dec 14 17:05:28.709: INFO: Pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.550015ms
Dec 14 17:05:30.724: INFO: Pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.020531071s
Dec 14 17:05:32.734: INFO: Pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4": Phase="Running", Reason="", readiness=false. Elapsed: 4.031046358s
Dec 14 17:05:34.749: INFO: Pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046039546s
STEP: Saw pod success
Dec 14 17:05:34.749: INFO: Pod "pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4" satisfied condition "Succeeded or Failed"
Dec 14 17:05:34.753: INFO: Trying to get logs from node queith7zooya-3 pod pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4 container test-container: <nil>
STEP: delete the pod
Dec 14 17:05:34.780: INFO: Waiting for pod pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4 to disappear
Dec 14 17:05:34.783: INFO: Pod pod-7ede69b5-f57c-4030-9b9b-8efc6e5736d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:05:34.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6120" for this suite.

• [SLOW TEST:6.187 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":308,"skipped":5688,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:34.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 14 17:05:34.843: INFO: Waiting up to 5m0s for pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c" in namespace "emptydir-9397" to be "Succeeded or Failed"
Dec 14 17:05:34.848: INFO: Pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453172ms
Dec 14 17:05:36.858: INFO: Pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015528455s
Dec 14 17:05:38.876: INFO: Pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c": Phase="Running", Reason="", readiness=false. Elapsed: 4.033154269s
Dec 14 17:05:40.905: INFO: Pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062225765s
STEP: Saw pod success
Dec 14 17:05:40.905: INFO: Pod "pod-c54ea7e4-9402-406d-a913-c570747ba04c" satisfied condition "Succeeded or Failed"
Dec 14 17:05:40.913: INFO: Trying to get logs from node queith7zooya-3 pod pod-c54ea7e4-9402-406d-a913-c570747ba04c container test-container: <nil>
STEP: delete the pod
Dec 14 17:05:40.960: INFO: Waiting for pod pod-c54ea7e4-9402-406d-a913-c570747ba04c to disappear
Dec 14 17:05:40.964: INFO: Pod pod-c54ea7e4-9402-406d-a913-c570747ba04c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:05:40.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9397" for this suite.

• [SLOW TEST:6.180 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":309,"skipped":5688,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:40.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:05:41.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Dec 14 17:05:44.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 create -f -'
Dec 14 17:05:45.632: INFO: stderr: ""
Dec 14 17:05:45.632: INFO: stdout: "e2e-test-crd-publish-openapi-5777-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 17:05:45.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 delete e2e-test-crd-publish-openapi-5777-crds test-foo'
Dec 14 17:05:45.767: INFO: stderr: ""
Dec 14 17:05:45.768: INFO: stdout: "e2e-test-crd-publish-openapi-5777-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 14 17:05:45.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 apply -f -'
Dec 14 17:05:46.776: INFO: stderr: ""
Dec 14 17:05:46.776: INFO: stdout: "e2e-test-crd-publish-openapi-5777-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 14 17:05:46.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 delete e2e-test-crd-publish-openapi-5777-crds test-foo'
Dec 14 17:05:46.892: INFO: stderr: ""
Dec 14 17:05:46.892: INFO: stdout: "e2e-test-crd-publish-openapi-5777-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Dec 14 17:05:46.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 create -f -'
Dec 14 17:05:47.182: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 14 17:05:47.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 create -f -'
Dec 14 17:05:47.472: INFO: rc: 1
Dec 14 17:05:47.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 apply -f -'
Dec 14 17:05:47.707: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Dec 14 17:05:47.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 create -f -'
Dec 14 17:05:47.954: INFO: rc: 1
Dec 14 17:05:47.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 --namespace=crd-publish-openapi-5976 apply -f -'
Dec 14 17:05:48.237: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 14 17:05:48.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 explain e2e-test-crd-publish-openapi-5777-crds'
Dec 14 17:05:48.559: INFO: stderr: ""
Dec 14 17:05:48.559: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5777-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 14 17:05:48.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 explain e2e-test-crd-publish-openapi-5777-crds.metadata'
Dec 14 17:05:48.920: INFO: stderr: ""
Dec 14 17:05:48.920: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5777-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 14 17:05:48.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 explain e2e-test-crd-publish-openapi-5777-crds.spec'
Dec 14 17:05:49.254: INFO: stderr: ""
Dec 14 17:05:49.254: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5777-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 14 17:05:49.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 explain e2e-test-crd-publish-openapi-5777-crds.spec.bars'
Dec 14 17:05:49.541: INFO: stderr: ""
Dec 14 17:05:49.541: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5777-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 14 17:05:49.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-5976 explain e2e-test-crd-publish-openapi-5777-crds.spec.bars2'
Dec 14 17:05:49.850: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:05:53.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5976" for this suite.

• [SLOW TEST:12.365 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":310,"skipped":5702,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:53.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 14 17:05:53.385: INFO: Waiting up to 5m0s for pod "pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e" in namespace "emptydir-40" to be "Succeeded or Failed"
Dec 14 17:05:53.391: INFO: Pod "pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.363044ms
Dec 14 17:05:55.400: INFO: Pod "pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015138379s
Dec 14 17:05:57.416: INFO: Pod "pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030923685s
STEP: Saw pod success
Dec 14 17:05:57.417: INFO: Pod "pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e" satisfied condition "Succeeded or Failed"
Dec 14 17:05:57.425: INFO: Trying to get logs from node queith7zooya-3 pod pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e container test-container: <nil>
STEP: delete the pod
Dec 14 17:05:57.575: INFO: Waiting for pod pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e to disappear
Dec 14 17:05:57.579: INFO: Pod pod-fe3e922d-8d1e-4695-a5c3-591708fcbd9e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:05:57.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-40" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:05:57.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-1ad5c0e0-838f-44e1-9aba-bac867d8a19f
STEP: Creating a pod to test consume configMaps
Dec 14 17:05:57.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e" in namespace "configmap-6239" to be "Succeeded or Failed"
Dec 14 17:05:57.702: INFO: Pod "pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.632512ms
Dec 14 17:05:59.714: INFO: Pod "pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025389356s
Dec 14 17:06:01.726: INFO: Pod "pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037788049s
STEP: Saw pod success
Dec 14 17:06:01.726: INFO: Pod "pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e" satisfied condition "Succeeded or Failed"
Dec 14 17:06:01.731: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e container agnhost-container: <nil>
STEP: delete the pod
Dec 14 17:06:01.756: INFO: Waiting for pod pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e to disappear
Dec 14 17:06:01.760: INFO: Pod pod-configmaps-ade10e2e-85be-4647-993a-2503ca18f90e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:06:01.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6239" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":312,"skipped":5758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:06:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Dec 14 17:06:11.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7483" for this suite.

• [SLOW TEST:10.071 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":313,"skipped":5820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:06:11.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1786
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 14 17:06:11.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Dec 14 17:06:12.000: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:06:14.012: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 17:06:16.018: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 17:06:18.012: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 17:06:20.016: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 17:06:22.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Dec 14 17:06:24.010: INFO: The status of Pod netserver-0 is Running (Ready = true)
Dec 14 17:06:24.028: INFO: The status of Pod netserver-1 is Running (Ready = true)
Dec 14 17:06:24.038: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:26.052: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:28.071: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:30.051: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:32.056: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:34.046: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:36.057: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:38.057: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:40.048: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:42.056: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:44.049: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:46.051: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:48.053: INFO: The status of Pod netserver-2 is Running (Ready = false)
Dec 14 17:06:50.050: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Dec 14 17:06:52.126: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Dec 14 17:06:52.126: INFO: Going to poll 10.233.64.165 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 17:06:52.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.165:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:06:52.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 17:06:52.137: INFO: ExecWithOptions: Clientset creation
Dec 14 17:06:52.137: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1786/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.165%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 17:06:52.286: INFO: Found all 1 expected endpoints: [netserver-0]
Dec 14 17:06:52.287: INFO: Going to poll 10.233.65.159 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 17:06:52.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:06:52.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 17:06:52.294: INFO: ExecWithOptions: Clientset creation
Dec 14 17:06:52.294: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1786/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 17:06:52.433: INFO: Found all 1 expected endpoints: [netserver-1]
Dec 14 17:06:52.434: INFO: Going to poll 10.233.66.151 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Dec 14 17:06:52.439: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1786 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Dec 14 17:06:52.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
Dec 14 17:06:52.440: INFO: ExecWithOptions: Clientset creation
Dec 14 17:06:52.440: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1786/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Dec 14 17:06:52.676: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Dec 14 17:06:52.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1786" for this suite.

• [SLOW TEST:40.836 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":314,"skipped":5852,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:06:52.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Dec 14 17:06:52.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-6375 api-versions'
Dec 14 17:06:52.841: INFO: stderr: ""
Dec 14 17:06:52.841: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 17:06:52.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6375" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":315,"skipped":5854,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:06:52.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 14 17:06:53.680: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 14 17:06:55.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:06:57.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:06:59.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:07:01.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:07:03.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 6, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 17:07:06.735: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:07:06.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:07:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5983" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:17.752 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":316,"skipped":5866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:07:10.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:07:10.841: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 17:07:10.907: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:07:10.907: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:07:11.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 17:07:11.943: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:07:12.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:07:12.931: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 14 17:07:12.986: INFO: Wrong image for pod: daemon-set-8b5f7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:12.986: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:12.986: INFO: Wrong image for pod: daemon-set-mk9k8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:14.000: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:14.000: INFO: Wrong image for pod: daemon-set-mk9k8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:15.007: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:15.008: INFO: Wrong image for pod: daemon-set-mk9k8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:16.009: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:17.006: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:17.006: INFO: Pod daemon-set-k9z9j is not available
Dec 14 17:07:18.007: INFO: Wrong image for pod: daemon-set-bflh7. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Dec 14 17:07:18.007: INFO: Pod daemon-set-k9z9j is not available
Dec 14 17:07:20.005: INFO: Pod daemon-set-cbctm is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 14 17:07:20.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:07:20.021: INFO: Node queith7zooya-3 is running 0 daemon pod, expected 1
Dec 14 17:07:21.050: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:07:21.050: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1753, will wait for the garbage collector to delete the pods
Dec 14 17:07:21.149: INFO: Deleting DaemonSet.extensions daemon-set took: 9.280221ms
Dec 14 17:07:21.249: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.623977ms
Dec 14 17:07:24.558: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:07:24.558: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 17:07:24.565: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31382"},"items":null}

Dec 14 17:07:24.571: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31382"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 17:07:24.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1753" for this suite.

• [SLOW TEST:13.999 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":317,"skipped":5909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:07:24.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 14 17:07:24.664: INFO: Waiting up to 5m0s for pod "pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b" in namespace "emptydir-485" to be "Succeeded or Failed"
Dec 14 17:07:24.669: INFO: Pod "pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838908ms
Dec 14 17:07:26.681: INFO: Pod "pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01690403s
Dec 14 17:07:28.693: INFO: Pod "pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028953204s
STEP: Saw pod success
Dec 14 17:07:28.693: INFO: Pod "pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b" satisfied condition "Succeeded or Failed"
Dec 14 17:07:28.698: INFO: Trying to get logs from node queith7zooya-3 pod pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b container test-container: <nil>
STEP: delete the pod
Dec 14 17:07:28.722: INFO: Waiting for pod pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b to disappear
Dec 14 17:07:28.725: INFO: Pod pod-4c9cfd9e-4fb2-4b02-b8ff-2dd55018174b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:07:28.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-485" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":318,"skipped":5954,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:07:28.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 17:07:28.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae" in namespace "projected-9898" to be "Succeeded or Failed"
Dec 14 17:07:28.794: INFO: Pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332157ms
Dec 14 17:07:30.805: INFO: Pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017582712s
Dec 14 17:07:32.817: INFO: Pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03020359s
Dec 14 17:07:34.827: INFO: Pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039673857s
STEP: Saw pod success
Dec 14 17:07:34.827: INFO: Pod "downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae" satisfied condition "Succeeded or Failed"
Dec 14 17:07:34.833: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae container client-container: <nil>
STEP: delete the pod
Dec 14 17:07:34.863: INFO: Waiting for pod downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae to disappear
Dec 14 17:07:34.867: INFO: Pod downwardapi-volume-2d0b2225-6cda-41f4-bf8f-c2c2265ef8ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Dec 14 17:07:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9898" for this suite.

• [SLOW TEST:6.142 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":319,"skipped":5962,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:07:34.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Dec 14 17:08:03.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7982" for this suite.

• [SLOW TEST:28.171 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":320,"skipped":5977,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 17:08:04.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 17:08:06.317: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:08:08.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:08:10.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:08:12.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:08:14.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 17:08:17.356: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:08:17.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6427" for this suite.
STEP: Destroying namespace "webhook-6427-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:14.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":321,"skipped":5978,"failed":0}
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:17.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 17:08:17.643: INFO: The status of Pod pod-update-c6a8085f-8861-41b3-b2ca-b9a6d578fa01 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:08:19.655: INFO: The status of Pod pod-update-c6a8085f-8861-41b3-b2ca-b9a6d578fa01 is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:08:21.664: INFO: The status of Pod pod-update-c6a8085f-8861-41b3-b2ca-b9a6d578fa01 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 17:08:22.198: INFO: Successfully updated pod "pod-update-c6a8085f-8861-41b3-b2ca-b9a6d578fa01"
STEP: verifying the updated pod is in kubernetes
Dec 14 17:08:22.208: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 17:08:22.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9878" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":5978,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:22.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:08:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Dec 14 17:08:25.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-4563 --namespace=crd-publish-openapi-4563 create -f -'
Dec 14 17:08:26.801: INFO: stderr: ""
Dec 14 17:08:26.803: INFO: stdout: "e2e-test-crd-publish-openapi-1489-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 17:08:26.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-4563 --namespace=crd-publish-openapi-4563 delete e2e-test-crd-publish-openapi-1489-crds test-cr'
Dec 14 17:08:26.937: INFO: stderr: ""
Dec 14 17:08:26.937: INFO: stdout: "e2e-test-crd-publish-openapi-1489-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 14 17:08:26.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-4563 --namespace=crd-publish-openapi-4563 apply -f -'
Dec 14 17:08:28.048: INFO: stderr: ""
Dec 14 17:08:28.048: INFO: stdout: "e2e-test-crd-publish-openapi-1489-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 14 17:08:28.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-4563 --namespace=crd-publish-openapi-4563 delete e2e-test-crd-publish-openapi-1489-crds test-cr'
Dec 14 17:08:28.251: INFO: stderr: ""
Dec 14 17:08:28.251: INFO: stdout: "e2e-test-crd-publish-openapi-1489-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 14 17:08:28.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=crd-publish-openapi-4563 explain e2e-test-crd-publish-openapi-1489-crds'
Dec 14 17:08:28.616: INFO: stderr: ""
Dec 14 17:08:28.616: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1489-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:08:32.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4563" for this suite.

• [SLOW TEST:9.836 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":323,"skipped":5981,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:32.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Dec 14 17:08:32.126: INFO: The status of Pod pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:08:34.133: INFO: The status of Pod pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 14 17:08:34.665: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f"
Dec 14 17:08:34.665: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f" in namespace "pods-5575" to be "terminated due to deadline exceeded"
Dec 14 17:08:34.670: INFO: Pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f": Phase="Running", Reason="", readiness=true. Elapsed: 4.708381ms
Dec 14 17:08:36.680: INFO: Pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014311973s
Dec 14 17:08:38.701: INFO: Pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f": Phase="Running", Reason="", readiness=false. Elapsed: 4.035918504s
Dec 14 17:08:40.719: INFO: Pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.053095011s
Dec 14 17:08:40.719: INFO: Pod "pod-update-activedeadlineseconds-dad0de25-eb5b-4dab-aecb-7fda0166881f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Dec 14 17:08:40.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5575" for this suite.

• [SLOW TEST:8.679 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":324,"skipped":5995,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:40.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Dec 14 17:08:40.808: INFO: Waiting up to 5m0s for pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f" in namespace "containers-2751" to be "Succeeded or Failed"
Dec 14 17:08:40.813: INFO: Pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.989722ms
Dec 14 17:08:42.824: INFO: Pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01563128s
Dec 14 17:08:44.833: INFO: Pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024932951s
Dec 14 17:08:46.841: INFO: Pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033238518s
STEP: Saw pod success
Dec 14 17:08:46.842: INFO: Pod "client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f" satisfied condition "Succeeded or Failed"
Dec 14 17:08:46.846: INFO: Trying to get logs from node queith7zooya-3 pod client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f container agnhost-container: <nil>
STEP: delete the pod
Dec 14 17:08:46.881: INFO: Waiting for pod client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f to disappear
Dec 14 17:08:46.885: INFO: Pod client-containers-3b905b83-3118-4611-bfe6-96d2c62c308f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Dec 14 17:08:46.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2751" for this suite.

• [SLOW TEST:6.149 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":6007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:46.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-a71d7668-d142-42d8-b4da-271b3fb0682f
STEP: Creating a pod to test consume secrets
Dec 14 17:08:46.958: INFO: Waiting up to 5m0s for pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848" in namespace "secrets-700" to be "Succeeded or Failed"
Dec 14 17:08:46.971: INFO: Pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848": Phase="Pending", Reason="", readiness=false. Elapsed: 13.098278ms
Dec 14 17:08:48.979: INFO: Pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020648209s
Dec 14 17:08:51.012: INFO: Pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054226323s
Dec 14 17:08:53.026: INFO: Pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067759243s
STEP: Saw pod success
Dec 14 17:08:53.026: INFO: Pod "pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848" satisfied condition "Succeeded or Failed"
Dec 14 17:08:53.032: INFO: Trying to get logs from node queith7zooya-3 pod pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848 container secret-volume-test: <nil>
STEP: delete the pod
Dec 14 17:08:53.063: INFO: Waiting for pod pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848 to disappear
Dec 14 17:08:53.069: INFO: Pod pod-secrets-27b6af6d-4e45-44b7-92f5-b2ca9ddb2848 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Dec 14 17:08:53.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-700" for this suite.

• [SLOW TEST:6.187 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":326,"skipped":6050,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:53.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Dec 14 17:08:53.170: INFO: Waiting up to 5m0s for pod "downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa" in namespace "downward-api-4920" to be "Succeeded or Failed"
Dec 14 17:08:53.175: INFO: Pod "downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716421ms
Dec 14 17:08:55.186: INFO: Pod "downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015086402s
Dec 14 17:08:57.196: INFO: Pod "downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025055427s
STEP: Saw pod success
Dec 14 17:08:57.196: INFO: Pod "downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa" satisfied condition "Succeeded or Failed"
Dec 14 17:08:57.200: INFO: Trying to get logs from node queith7zooya-3 pod downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa container dapi-container: <nil>
STEP: delete the pod
Dec 14 17:08:57.255: INFO: Waiting for pod downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa to disappear
Dec 14 17:08:57.260: INFO: Pod downward-api-6fde4772-7c5f-4d9c-aa05-6917274bc2aa no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Dec 14 17:08:57.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4920" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":327,"skipped":6051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:08:57.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 17:08:57.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:08:57.373: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:08:58.392: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:08:58.392: INFO: Node queith7zooya-3 is running 0 daemon pod, expected 1
Dec 14 17:08:59.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:08:59.389: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 14 17:08:59.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:08:59.425: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:09:00.440: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:09:00.440: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:09:01.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:09:01.444: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:09:02.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:09:02.441: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5677, will wait for the garbage collector to delete the pods
Dec 14 17:09:02.515: INFO: Deleting DaemonSet.extensions daemon-set took: 9.681147ms
Dec 14 17:09:02.718: INFO: Terminating DaemonSet.extensions daemon-set pods took: 202.697554ms
Dec 14 17:09:05.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:09:05.533: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 17:09:05.540: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31943"},"items":null}

Dec 14 17:09:05.546: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31943"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 17:09:05.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5677" for this suite.

• [SLOW TEST:8.311 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":328,"skipped":6076,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:09:05.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Dec 14 17:09:05.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 create -f -'
Dec 14 17:09:06.777: INFO: stderr: ""
Dec 14 17:09:06.777: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 17:09:06.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:09:06.955: INFO: stderr: ""
Dec 14 17:09:06.955: INFO: stdout: "update-demo-nautilus-hfmm8 update-demo-nautilus-xp2vj "
Dec 14 17:09:06.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:07.059: INFO: stderr: ""
Dec 14 17:09:07.059: INFO: stdout: ""
Dec 14 17:09:07.059: INFO: update-demo-nautilus-hfmm8 is created but not running
Dec 14 17:09:12.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:09:12.200: INFO: stderr: ""
Dec 14 17:09:12.200: INFO: stdout: "update-demo-nautilus-hfmm8 update-demo-nautilus-xp2vj "
Dec 14 17:09:12.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:12.405: INFO: stderr: ""
Dec 14 17:09:12.406: INFO: stdout: "true"
Dec 14 17:09:12.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:09:12.577: INFO: stderr: ""
Dec 14 17:09:12.577: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:09:12.577: INFO: validating pod update-demo-nautilus-hfmm8
Dec 14 17:09:12.594: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:09:12.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:09:12.595: INFO: update-demo-nautilus-hfmm8 is verified up and running
Dec 14 17:09:12.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-xp2vj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:12.717: INFO: stderr: ""
Dec 14 17:09:12.717: INFO: stdout: "true"
Dec 14 17:09:12.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-xp2vj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:09:12.818: INFO: stderr: ""
Dec 14 17:09:12.818: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:09:12.818: INFO: validating pod update-demo-nautilus-xp2vj
Dec 14 17:09:12.836: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:09:12.836: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:09:12.836: INFO: update-demo-nautilus-xp2vj is verified up and running
STEP: scaling down the replication controller
Dec 14 17:09:12.852: INFO: scanned /root for discovery docs: <nil>
Dec 14 17:09:12.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Dec 14 17:09:14.006: INFO: stderr: ""
Dec 14 17:09:14.006: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 17:09:14.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:09:14.185: INFO: stderr: ""
Dec 14 17:09:14.185: INFO: stdout: "update-demo-nautilus-hfmm8 "
Dec 14 17:09:14.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:14.335: INFO: stderr: ""
Dec 14 17:09:14.335: INFO: stdout: "true"
Dec 14 17:09:14.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:09:14.486: INFO: stderr: ""
Dec 14 17:09:14.486: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:09:14.486: INFO: validating pod update-demo-nautilus-hfmm8
Dec 14 17:09:14.494: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:09:14.494: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:09:14.494: INFO: update-demo-nautilus-hfmm8 is verified up and running
STEP: scaling up the replication controller
Dec 14 17:09:14.510: INFO: scanned /root for discovery docs: <nil>
Dec 14 17:09:14.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Dec 14 17:09:15.707: INFO: stderr: ""
Dec 14 17:09:15.707: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 14 17:09:15.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:09:15.848: INFO: stderr: ""
Dec 14 17:09:15.848: INFO: stdout: "update-demo-nautilus-gv5sh update-demo-nautilus-hfmm8 "
Dec 14 17:09:15.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-gv5sh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:16.000: INFO: stderr: ""
Dec 14 17:09:16.000: INFO: stdout: ""
Dec 14 17:09:16.000: INFO: update-demo-nautilus-gv5sh is created but not running
Dec 14 17:09:21.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Dec 14 17:09:21.119: INFO: stderr: ""
Dec 14 17:09:21.119: INFO: stdout: "update-demo-nautilus-gv5sh update-demo-nautilus-hfmm8 "
Dec 14 17:09:21.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-gv5sh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:21.248: INFO: stderr: ""
Dec 14 17:09:21.248: INFO: stdout: "true"
Dec 14 17:09:21.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-gv5sh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:09:21.389: INFO: stderr: ""
Dec 14 17:09:21.389: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:09:21.389: INFO: validating pod update-demo-nautilus-gv5sh
Dec 14 17:09:21.402: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:09:21.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:09:21.402: INFO: update-demo-nautilus-gv5sh is verified up and running
Dec 14 17:09:21.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Dec 14 17:09:21.592: INFO: stderr: ""
Dec 14 17:09:21.592: INFO: stdout: "true"
Dec 14 17:09:21.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods update-demo-nautilus-hfmm8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Dec 14 17:09:21.735: INFO: stderr: ""
Dec 14 17:09:21.735: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Dec 14 17:09:21.735: INFO: validating pod update-demo-nautilus-hfmm8
Dec 14 17:09:21.740: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 14 17:09:21.740: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 14 17:09:21.740: INFO: update-demo-nautilus-hfmm8 is verified up and running
STEP: using delete to clean up resources
Dec 14 17:09:21.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 delete --grace-period=0 --force -f -'
Dec 14 17:09:21.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:21.872: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 14 17:09:21.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get rc,svc -l name=update-demo --no-headers'
Dec 14 17:09:22.146: INFO: stderr: "No resources found in kubectl-2564 namespace.\n"
Dec 14 17:09:22.146: INFO: stdout: ""
Dec 14 17:09:22.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2564 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 14 17:09:22.363: INFO: stderr: ""
Dec 14 17:09:22.363: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 17:09:22.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2564" for this suite.

• [SLOW TEST:16.779 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":329,"skipped":6110,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:09:22.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 14 17:09:23.375: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 14 17:09:25.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.December, 14, 17, 9, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 9, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 9, 23, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 9, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 14 17:09:28.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:09:28.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1132" for this suite.
STEP: Destroying namespace "webhook-1132-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.357 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":330,"skipped":6110,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:09:28.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:09:28.842: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5" in namespace "security-context-test-8477" to be "Succeeded or Failed"
Dec 14 17:09:28.847: INFO: Pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092081ms
Dec 14 17:09:30.854: INFO: Pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011330394s
Dec 14 17:09:32.868: INFO: Pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025563193s
Dec 14 17:09:34.879: INFO: Pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036020249s
Dec 14 17:09:34.879: INFO: Pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5" satisfied condition "Succeeded or Failed"
Dec 14 17:09:34.894: INFO: Got logs for pod "busybox-privileged-false-5aa33618-0199-4d88-ab32-b6b4e58e6ce5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Dec 14 17:09:34.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8477" for this suite.

• [SLOW TEST:6.171 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:234
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":331,"skipped":6139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:09:34.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Dec 14 17:09:34.974: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Dec 14 17:09:34.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:35.283: INFO: stderr: ""
Dec 14 17:09:35.283: INFO: stdout: "service/agnhost-replica created\n"
Dec 14 17:09:35.283: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Dec 14 17:09:35.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:35.681: INFO: stderr: ""
Dec 14 17:09:35.681: INFO: stdout: "service/agnhost-primary created\n"
Dec 14 17:09:35.681: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 14 17:09:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:36.033: INFO: stderr: ""
Dec 14 17:09:36.033: INFO: stdout: "service/frontend created\n"
Dec 14 17:09:36.033: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Dec 14 17:09:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:36.528: INFO: stderr: ""
Dec 14 17:09:36.528: INFO: stdout: "deployment.apps/frontend created\n"
Dec 14 17:09:36.528: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 17:09:36.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:36.927: INFO: stderr: ""
Dec 14 17:09:36.928: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Dec 14 17:09:36.929: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 14 17:09:36.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 create -f -'
Dec 14 17:09:37.354: INFO: stderr: ""
Dec 14 17:09:37.354: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Dec 14 17:09:37.354: INFO: Waiting for all frontend pods to be Running.
Dec 14 17:09:42.406: INFO: Waiting for frontend to serve content.
Dec 14 17:09:42.433: INFO: Trying to add a new entry to the guestbook.
Dec 14 17:09:42.458: INFO: Verifying that added entry can be retrieved.
Dec 14 17:09:42.474: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Dec 14 17:09:47.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:47.779: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:47.779: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 17:09:47.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:48.053: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:48.053: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 17:09:48.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:48.265: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:48.265: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 17:09:48.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:48.423: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:48.423: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 17:09:48.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:48.687: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:48.687: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Dec 14 17:09:48.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3391052245 --namespace=kubectl-2250 delete --grace-period=0 --force -f -'
Dec 14 17:09:48.852: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 14 17:09:48.852: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Dec 14 17:09:48.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2250" for this suite.

• [SLOW TEST:13.947 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":332,"skipped":6205,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:09:48.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2896
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-2896
Dec 14 17:09:49.012: INFO: Found 0 stateful pods, waiting for 1
Dec 14 17:09:59.027: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Dec 14 17:09:59.066: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Dec 14 17:09:59.092: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Dec 14 17:09:59.100: INFO: Observed &StatefulSet event: ADDED
Dec 14 17:09:59.100: INFO: Found Statefulset ss in namespace statefulset-2896 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 17:09:59.100: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Dec 14 17:09:59.100: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 17:09:59.131: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Dec 14 17:09:59.134: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 17:09:59.134: INFO: Deleting all statefulset in ns statefulset-2896
Dec 14 17:09:59.139: INFO: Scaling statefulset ss to 0
Dec 14 17:10:09.186: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 17:10:09.198: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 17:10:09.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2896" for this suite.

• [SLOW TEST:20.414 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":333,"skipped":6212,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:10:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Dec 14 17:10:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-201" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":334,"skipped":6224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:10:12.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6061
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Dec 14 17:10:12.277: INFO: Found 0 stateful pods, waiting for 3
Dec 14 17:10:22.293: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:10:22.293: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:10:22.293: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Dec 14 17:10:22.343: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 14 17:10:32.454: INFO: Updating stateful set ss2
Dec 14 17:10:32.466: INFO: Waiting for Pod statefulset-6061/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Dec 14 17:10:42.562: INFO: Found 1 stateful pods, waiting for 3
Dec 14 17:10:52.580: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:10:52.580: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 14 17:10:52.580: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 14 17:10:52.615: INFO: Updating stateful set ss2
Dec 14 17:10:52.628: INFO: Waiting for Pod statefulset-6061/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Dec 14 17:11:02.681: INFO: Updating stateful set ss2
Dec 14 17:11:02.699: INFO: Waiting for StatefulSet statefulset-6061/ss2 to complete update
Dec 14 17:11:02.699: INFO: Waiting for Pod statefulset-6061/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Dec 14 17:11:12.738: INFO: Deleting all statefulset in ns statefulset-6061
Dec 14 17:11:12.743: INFO: Scaling statefulset ss2 to 0
Dec 14 17:11:22.799: INFO: Waiting for statefulset status.replicas updated to 0
Dec 14 17:11:22.804: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Dec 14 17:11:22.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6061" for this suite.

• [SLOW TEST:70.631 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":335,"skipped":6247,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:22.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 17:11:22.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd" in namespace "downward-api-2735" to be "Succeeded or Failed"
Dec 14 17:11:22.907: INFO: Pod "downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.247434ms
Dec 14 17:11:24.919: INFO: Pod "downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01570469s
Dec 14 17:11:26.926: INFO: Pod "downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022875773s
STEP: Saw pod success
Dec 14 17:11:26.926: INFO: Pod "downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd" satisfied condition "Succeeded or Failed"
Dec 14 17:11:26.932: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd container client-container: <nil>
STEP: delete the pod
Dec 14 17:11:26.996: INFO: Waiting for pod downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd to disappear
Dec 14 17:11:27.002: INFO: Pod downwardapi-volume-12d2486b-702f-492f-8df7-d6eae8ff33cd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 17:11:27.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2735" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":336,"skipped":6259,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:27.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-10597057-f49c-4ee8-82ed-4587ba43b19b
STEP: Creating a pod to test consume configMaps
Dec 14 17:11:27.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7" in namespace "configmap-195" to be "Succeeded or Failed"
Dec 14 17:11:27.086: INFO: Pod "pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.103331ms
Dec 14 17:11:29.103: INFO: Pod "pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02387498s
Dec 14 17:11:31.112: INFO: Pod "pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033413253s
STEP: Saw pod success
Dec 14 17:11:31.112: INFO: Pod "pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7" satisfied condition "Succeeded or Failed"
Dec 14 17:11:31.121: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 17:11:31.596: INFO: Waiting for pod pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7 to disappear
Dec 14 17:11:31.602: INFO: Pod pod-configmaps-9e2cc9fc-a647-40fb-92fd-76cda156ffc7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:11:31.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-195" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:11:31.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7517" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":338,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:31.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:11:31.811: INFO: Create a RollingUpdate DaemonSet
Dec 14 17:11:31.821: INFO: Check that daemon pods launch on every node of the cluster
Dec 14 17:11:31.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:11:31.832: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:11:32.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:11:32.873: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:11:33.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Dec 14 17:11:33.858: INFO: Node queith7zooya-3 is running 0 daemon pod, expected 1
Dec 14 17:11:34.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:11:34.847: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Dec 14 17:11:34.847: INFO: Update the DaemonSet to trigger a rollout
Dec 14 17:11:34.864: INFO: Updating DaemonSet daemon-set
Dec 14 17:11:36.902: INFO: Roll back the DaemonSet before rollout is complete
Dec 14 17:11:36.988: INFO: Updating DaemonSet daemon-set
Dec 14 17:11:36.988: INFO: Make sure DaemonSet rollback is complete
Dec 14 17:11:41.058: INFO: Pod daemon-set-gkn68 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3900, will wait for the garbage collector to delete the pods
Dec 14 17:11:42.141: INFO: Deleting DaemonSet.extensions daemon-set took: 15.085835ms
Dec 14 17:11:42.243: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.526027ms
Dec 14 17:11:45.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:11:45.156: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Dec 14 17:11:45.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33225"},"items":null}

Dec 14 17:11:45.169: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33225"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 17:11:45.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3900" for this suite.

• [SLOW TEST:13.498 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":339,"skipped":6341,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:45.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:11:45.318: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e77803bb-4d33-463e-bc97-3fb4b889dbc4", Controller:(*bool)(0xc003dd1a16), BlockOwnerDeletion:(*bool)(0xc003dd1a17)}}
Dec 14 17:11:45.328: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dba882af-1323-4510-8f9a-07e3d76bdcd2", Controller:(*bool)(0xc003dfdd7e), BlockOwnerDeletion:(*bool)(0xc003dfdd7f)}}
Dec 14 17:11:45.338: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c0e4a0ed-2ed5-4c7e-8919-304cefba1fc9", Controller:(*bool)(0xc003dfdfb6), BlockOwnerDeletion:(*bool)(0xc003dfdfb7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Dec 14 17:11:50.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5719" for this suite.

• [SLOW TEST:5.169 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":340,"skipped":6344,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:50.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 14 17:11:50.446: INFO: Waiting up to 5m0s for pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792" in namespace "emptydir-3076" to be "Succeeded or Failed"
Dec 14 17:11:50.450: INFO: Pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478782ms
Dec 14 17:11:52.461: INFO: Pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015578424s
Dec 14 17:11:54.479: INFO: Pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032783765s
Dec 14 17:11:56.497: INFO: Pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0509837s
STEP: Saw pod success
Dec 14 17:11:56.497: INFO: Pod "pod-90f3d423-e38c-4baa-9a09-481ca5347792" satisfied condition "Succeeded or Failed"
Dec 14 17:11:56.504: INFO: Trying to get logs from node queith7zooya-3 pod pod-90f3d423-e38c-4baa-9a09-481ca5347792 container test-container: <nil>
STEP: delete the pod
Dec 14 17:11:56.538: INFO: Waiting for pod pod-90f3d423-e38c-4baa-9a09-481ca5347792 to disappear
Dec 14 17:11:56.548: INFO: Pod pod-90f3d423-e38c-4baa-9a09-481ca5347792 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:11:56.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3076" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:11:56.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:11:56.627: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 14 17:12:01.646: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 14 17:12:01.646: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 14 17:12:03.659: INFO: Creating deployment "test-rollover-deployment"
Dec 14 17:12:03.677: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 14 17:12:05.698: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 14 17:12:05.709: INFO: Ensure that both replica sets have 1 created replica
Dec 14 17:12:05.719: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 14 17:12:05.739: INFO: Updating deployment test-rollover-deployment
Dec 14 17:12:05.739: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 14 17:12:07.751: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 14 17:12:07.765: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 14 17:12:07.773: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 17:12:07.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:09.795: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 17:12:09.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:11.795: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 17:12:11.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:13.789: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 17:12:13.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:15.790: INFO: all replica sets need to contain the pod-template-hash label
Dec 14 17:12:15.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:17.803: INFO: 
Dec 14 17:12:17.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.December, 14, 17, 12, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.December, 14, 17, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 14 17:12:19.792: INFO: 
Dec 14 17:12:19.792: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Dec 14 17:12:19.810: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7152  814f5c4e-d896-470c-b904-137f6a709de6 33462 2 2022-12-14 17:12:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-12-14 17:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:12:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004478a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-12-14 17:12:03 +0000 UTC,LastTransitionTime:2022-12-14 17:12:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2022-12-14 17:12:17 +0000 UTC,LastTransitionTime:2022-12-14 17:12:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 14 17:12:19.820: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-7152  db3b984f-4c01-458f-9097-fc662ade8d96 33452 2 2022-12-14 17:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 814f5c4e-d896-470c-b904-137f6a709de6 0xc004478f87 0xc004478f88}] []  [{kube-controller-manager Update apps/v1 2022-12-14 17:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f5c4e-d896-470c-b904-137f6a709de6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:12:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004479038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 14 17:12:19.820: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 14 17:12:19.820: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7152  881fe1bc-61aa-48b5-812b-8e6c9650c2c0 33461 2 2022-12-14 17:11:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 814f5c4e-d896-470c-b904-137f6a709de6 0xc004478e57 0xc004478e58}] []  [{e2e.test Update apps/v1 2022-12-14 17:11:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:12:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f5c4e-d896-470c-b904-137f6a709de6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:12:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004478f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 17:12:19.821: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-7152  73b35ea0-af26-4c14-9193-a83b8968ea30 33422 2 2022-12-14 17:12:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 814f5c4e-d896-470c-b904-137f6a709de6 0xc0044790a0 0xc0044790a1}] []  [{kube-controller-manager Update apps/v1 2022-12-14 17:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"814f5c4e-d896-470c-b904-137f6a709de6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-12-14 17:12:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004479148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 14 17:12:19.847: INFO: Pod "test-rollover-deployment-779c67f4f8-7d2sq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-7d2sq test-rollover-deployment-779c67f4f8- deployment-7152  c28aa049-29ab-475b-a37c-36457c1acb92 33434 0 2022-12-14 17:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 db3b984f-4c01-458f-9097-fc662ade8d96 0xc004479687 0xc004479688}] []  [{kube-controller-manager Update v1 2022-12-14 17:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db3b984f-4c01-458f-9097-fc662ade8d96\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-12-14 17:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjc2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjc2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:queith7zooya-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-12-14 17:12:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.248,PodIP:10.233.66.184,StartTime:2022-12-14 17:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-12-14 17:12:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://5869a3c128583df9e00bc682185a283270da3a7e2f237096b60e0d4fb34b55f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Dec 14 17:12:19.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7152" for this suite.

• [SLOW TEST:23.307 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":342,"skipped":6391,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:12:19.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:12:19.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Creating first CR 
Dec 14 17:12:22.585: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:22Z]] name:name1 resourceVersion:33480 uid:0fc133cf-68a7-4112-ad46-70cbfe16f5af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 14 17:12:32.618: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:32Z]] name:name2 resourceVersion:33531 uid:0603f806-85bd-484f-bca6-988265a11e60] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 14 17:12:42.650: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:42Z]] name:name1 resourceVersion:33547 uid:0fc133cf-68a7-4112-ad46-70cbfe16f5af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 14 17:12:52.684: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:52Z]] name:name2 resourceVersion:33565 uid:0603f806-85bd-484f-bca6-988265a11e60] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 14 17:13:02.717: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:42Z]] name:name1 resourceVersion:33581 uid:0fc133cf-68a7-4112-ad46-70cbfe16f5af] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 14 17:13:12.752: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-12-14T17:12:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-12-14T17:12:52Z]] name:name2 resourceVersion:33598 uid:0603f806-85bd-484f-bca6-988265a11e60] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Dec 14 17:13:23.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-123" for this suite.

• [SLOW TEST:63.437 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":343,"skipped":6400,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:13:23.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Dec 14 17:13:23.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249" in namespace "downward-api-2684" to be "Succeeded or Failed"
Dec 14 17:13:23.402: INFO: Pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712832ms
Dec 14 17:13:25.414: INFO: Pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017261463s
Dec 14 17:13:27.427: INFO: Pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030246748s
Dec 14 17:13:29.447: INFO: Pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049504017s
STEP: Saw pod success
Dec 14 17:13:29.447: INFO: Pod "downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249" satisfied condition "Succeeded or Failed"
Dec 14 17:13:29.452: INFO: Trying to get logs from node queith7zooya-3 pod downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249 container client-container: <nil>
STEP: delete the pod
Dec 14 17:13:29.675: INFO: Waiting for pod downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249 to disappear
Dec 14 17:13:29.681: INFO: Pod downwardapi-volume-ca681771-6f91-4947-9494-0088bb68a249 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Dec 14 17:13:29.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2684" for this suite.

• [SLOW TEST:6.382 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6404,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:13:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-d0646395-6005-4c47-975e-02e64b3e2353
STEP: Creating the pod
Dec 14 17:13:29.790: INFO: The status of Pod pod-configmaps-7e48ff53-1ed8-432c-85e8-254819c4197b is Pending, waiting for it to be Running (with Ready = true)
Dec 14 17:13:31.812: INFO: The status of Pod pod-configmaps-7e48ff53-1ed8-432c-85e8-254819c4197b is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-d0646395-6005-4c47-975e-02e64b3e2353
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:13:33.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-450" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":345,"skipped":6412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:13:33.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-9429/configmap-test-53d7b77e-ffed-4f6f-814a-1e52493ea4a2
STEP: Creating a pod to test consume configMaps
Dec 14 17:13:33.973: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506" in namespace "configmap-9429" to be "Succeeded or Failed"
Dec 14 17:13:33.982: INFO: Pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506": Phase="Pending", Reason="", readiness=false. Elapsed: 9.103168ms
Dec 14 17:13:35.997: INFO: Pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024046721s
Dec 14 17:13:38.008: INFO: Pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035127905s
Dec 14 17:13:40.024: INFO: Pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05061209s
STEP: Saw pod success
Dec 14 17:13:40.024: INFO: Pod "pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506" satisfied condition "Succeeded or Failed"
Dec 14 17:13:40.034: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506 container env-test: <nil>
STEP: delete the pod
Dec 14 17:13:40.074: INFO: Waiting for pod pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506 to disappear
Dec 14 17:13:40.077: INFO: Pod pod-configmaps-4b3ba717-4b34-4eb6-88f0-09bcea4ca506 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:13:40.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9429" for this suite.

• [SLOW TEST:6.190 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6448,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:13:40.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Dec 14 17:13:40.161: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 14 17:14:40.203: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:14:40.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Dec 14 17:14:42.308: INFO: found a healthy node: queith7zooya-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:14:56.459: INFO: pods created so far: [1 1 1]
Dec 14 17:14:56.459: INFO: length of pods created so far: 3
Dec 14 17:14:58.512: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Dec 14 17:15:05.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2304" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Dec 14 17:15:05.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4161" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:85.569 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":347,"skipped":6448,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:05.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Dec 14 17:15:05.720: INFO: Waiting up to 5m0s for pod "client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975" in namespace "containers-5154" to be "Succeeded or Failed"
Dec 14 17:15:05.725: INFO: Pod "client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745485ms
Dec 14 17:15:07.737: INFO: Pod "client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016792316s
Dec 14 17:15:09.748: INFO: Pod "client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028135942s
STEP: Saw pod success
Dec 14 17:15:09.749: INFO: Pod "client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975" satisfied condition "Succeeded or Failed"
Dec 14 17:15:09.753: INFO: Trying to get logs from node queith7zooya-3 pod client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975 container agnhost-container: <nil>
STEP: delete the pod
Dec 14 17:15:09.796: INFO: Waiting for pod client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975 to disappear
Dec 14 17:15:09.804: INFO: Pod client-containers-d78dac4a-3abc-41eb-8cc4-89d938874975 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Dec 14 17:15:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5154" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":348,"skipped":6462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:09.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 14 17:15:09.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:09.911: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:15:10.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Dec 14 17:15:10.932: INFO: Node queith7zooya-1 is running 0 daemon pod, expected 1
Dec 14 17:15:11.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Dec 14 17:15:11.930: INFO: Node queith7zooya-2 is running 0 daemon pod, expected 1
Dec 14 17:15:12.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Dec 14 17:15:12.934: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Dec 14 17:15:12.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34079"},"items":null}

Dec 14 17:15:12.986: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34079"},"items":[{"metadata":{"name":"daemon-set-92wgb","generateName":"daemon-set-","namespace":"daemonsets-5597","uid":"120c5a64-f86b-4d9c-a6e0-cd00d29d74c8","resourceVersion":"34076","creationTimestamp":"2022-12-14T17:15:09Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fa7df6f1-782a-481c-930b-2445be2d02a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa7df6f1-782a-481c-930b-2445be2d02a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.172\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cfgpx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cfgpx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"queith7zooya-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["queith7zooya-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"}],"hostIP":"192.168.121.111","podIP":"10.233.65.172","podIPs":[{"ip":"10.233.65.172"}],"startTime":"2022-12-14T17:15:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T17:15:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://80bff1455850b07b889a89c8ae210cabf374781ce7ac501037d1cb515e92f5d0","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bn2ds","generateName":"daemon-set-","namespace":"daemonsets-5597","uid":"131da98d-6345-4a91-963e-19c7320e5efe","resourceVersion":"34060","creationTimestamp":"2022-12-14T17:15:09Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fa7df6f1-782a-481c-930b-2445be2d02a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa7df6f1-782a-481c-930b-2445be2d02a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n8pnm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n8pnm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"queith7zooya-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["queith7zooya-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"}],"hostIP":"192.168.121.209","podIP":"10.233.64.175","podIPs":[{"ip":"10.233.64.175"}],"startTime":"2022-12-14T17:15:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T17:15:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://cdb97c15a7cb7f711566101f38acf9edf9d3e46fd01f3330eb3fd7dfd1bdddd5","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-j9sgx","generateName":"daemon-set-","namespace":"daemonsets-5597","uid":"48367440-8473-4fc3-b3eb-5ef17cc0e04e","resourceVersion":"34077","creationTimestamp":"2022-12-14T17:15:09Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fa7df6f1-782a-481c-930b-2445be2d02a0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa7df6f1-782a-481c-930b-2445be2d02a0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-12-14T17:15:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dsc6n","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dsc6n","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"queith7zooya-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["queith7zooya-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-12-14T17:15:09Z"}],"hostIP":"192.168.121.248","podIP":"10.233.66.194","podIPs":[{"ip":"10.233.66.194"}],"startTime":"2022-12-14T17:15:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-12-14T17:15:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://6e0b48b97c177b6579ac203c8f2116af8a229f768b3e90750921444c934d21e8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Dec 14 17:15:13.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5597" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":349,"skipped":6495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:13.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Dec 14 17:15:13.097: INFO: Waiting up to 5m0s for pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f" in namespace "downward-api-9470" to be "Succeeded or Failed"
Dec 14 17:15:13.108: INFO: Pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.053162ms
Dec 14 17:15:15.134: INFO: Pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03692852s
Dec 14 17:15:17.146: INFO: Pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048376615s
Dec 14 17:15:19.153: INFO: Pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055716825s
STEP: Saw pod success
Dec 14 17:15:19.153: INFO: Pod "downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f" satisfied condition "Succeeded or Failed"
Dec 14 17:15:19.180: INFO: Trying to get logs from node queith7zooya-3 pod downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f container dapi-container: <nil>
STEP: delete the pod
Dec 14 17:15:19.442: INFO: Waiting for pod downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f to disappear
Dec 14 17:15:19.447: INFO: Pod downward-api-36ac349c-9a4d-481e-a1b3-26dac023482f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Dec 14 17:15:19.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9470" for this suite.

• [SLOW TEST:6.424 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":350,"skipped":6528,"failed":0}
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:19.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-7934-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Dec 14 17:15:19.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7934" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":351,"skipped":6528,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-4149/configmap-test-cf219f8d-c6f4-46c9-83fa-c9207debbf8e
STEP: Creating a pod to test consume configMaps
Dec 14 17:15:19.615: INFO: Waiting up to 5m0s for pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20" in namespace "configmap-4149" to be "Succeeded or Failed"
Dec 14 17:15:19.620: INFO: Pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499985ms
Dec 14 17:15:21.632: INFO: Pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016304616s
Dec 14 17:15:23.647: INFO: Pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031483267s
Dec 14 17:15:25.660: INFO: Pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044742995s
STEP: Saw pod success
Dec 14 17:15:25.660: INFO: Pod "pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20" satisfied condition "Succeeded or Failed"
Dec 14 17:15:25.666: INFO: Trying to get logs from node queith7zooya-3 pod pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20 container env-test: <nil>
STEP: delete the pod
Dec 14 17:15:25.702: INFO: Waiting for pod pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20 to disappear
Dec 14 17:15:25.707: INFO: Pod pod-configmaps-403e91e4-135d-4183-9f22-cfd97874bd20 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Dec 14 17:15:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4149" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:25.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Dec 14 17:15:29.808: INFO: Deleting pod "var-expansion-1a712ed7-3f2a-41fb-b25d-176cbbd241ec" in namespace "var-expansion-4233"
Dec 14 17:15:29.827: INFO: Wait up to 5m0s for pod "var-expansion-1a712ed7-3f2a-41fb-b25d-176cbbd241ec" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Dec 14 17:15:31.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4233" for this suite.

• [SLOW TEST:6.137 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":353,"skipped":6557,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:31.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Dec 14 17:15:31.947: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 14 17:15:36.999: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Dec 14 17:15:37.004: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Dec 14 17:15:37.029: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Dec 14 17:15:37.032: INFO: Observed &ReplicaSet event: ADDED
Dec 14 17:15:37.033: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.033: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.033: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.033: INFO: Found replicaset test-rs in namespace replicaset-1717 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Dec 14 17:15:37.034: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Dec 14 17:15:37.034: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Dec 14 17:15:37.049: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Dec 14 17:15:37.052: INFO: Observed &ReplicaSet event: ADDED
Dec 14 17:15:37.052: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.052: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.052: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.052: INFO: Observed replicaset test-rs in namespace replicaset-1717 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Dec 14 17:15:37.053: INFO: Observed &ReplicaSet event: MODIFIED
Dec 14 17:15:37.053: INFO: Found replicaset test-rs in namespace replicaset-1717 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Dec 14 17:15:37.053: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Dec 14 17:15:37.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1717" for this suite.

• [SLOW TEST:5.206 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":354,"skipped":6565,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:37.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-h58l4 in namespace proxy-9590
I1214 17:15:37.197028      14 runners.go:193] Created replication controller with name: proxy-service-h58l4, namespace: proxy-9590, replica count: 1
I1214 17:15:38.248822      14 runners.go:193] proxy-service-h58l4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 17:15:39.250093      14 runners.go:193] proxy-service-h58l4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1214 17:15:40.251477      14 runners.go:193] proxy-service-h58l4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1214 17:15:41.251766      14 runners.go:193] proxy-service-h58l4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 14 17:15:41.263: INFO: setup took 4.119569166s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 14 17:15:41.296: INFO: (0) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 32.490678ms)
Dec 14 17:15:41.296: INFO: (0) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 32.098344ms)
Dec 14 17:15:41.297: INFO: (0) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 31.589499ms)
Dec 14 17:15:41.310: INFO: (0) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 46.8152ms)
Dec 14 17:15:41.311: INFO: (0) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 46.730713ms)
Dec 14 17:15:41.311: INFO: (0) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 46.455229ms)
Dec 14 17:15:41.311: INFO: (0) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 46.533521ms)
Dec 14 17:15:41.311: INFO: (0) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 45.279561ms)
Dec 14 17:15:41.311: INFO: (0) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 46.399031ms)
Dec 14 17:15:41.314: INFO: (0) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 49.328962ms)
Dec 14 17:15:41.314: INFO: (0) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 48.135372ms)
Dec 14 17:15:41.318: INFO: (0) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 54.272082ms)
Dec 14 17:15:41.318: INFO: (0) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 54.102703ms)
Dec 14 17:15:41.319: INFO: (0) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 53.172031ms)
Dec 14 17:15:41.319: INFO: (0) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 53.245672ms)
Dec 14 17:15:41.319: INFO: (0) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 52.750849ms)
Dec 14 17:15:41.330: INFO: (1) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 11.429ms)
Dec 14 17:15:41.337: INFO: (1) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 16.385437ms)
Dec 14 17:15:41.341: INFO: (1) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 20.808922ms)
Dec 14 17:15:41.341: INFO: (1) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 20.058774ms)
Dec 14 17:15:41.343: INFO: (1) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 22.550979ms)
Dec 14 17:15:41.343: INFO: (1) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 21.102327ms)
Dec 14 17:15:41.343: INFO: (1) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 20.873768ms)
Dec 14 17:15:41.343: INFO: (1) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 23.525685ms)
Dec 14 17:15:41.343: INFO: (1) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 23.881254ms)
Dec 14 17:15:41.349: INFO: (1) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 28.784052ms)
Dec 14 17:15:41.349: INFO: (1) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 29.099943ms)
Dec 14 17:15:41.349: INFO: (1) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 30.358544ms)
Dec 14 17:15:41.350: INFO: (1) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 29.724166ms)
Dec 14 17:15:41.350: INFO: (1) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 30.105649ms)
Dec 14 17:15:41.350: INFO: (1) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 28.205882ms)
Dec 14 17:15:41.355: INFO: (1) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 34.405774ms)
Dec 14 17:15:41.365: INFO: (2) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 9.848801ms)
Dec 14 17:15:41.367: INFO: (2) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 11.790385ms)
Dec 14 17:15:41.372: INFO: (2) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 16.993465ms)
Dec 14 17:15:41.372: INFO: (2) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 16.045025ms)
Dec 14 17:15:41.373: INFO: (2) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 17.528617ms)
Dec 14 17:15:41.373: INFO: (2) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 17.057952ms)
Dec 14 17:15:41.373: INFO: (2) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 17.96538ms)
Dec 14 17:15:41.373: INFO: (2) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 17.117477ms)
Dec 14 17:15:41.373: INFO: (2) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 17.734357ms)
Dec 14 17:15:41.375: INFO: (2) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 18.525579ms)
Dec 14 17:15:41.375: INFO: (2) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 18.552522ms)
Dec 14 17:15:41.375: INFO: (2) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 18.582376ms)
Dec 14 17:15:41.377: INFO: (2) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 20.47352ms)
Dec 14 17:15:41.379: INFO: (2) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 22.804896ms)
Dec 14 17:15:41.380: INFO: (2) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 23.860993ms)
Dec 14 17:15:41.380: INFO: (2) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 23.429926ms)
Dec 14 17:15:41.390: INFO: (3) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 9.495055ms)
Dec 14 17:15:41.391: INFO: (3) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 10.533299ms)
Dec 14 17:15:41.392: INFO: (3) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 11.348014ms)
Dec 14 17:15:41.392: INFO: (3) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 9.550454ms)
Dec 14 17:15:41.396: INFO: (3) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 13.847722ms)
Dec 14 17:15:41.403: INFO: (3) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 21.691307ms)
Dec 14 17:15:41.403: INFO: (3) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 22.09258ms)
Dec 14 17:15:41.403: INFO: (3) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 22.762902ms)
Dec 14 17:15:41.403: INFO: (3) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 22.602135ms)
Dec 14 17:15:41.404: INFO: (3) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 21.688772ms)
Dec 14 17:15:41.404: INFO: (3) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 21.648235ms)
Dec 14 17:15:41.404: INFO: (3) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 21.806554ms)
Dec 14 17:15:41.405: INFO: (3) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 22.997402ms)
Dec 14 17:15:41.405: INFO: (3) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 23.404574ms)
Dec 14 17:15:41.405: INFO: (3) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 22.703302ms)
Dec 14 17:15:41.405: INFO: (3) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 23.928165ms)
Dec 14 17:15:41.416: INFO: (4) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 10.666877ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 13.186471ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 14.062752ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 13.535055ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 13.766872ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 13.643634ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 14.795024ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 14.521359ms)
Dec 14 17:15:41.420: INFO: (4) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 14.033957ms)
Dec 14 17:15:41.423: INFO: (4) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 16.335135ms)
Dec 14 17:15:41.426: INFO: (4) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 19.585503ms)
Dec 14 17:15:41.426: INFO: (4) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 20.60894ms)
Dec 14 17:15:41.426: INFO: (4) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 20.59999ms)
Dec 14 17:15:41.427: INFO: (4) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 20.662683ms)
Dec 14 17:15:41.427: INFO: (4) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 20.720862ms)
Dec 14 17:15:41.429: INFO: (4) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 23.520838ms)
Dec 14 17:15:41.437: INFO: (5) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 8.185475ms)
Dec 14 17:15:41.441: INFO: (5) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 11.615387ms)
Dec 14 17:15:41.443: INFO: (5) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 12.788645ms)
Dec 14 17:15:41.457: INFO: (5) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 27.673902ms)
Dec 14 17:15:41.458: INFO: (5) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 28.315949ms)
Dec 14 17:15:41.459: INFO: (5) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 28.481297ms)
Dec 14 17:15:41.459: INFO: (5) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 28.159377ms)
Dec 14 17:15:41.460: INFO: (5) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 29.847168ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 29.635279ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 30.086472ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 30.505624ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 30.872589ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 31.870729ms)
Dec 14 17:15:41.461: INFO: (5) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 32.199834ms)
Dec 14 17:15:41.462: INFO: (5) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 31.79243ms)
Dec 14 17:15:41.466: INFO: (5) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 35.344476ms)
Dec 14 17:15:41.476: INFO: (6) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 9.892651ms)
Dec 14 17:15:41.477: INFO: (6) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 10.699707ms)
Dec 14 17:15:41.477: INFO: (6) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 10.478898ms)
Dec 14 17:15:41.477: INFO: (6) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 10.767038ms)
Dec 14 17:15:41.479: INFO: (6) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 12.595032ms)
Dec 14 17:15:41.480: INFO: (6) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 13.311173ms)
Dec 14 17:15:41.480: INFO: (6) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 13.511632ms)
Dec 14 17:15:41.482: INFO: (6) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 15.109714ms)
Dec 14 17:15:41.482: INFO: (6) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 15.059613ms)
Dec 14 17:15:41.483: INFO: (6) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 16.016028ms)
Dec 14 17:15:41.483: INFO: (6) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 16.197397ms)
Dec 14 17:15:41.507: INFO: (6) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 40.214218ms)
Dec 14 17:15:41.507: INFO: (6) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 39.67671ms)
Dec 14 17:15:41.507: INFO: (6) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 39.666226ms)
Dec 14 17:15:41.507: INFO: (6) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 40.024142ms)
Dec 14 17:15:41.507: INFO: (6) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 40.08265ms)
Dec 14 17:15:41.529: INFO: (7) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 21.180709ms)
Dec 14 17:15:41.529: INFO: (7) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 20.989137ms)
Dec 14 17:15:41.529: INFO: (7) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 21.638137ms)
Dec 14 17:15:41.529: INFO: (7) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 21.746189ms)
Dec 14 17:15:41.530: INFO: (7) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 21.227624ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 30.6842ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 29.871022ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 30.374052ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 31.106522ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 30.287734ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 30.606497ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 30.328574ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 32.183686ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 32.299344ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 30.006952ms)
Dec 14 17:15:41.539: INFO: (7) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 30.222216ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 16.639662ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 15.776377ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 17.117291ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 16.785282ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 16.4913ms)
Dec 14 17:15:41.557: INFO: (8) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 15.895535ms)
Dec 14 17:15:41.563: INFO: (8) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 22.605209ms)
Dec 14 17:15:41.564: INFO: (8) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 22.943577ms)
Dec 14 17:15:41.564: INFO: (8) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 24.128983ms)
Dec 14 17:15:41.565: INFO: (8) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 23.933317ms)
Dec 14 17:15:41.565: INFO: (8) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 23.608838ms)
Dec 14 17:15:41.565: INFO: (8) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 24.625968ms)
Dec 14 17:15:41.569: INFO: (8) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 28.74679ms)
Dec 14 17:15:41.569: INFO: (8) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 27.67266ms)
Dec 14 17:15:41.569: INFO: (8) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 27.936233ms)
Dec 14 17:15:41.578: INFO: (8) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 37.668507ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 23.490204ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 23.966404ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 23.634545ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 23.412519ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 23.85751ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 24.135063ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 23.26182ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 24.018181ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 23.400875ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 23.909517ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 23.50089ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 23.760046ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 24.032924ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 24.225912ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 23.7945ms)
Dec 14 17:15:41.623: INFO: (9) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 23.68438ms)
Dec 14 17:15:41.642: INFO: (10) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 17.373324ms)
Dec 14 17:15:41.642: INFO: (10) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 18.883837ms)
Dec 14 17:15:41.643: INFO: (10) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 19.350036ms)
Dec 14 17:15:41.643: INFO: (10) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 18.276552ms)
Dec 14 17:15:41.643: INFO: (10) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 19.270752ms)
Dec 14 17:15:41.644: INFO: (10) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 19.161183ms)
Dec 14 17:15:41.653: INFO: (10) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 28.834145ms)
Dec 14 17:15:41.654: INFO: (10) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 29.037694ms)
Dec 14 17:15:41.654: INFO: (10) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 30.389563ms)
Dec 14 17:15:41.654: INFO: (10) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 30.738419ms)
Dec 14 17:15:41.656: INFO: (10) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 32.430318ms)
Dec 14 17:15:41.656: INFO: (10) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 30.193023ms)
Dec 14 17:15:41.656: INFO: (10) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 31.660651ms)
Dec 14 17:15:41.657: INFO: (10) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 32.570227ms)
Dec 14 17:15:41.657: INFO: (10) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 33.150437ms)
Dec 14 17:15:41.660: INFO: (10) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 33.361709ms)
Dec 14 17:15:41.673: INFO: (11) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 12.864821ms)
Dec 14 17:15:41.679: INFO: (11) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 18.936437ms)
Dec 14 17:15:41.679: INFO: (11) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 18.860491ms)
Dec 14 17:15:41.680: INFO: (11) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 18.811608ms)
Dec 14 17:15:41.689: INFO: (11) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 27.331451ms)
Dec 14 17:15:41.690: INFO: (11) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 28.454741ms)
Dec 14 17:15:41.695: INFO: (11) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 34.801653ms)
Dec 14 17:15:41.695: INFO: (11) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 34.714423ms)
Dec 14 17:15:41.695: INFO: (11) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 34.228079ms)
Dec 14 17:15:41.696: INFO: (11) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 34.148828ms)
Dec 14 17:15:41.696: INFO: (11) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 34.593566ms)
Dec 14 17:15:41.698: INFO: (11) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 36.729318ms)
Dec 14 17:15:41.699: INFO: (11) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 37.096496ms)
Dec 14 17:15:41.700: INFO: (11) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 37.889615ms)
Dec 14 17:15:41.700: INFO: (11) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 38.202544ms)
Dec 14 17:15:41.703: INFO: (11) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 40.678551ms)
Dec 14 17:15:41.723: INFO: (12) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 19.583981ms)
Dec 14 17:15:41.723: INFO: (12) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 20.358527ms)
Dec 14 17:15:41.723: INFO: (12) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 20.123272ms)
Dec 14 17:15:41.723: INFO: (12) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 20.446247ms)
Dec 14 17:15:41.734: INFO: (12) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 30.867138ms)
Dec 14 17:15:41.735: INFO: (12) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 31.231164ms)
Dec 14 17:15:41.735: INFO: (12) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 30.853754ms)
Dec 14 17:15:41.735: INFO: (12) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 32.177085ms)
Dec 14 17:15:41.735: INFO: (12) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 31.089313ms)
Dec 14 17:15:41.735: INFO: (12) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 31.536455ms)
Dec 14 17:15:41.736: INFO: (12) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 31.913602ms)
Dec 14 17:15:41.736: INFO: (12) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 32.235668ms)
Dec 14 17:15:41.736: INFO: (12) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 32.635516ms)
Dec 14 17:15:41.752: INFO: (12) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 47.303512ms)
Dec 14 17:15:41.753: INFO: (12) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 49.604692ms)
Dec 14 17:15:41.754: INFO: (12) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 49.454582ms)
Dec 14 17:15:41.773: INFO: (13) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 18.046066ms)
Dec 14 17:15:41.778: INFO: (13) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 21.847229ms)
Dec 14 17:15:41.781: INFO: (13) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 26.02215ms)
Dec 14 17:15:41.781: INFO: (13) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 24.994685ms)
Dec 14 17:15:41.781: INFO: (13) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 26.244988ms)
Dec 14 17:15:41.781: INFO: (13) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 25.725469ms)
Dec 14 17:15:41.781: INFO: (13) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 25.621216ms)
Dec 14 17:15:41.782: INFO: (13) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 26.209951ms)
Dec 14 17:15:41.783: INFO: (13) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 26.375507ms)
Dec 14 17:15:41.783: INFO: (13) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 27.523485ms)
Dec 14 17:15:41.784: INFO: (13) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 29.549103ms)
Dec 14 17:15:41.785: INFO: (13) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 29.50489ms)
Dec 14 17:15:41.785: INFO: (13) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 28.740067ms)
Dec 14 17:15:41.785: INFO: (13) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 29.176417ms)
Dec 14 17:15:41.786: INFO: (13) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 30.407453ms)
Dec 14 17:15:41.787: INFO: (13) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 31.088272ms)
Dec 14 17:15:41.793: INFO: (14) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 5.824962ms)
Dec 14 17:15:41.804: INFO: (14) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 17.037988ms)
Dec 14 17:15:41.809: INFO: (14) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 20.57099ms)
Dec 14 17:15:41.809: INFO: (14) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 20.815805ms)
Dec 14 17:15:41.811: INFO: (14) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 23.063878ms)
Dec 14 17:15:41.816: INFO: (14) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 28.379557ms)
Dec 14 17:15:41.816: INFO: (14) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 27.058512ms)
Dec 14 17:15:41.818: INFO: (14) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 28.958731ms)
Dec 14 17:15:41.818: INFO: (14) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 28.467204ms)
Dec 14 17:15:41.818: INFO: (14) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 29.548504ms)
Dec 14 17:15:41.818: INFO: (14) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 29.175018ms)
Dec 14 17:15:41.818: INFO: (14) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 29.654611ms)
Dec 14 17:15:41.819: INFO: (14) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 30.575308ms)
Dec 14 17:15:41.820: INFO: (14) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 30.754035ms)
Dec 14 17:15:41.821: INFO: (14) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 31.333618ms)
Dec 14 17:15:41.821: INFO: (14) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 33.431659ms)
Dec 14 17:15:41.843: INFO: (15) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 21.593529ms)
Dec 14 17:15:41.845: INFO: (15) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 22.910591ms)
Dec 14 17:15:41.845: INFO: (15) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 21.584589ms)
Dec 14 17:15:41.845: INFO: (15) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 22.540839ms)
Dec 14 17:15:41.847: INFO: (15) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 23.100069ms)
Dec 14 17:15:41.847: INFO: (15) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 23.93579ms)
Dec 14 17:15:41.847: INFO: (15) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 23.342815ms)
Dec 14 17:15:41.847: INFO: (15) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 24.557708ms)
Dec 14 17:15:41.848: INFO: (15) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 24.791437ms)
Dec 14 17:15:41.848: INFO: (15) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 25.352303ms)
Dec 14 17:15:41.848: INFO: (15) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 26.098929ms)
Dec 14 17:15:41.848: INFO: (15) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 26.030825ms)
Dec 14 17:15:41.848: INFO: (15) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 25.781893ms)
Dec 14 17:15:41.852: INFO: (15) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 29.285754ms)
Dec 14 17:15:41.852: INFO: (15) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 28.97484ms)
Dec 14 17:15:41.854: INFO: (15) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 30.636193ms)
Dec 14 17:15:41.878: INFO: (16) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 23.872967ms)
Dec 14 17:15:41.879: INFO: (16) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 23.423236ms)
Dec 14 17:15:41.879: INFO: (16) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 23.821665ms)
Dec 14 17:15:41.882: INFO: (16) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 27.251091ms)
Dec 14 17:15:41.883: INFO: (16) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 26.13286ms)
Dec 14 17:15:41.890: INFO: (16) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 35.571945ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 34.679504ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 34.806287ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 36.140832ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 34.651881ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 35.214792ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 35.12005ms)
Dec 14 17:15:41.891: INFO: (16) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 35.786571ms)
Dec 14 17:15:41.899: INFO: (16) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 43.149035ms)
Dec 14 17:15:41.901: INFO: (16) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 44.694217ms)
Dec 14 17:15:41.906: INFO: (16) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 50.885542ms)
Dec 14 17:15:41.925: INFO: (17) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 18.284674ms)
Dec 14 17:15:41.927: INFO: (17) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 20.153459ms)
Dec 14 17:15:41.929: INFO: (17) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 22.297743ms)
Dec 14 17:15:41.929: INFO: (17) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 22.156669ms)
Dec 14 17:15:41.931: INFO: (17) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 24.204846ms)
Dec 14 17:15:41.931: INFO: (17) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 23.238438ms)
Dec 14 17:15:41.931: INFO: (17) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 24.332608ms)
Dec 14 17:15:41.931: INFO: (17) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 23.568082ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 24.998782ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 24.7717ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 24.654716ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 25.015378ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 24.196526ms)
Dec 14 17:15:41.932: INFO: (17) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 24.698426ms)
Dec 14 17:15:41.934: INFO: (17) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 27.174616ms)
Dec 14 17:15:41.936: INFO: (17) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 28.271811ms)
Dec 14 17:15:41.944: INFO: (18) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 6.877307ms)
Dec 14 17:15:41.944: INFO: (18) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 7.649799ms)
Dec 14 17:15:41.944: INFO: (18) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 7.439469ms)
Dec 14 17:15:41.957: INFO: (18) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 19.193713ms)
Dec 14 17:15:41.957: INFO: (18) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 21.299488ms)
Dec 14 17:15:41.957: INFO: (18) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 20.405523ms)
Dec 14 17:15:41.959: INFO: (18) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 20.489315ms)
Dec 14 17:15:41.959: INFO: (18) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 21.155278ms)
Dec 14 17:15:41.959: INFO: (18) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 22.724792ms)
Dec 14 17:15:41.959: INFO: (18) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 21.273702ms)
Dec 14 17:15:41.959: INFO: (18) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 22.020887ms)
Dec 14 17:15:41.962: INFO: (18) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 26.476851ms)
Dec 14 17:15:41.962: INFO: (18) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 25.33662ms)
Dec 14 17:15:41.963: INFO: (18) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 26.023909ms)
Dec 14 17:15:41.967: INFO: (18) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 30.580449ms)
Dec 14 17:15:41.967: INFO: (18) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 29.200948ms)
Dec 14 17:15:41.980: INFO: (19) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">... (200; 12.832889ms)
Dec 14 17:15:41.982: INFO: (19) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname1/proxy/: foo (200; 15.202991ms)
Dec 14 17:15:41.982: INFO: (19) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:1080/proxy/rewriteme">test<... (200; 13.773562ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 15.497706ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:462/proxy/: tls qux (200; 16.742238ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:443/proxy/tlsrewritem... (200; 16.188144ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:162/proxy/: bar (200; 14.964981ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/pods/http:proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 15.320658ms)
Dec 14 17:15:41.984: INFO: (19) /api/v1/namespaces/proxy-9590/services/http:proxy-service-h58l4:portname2/proxy/: bar (200; 15.852769ms)
Dec 14 17:15:41.989: INFO: (19) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw:160/proxy/: foo (200; 20.908378ms)
Dec 14 17:15:41.989: INFO: (19) /api/v1/namespaces/proxy-9590/pods/https:proxy-service-h58l4-dgmnw:460/proxy/: tls baz (200; 19.910006ms)
Dec 14 17:15:41.989: INFO: (19) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname2/proxy/: bar (200; 21.644165ms)
Dec 14 17:15:41.989: INFO: (19) /api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/: <a href="/api/v1/namespaces/proxy-9590/pods/proxy-service-h58l4-dgmnw/proxy/rewriteme">test</a> (200; 20.983312ms)
Dec 14 17:15:41.994: INFO: (19) /api/v1/namespaces/proxy-9590/services/proxy-service-h58l4:portname1/proxy/: foo (200; 26.198372ms)
Dec 14 17:15:41.996: INFO: (19) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname1/proxy/: tls baz (200; 27.115987ms)
Dec 14 17:15:42.000: INFO: (19) /api/v1/namespaces/proxy-9590/services/https:proxy-service-h58l4:tlsportname2/proxy/: tls qux (200; 31.448391ms)
STEP: deleting ReplicationController proxy-service-h58l4 in namespace proxy-9590, will wait for the garbage collector to delete the pods
Dec 14 17:15:42.068: INFO: Deleting ReplicationController proxy-service-h58l4 took: 9.377452ms
Dec 14 17:15:42.169: INFO: Terminating ReplicationController proxy-service-h58l4 pods took: 101.558525ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Dec 14 17:15:45.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9590" for this suite.

• [SLOW TEST:8.128 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":355,"skipped":6578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Dec 14 17:15:45.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3391052245
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 14 17:15:45.265: INFO: Waiting up to 5m0s for pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee" in namespace "emptydir-2054" to be "Succeeded or Failed"
Dec 14 17:15:45.271: INFO: Pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072788ms
Dec 14 17:15:47.278: INFO: Pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013010117s
Dec 14 17:15:49.292: INFO: Pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027446199s
Dec 14 17:15:51.305: INFO: Pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040123558s
STEP: Saw pod success
Dec 14 17:15:51.305: INFO: Pod "pod-08bac644-ca02-4a3c-a34e-beee4b1136ee" satisfied condition "Succeeded or Failed"
Dec 14 17:15:51.310: INFO: Trying to get logs from node queith7zooya-3 pod pod-08bac644-ca02-4a3c-a34e-beee4b1136ee container test-container: <nil>
STEP: delete the pod
Dec 14 17:15:51.533: INFO: Waiting for pod pod-08bac644-ca02-4a3c-a34e-beee4b1136ee to disappear
Dec 14 17:15:51.537: INFO: Pod pod-08bac644-ca02-4a3c-a34e-beee4b1136ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Dec 14 17:15:51.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2054" for this suite.

• [SLOW TEST:6.349 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":356,"skipped":6610,"failed":0}
SSSSSSSDec 14 17:15:51.558: INFO: Running AfterSuite actions on all nodes
Dec 14 17:15:51.558: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Dec 14 17:15:51.558: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Dec 14 17:15:51.559: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Dec 14 17:15:51.559: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Dec 14 17:15:51.559: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Dec 14 17:15:51.559: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Dec 14 17:15:51.559: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Dec 14 17:15:51.559: INFO: Running AfterSuite actions on node 1
Dec 14 17:15:51.559: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 5954.797 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h39m17.883116811s
Test Suite Passed
